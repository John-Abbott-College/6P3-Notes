[
  {
    "objectID": "about/this-site/index.html",
    "href": "about/this-site/index.html",
    "title": "About this website",
    "section": "",
    "text": "Course content, both for lectures and assignments, has been adapted by me from a variety of sources. This section serves the purpose of both acknowledging these references as well as pointing the way for curious students to begin investigating further into course material than we had time to cover.\n\n\nAll course content was either written by Mauricio Buschinelli, Michael Haaf, or explicitly adapted from external resources with attributions made clear. This course content retains the licenses of the original works where relevant, and is otherwise licensed under the Creative Commons Attribution 4.0 International License.\n\nTerms of use\nYou are free to:\n\nShare: copy and redistribute the material in any medium or format\nAdapt: remix, transform, and build upon the material for any purpose, even commercially.\n\nUnder the following terms:\n\nAttribution: You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.\n\nCreative Commons Attribution 4.0 International License"
  },
  {
    "objectID": "about/this-site/index.html#course-content",
    "href": "about/this-site/index.html#course-content",
    "title": "About this website",
    "section": "",
    "text": "Course content, both for lectures and assignments, has been adapted by me from a variety of sources. This section serves the purpose of both acknowledging these references as well as pointing the way for curious students to begin investigating further into course material than we had time to cover.\n\n\nAll course content was either written by Mauricio Buschinelli, Michael Haaf, or explicitly adapted from external resources with attributions made clear. This course content retains the licenses of the original works where relevant, and is otherwise licensed under the Creative Commons Attribution 4.0 International License.\n\nTerms of use\nYou are free to:\n\nShare: copy and redistribute the material in any medium or format\nAdapt: remix, transform, and build upon the material for any purpose, even commercially.\n\nUnder the following terms:\n\nAttribution: You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.\n\nCreative Commons Attribution 4.0 International License"
  },
  {
    "objectID": "about/this-site/index.html#course-website",
    "href": "about/this-site/index.html#course-website",
    "title": "About this website",
    "section": "2 Course website",
    "text": "2 Course website\nDetails about how this website was built follow.\n\n2.1 Colophon\n\n\n\n\n\nFrom Wikipedia: In publishing, a colophon is a brief statement containing information about the publication of a book… Some web pages also have colophons, which frequently contain (X)HTML, CSS, or usability standards compliance information and links to website validation tests.\n\n\n\nMarkup: Markdown\nFramework: Quarto\nDeployment: GitHub Pages\n\nYou can follow along directly with course updates at the course repository."
  },
  {
    "objectID": "about/syllabus/index.html",
    "href": "about/syllabus/index.html",
    "title": "Syllabus",
    "section": "",
    "text": "Room, times, etc.: see Course Outline\nOffice: Penfield 311\nOffice hours: Mon/Fri 11:00 - 12:30pm or so\nEmail: michael DOT haaf AT johnabbott DOT qc DOT ca\nCourse webpage: This website for all content, Moodle for assignment/project/quiz submissions.\n\n\n\n\n\n\nForgive me the smarmy comic, this is usually true…\n\n\n\n\n\nTeams - For communicating (fastest, most reliable that I will check it same-day)\nMIO - For communicating (non-time-critical)\nEmail - For communicating (time-critical). Will aim for &lt;24hr response.\nMoodle - For receiving & submitting exercises/assignments/project and getting marks\n\n\n\n\n\n45% Assignments and Labs\n20% Test (first week of April)\n35% Project Milestones"
  },
  {
    "objectID": "about/syllabus/index.html#logistics",
    "href": "about/syllabus/index.html#logistics",
    "title": "Syllabus",
    "section": "",
    "text": "Room, times, etc.: see Course Outline\nOffice: Penfield 311\nOffice hours: Mon/Fri 11:00 - 12:30pm or so\nEmail: michael DOT haaf AT johnabbott DOT qc DOT ca\nCourse webpage: This website for all content, Moodle for assignment/project/quiz submissions.\n\n\n\n\n\n\nForgive me the smarmy comic, this is usually true…\n\n\n\n\n\nTeams - For communicating (fastest, most reliable that I will check it same-day)\nMIO - For communicating (non-time-critical)\nEmail - For communicating (time-critical). Will aim for &lt;24hr response.\nMoodle - For receiving & submitting exercises/assignments/project and getting marks\n\n\n\n\n\n45% Assignments and Labs\n20% Test (first week of April)\n35% Project Milestones"
  },
  {
    "objectID": "about/syllabus/index.html#course-material",
    "href": "about/syllabus/index.html#course-material",
    "title": "Syllabus",
    "section": "2 Course material",
    "text": "2 Course material\nThere are no required textbooks for this course – this website will contain all of the content that you need to complete course deliverables. Additionally, each lecture will contain references to additional resources for exploring each topic in further detail beyond the scope of the course."
  },
  {
    "objectID": "notes/azure-storage/index.html",
    "href": "notes/azure-storage/index.html",
    "title": "Azure Blob Storage",
    "section": "",
    "text": "As one can imagine, there are many types of storage accounts in Azure. In this course we’ll explored Blob Storage (object storage).\n\nTo know more about the other storage services offered by Azure see Introduction to Azure Storage.",
    "crumbs": [
      "Azure Blob Storage"
    ]
  },
  {
    "objectID": "notes/azure-storage/index.html#storage-accounts",
    "href": "notes/azure-storage/index.html#storage-accounts",
    "title": "Azure Blob Storage",
    "section": "",
    "text": "As one can imagine, there are many types of storage accounts in Azure. In this course we’ll explored Blob Storage (object storage).\n\nTo know more about the other storage services offered by Azure see Introduction to Azure Storage.",
    "crumbs": [
      "Azure Blob Storage"
    ]
  },
  {
    "objectID": "notes/azure-storage/index.html#blob-storage",
    "href": "notes/azure-storage/index.html#blob-storage",
    "title": "Azure Blob Storage",
    "section": "2 Blob Storage",
    "text": "2 Blob Storage\nAzure Blob storage is an object storage solution optimized for storing massive amounts of text or binary data.\nBlob storage is designed for:\n\nServing images or documents directly to a browser.\nStoring files for distributed access.\nStreaming video and audio.\nWriting to log files.\nStoring data for backup and restore, disaster recovery, and archiving.\nStoring data for analysis by an on-premises or Azure-hosted service.\n\n\n2.1 Storage Structure\nBlob storage has the following structure:\n\nStorage account: provides a unique address that includes your account name.\nContainers: organizes a set of blobs, similar to a directory in a file system.\nBlobs: the file being stored.\n\n\n\n\n\n\n\nFigure 1: Structure of Blob storage From Microsoft Docs, introduction to Azure Blob storage.\n\n\n\n\n\n2.2 Creating a Storage Account & Container\n\nFor details on how to create a storage account see Create a storage account.\n\nThe default setting for creating a Storage Account and Blob Container are sufficient for this course.\nHowever, if you would like your containers to have sub-folders, see the next section.\n\n2.2.1 Hierarchical Directories & Data Lake Storage\nBy default, Blob storage has a flat file structure. This means that all blobs (files) sit at the same level inside the container.\nAzure Data Lake Storage Gen2 is built on top of Blob storage, but offers additional features designed for big data analytics.\nIn particular, it offers hierarchical directory organisation at a marginal price increase.\n\nThis simplifies storage retrieval and organisation so it will be the preferred storage option for IoT telemetry data.\nFor more information see Introduction to Azure Data Lake Storage Gen2.\n\nNote: A Blob storage container is the Azure equivalent of an Amazon S3 bucket.",
    "crumbs": [
      "Azure Blob Storage"
    ]
  },
  {
    "objectID": "notes/azure-storage/index.html#blob-storage-sdks",
    "href": "notes/azure-storage/index.html#blob-storage-sdks",
    "title": "Azure Blob Storage",
    "section": "3 Blob Storage SDKs",
    "text": "3 Blob Storage SDKs\nAs usual, Azure provides a SDK to interact with blob storage.\n\n3.0.1 Python SDK\n\nPython SDK documentation overview for: - Getting started. - Key concepts. - Examples.\n\nQuick Links Package (PyPI) | API reference documentation | Product documentation | Samples\n\n\n3.0.2 .NET SDK\n\n.NET SDK documentation overview for: - Getting started. - Key concepts. - Examples.\n\nQuick Links Package (NuGet) | API reference documentation | REST API documentation | Product documentation\n\n\n3.0.3 Notes on Blob Storage SDKs:\nBlob name is usually the path to the blob object relative to the container.\n\n\n\n\n\n\nFigure 2: Here we have the container name on the left and the blob name on the right.",
    "crumbs": [
      "Azure Blob Storage"
    ]
  },
  {
    "objectID": "notes/azure-storage/index.html#blog-storage-event-hubs",
    "href": "notes/azure-storage/index.html#blog-storage-event-hubs",
    "title": "Azure Blob Storage",
    "section": "4 Blog Storage & Event Hubs",
    "text": "4 Blog Storage & Event Hubs\nWhen ingesting EventHub events using the Nuget package Azure.Messaging.EventHubs, you might require a BlobContainerClient.\nTo create an instance of a BlobContainerClient , you must provide the following information:\n\nStorage account connection string\n\nFollow steps bellow (from section: Get a connection string for the storage account.)\n\nNavigate to your storage account in the Azure portal.\nIn the Security + networking section, locate the Access keys setting.\nTo display the account keys and associated connection strings, select the Show keys button at the top of the page.\n\n\nBlob Container Name.\n\nSimply the name of the container you created inside the storage account.\n\n\n\n\n\n\n\n\nFigure 3: Here we have checkpoint-blob as the name of the container.",
    "crumbs": [
      "Azure Blob Storage"
    ]
  },
  {
    "objectID": "notes/azure-storage/index.html#references",
    "href": "notes/azure-storage/index.html#references",
    "title": "Azure Blob Storage",
    "section": "5 References",
    "text": "5 References\n\nIntroduction to Azure Blob storage by Microsoft.\nCreate Azure Data Lake Database, Schema, Table, View, Function and Stored Procedure, by: Ron L’Esteve\nGithub Repo w/ Samples: Azure Storage Files Data Lake client library for .NET\nSee the videos below.\n\n\n\n\n\n\n\n\nFigure 4: Excellent introduction to Data Lake Storage\n\n\n\n\n\n\n\n\n\n\nFigure 5: Examples of using Data Lake SDK with C#:",
    "crumbs": [
      "Azure Blob Storage"
    ]
  },
  {
    "objectID": "notes/microcomputers/index.html",
    "href": "notes/microcomputers/index.html",
    "title": "Microcomputers",
    "section": "",
    "text": "This section explores the differences between single board computers (SBC) such as the Raspberry Pi and micro-controllers (MCU) such as the Arduino.",
    "crumbs": [
      "Microcomputers"
    ]
  },
  {
    "objectID": "notes/microcomputers/index.html#single-board-computers-sbcs",
    "href": "notes/microcomputers/index.html#single-board-computers-sbcs",
    "title": "Microcomputers",
    "section": "1 Single Board Computers (SBCs)",
    "text": "1 Single Board Computers (SBCs)\nThe Raspberry Pi is a single board computer (SBC), which means it has all the components of a full sized computer, however, they are attached to a single board.\n\n\n\nDiagram of the different raspberry pi modules\n\n\n\n Modules of a Raspberry Pi 4   - Wikipedia.\n\nAs a full blown computer, the Raspberry Pi runs a Linux Operating System that manages:\n\nDevice drivers (touch screens, buttons, LEDs, keyboard and mouse inputs, etc).\nMemory (per process RAM allocation).\nNetworking (ethernet and wifi).\nVideo output (GPU).\n\nThe Raspberry Pi can use any language that the Linux operating system supports. For example:\n\nPython\nJavaScript (including Node.js)\nC and C++\nJava\nC#\netc\n\nThe Raspberry Pi’s hardware, including the GPIO’s, are available at the operating system level. Most of the languages listed above offer libraries that facilitate access and use of the hardware.",
    "crumbs": [
      "Microcomputers"
    ]
  },
  {
    "objectID": "notes/microcomputers/index.html#other-sbcs",
    "href": "notes/microcomputers/index.html#other-sbcs",
    "title": "Microcomputers",
    "section": "2 Other SBCs",
    "text": "2 Other SBCs\nThe Raspberry Pi is not the only SBC available for purchase. There are many other boards such as:\n\nBeagleBone\nNvidia Jetson\nRock64\nAsus Tinker Board",
    "crumbs": [
      "Microcomputers"
    ]
  },
  {
    "objectID": "notes/microcomputers/index.html#microcontrollers-mcu",
    "href": "notes/microcomputers/index.html#microcontrollers-mcu",
    "title": "Microcomputers",
    "section": "3 Microcontrollers (MCU)",
    "text": "3 Microcontrollers (MCU)\nA microcontroller board is a “bare-bones” form of a computing module.\n\nAt its core, a microcontroller is a single chip capable of executing the fetch-decode-execute cycle of a program.\n\n\n\n\nblock diagram for fetch-decode-execute-loop\n\n\n\n Fetch-decode-execute cycle of a program   - Microsoft, IoT for Beginners.\n\nOne of the most well known microcontrollers today is the Arduino platform. It became popular with the introduction of the Arduino UNO (see image below) in 2003.\nThe heart of the Arduino UNO is the microcontroller Atmel328, which functions as both the CPU, the RAM (for run-time memory) and the Flash memory (for storing program instructions).\n\n\n\nbasic atmel mcu besides a arduino uno\n\n\n\n Microcontroller Atmel328 in a basic setup compared to the Arduino UNO   The ShrimpingIt Projects.\n\nToday there are dozens of variants of the Arduino board, far more capable and permanent than the original UNO.\n\n\n\nExamples of other arduino boards\n\n\n\n Examples of Arduino compatible boards   Yusuf1809 on Fiverr.com.\n\nPrograms written for Arduino boards are typically written in C++.\nThe basic program structure is a setup() function that initializes pins and variables and a loop() function that loops forever.\nThe code needs to be compiled before it is flashed to the chip’s flash memory.",
    "crumbs": [
      "Microcomputers"
    ]
  },
  {
    "objectID": "notes/microcomputers/index.html#mcus-vs-sbcs",
    "href": "notes/microcomputers/index.html#mcus-vs-sbcs",
    "title": "Microcomputers",
    "section": "4 MCUs vs SBCs",
    "text": "4 MCUs vs SBCs\nLet’s compare microcontrollers and single board computers using the Arduino platform and the Raspberry Pi.\n\n\n\n\nRaspberry Pi 4\nRaspberry Pi Zero W\nArduino Uno\nArduino Portenta H7\n\n\n\n\nPrice (CAD)\nFrom $55\nFrom $15\nFrom $27\nFrom $147\n\n\nRAM\n1GB to 8GB\n512MB\n2KB SRAM\nBase model: 8MB, Custom: 64MB\n\n\nCPU\n1.5GHz quad-core ARMv8-A\n1GHz ARM1176JZF-S\n16MHz ATMega328P\nSTM32 dual Cortex-M7+M4 32bit low power Arm\n\n\nGPIO\n40 (no analog input)\n40 (no analog analog)\n14 including, 6 Analog inputs, 6 PWM\n28 including, 7 Analog inputs\n\n\nStorage\nMicro SD, USB\nMicro SD, USB\n32KB Flash memory\nBase model: 16MB Flash, Custom: Up to 128MB Flash storage\n\n\nNetwork\nGigabit Ethernet, WiFi b/g/n & 5GHz AC\n802.11 b/g/n wireless LAN, Bluetooth 4.1, Bluetooth Low Energy (BLE)\nRequires Shield Add On\nMurata 1DX dual WiFi 802.11b/g/n 65 Mbps and Bluetooth 5.1\n\n\nPower Consumption\n~700+ mA (5V)\n170 mA (5V)\n~50 mA (5-12V)\n? (5V)\n\n\n\n\n4.1 Arduino Pros\n\nLow cost.\nVery low power consumption (ideal for battery use).\nNo need for operating system.\nBest at single tasks (multi-threading possible).\nProvides analog pins.\nEasy learning curve.\n\n\n\n4.2 Raspberry Pi Pros\n\nFull Linux based OS.\nLots of computing power (32 bit and 64 bit).\nVideo and USB controllers onboard.\n40 GPIOs (but none are analog).\nIntegrated wifi, bluetooth and ethernet connectivity.",
    "crumbs": [
      "Microcomputers"
    ]
  },
  {
    "objectID": "notes/microcomputers/index.html#references",
    "href": "notes/microcomputers/index.html#references",
    "title": "Microcomputers",
    "section": "5 References",
    "text": "5 References\nA deeper dive into IoT by Microsoft.\nRaspberry Pi vs Arduino: Which Board is Best? by Tom’s Hardware.\nArduino vs Raspberry Pi for Robotics by Learn Robotics\n\n5.1 Video reference",
    "crumbs": [
      "Microcomputers"
    ]
  },
  {
    "objectID": "notes/python-scripting/index.html",
    "href": "notes/python-scripting/index.html",
    "title": "Python scripting",
    "section": "",
    "text": "This section explores intermediate Python topics to improve our technique at specific linux/IoT scripting tasks.\nWe’ll explore:\n\nPassing terminal arguments to a script\n__main__ and top-level environment\nRunning modules as scripts using __main__\nTip for managing gpio dependencies",
    "crumbs": [
      "Python scripting"
    ]
  },
  {
    "objectID": "notes/python-scripting/index.html#overview",
    "href": "notes/python-scripting/index.html#overview",
    "title": "Python scripting",
    "section": "",
    "text": "This section explores intermediate Python topics to improve our technique at specific linux/IoT scripting tasks.\nWe’ll explore:\n\nPassing terminal arguments to a script\n__main__ and top-level environment\nRunning modules as scripts using __main__\nTip for managing gpio dependencies",
    "crumbs": [
      "Python scripting"
    ]
  },
  {
    "objectID": "notes/python-scripting/index.html#passing-script-arguments",
    "href": "notes/python-scripting/index.html#passing-script-arguments",
    "title": "Python scripting",
    "section": "2 Passing Script Arguments",
    "text": "2 Passing Script Arguments\nIt’s possible to pass command line arguments to a Python script directly from your shell:\n$ python myscript.py first 2 True\nThe arguments passed are first, 2 and True .\n\n2.1 sys.argv\nThe Python sys module provides access to these arguments via sys.argv:\n\nsys.argv is a Python list of arguments.\n\n# myscript.py\n\nprint(f\"Argument List: {sys.argv}\")\nOutput:\n$ python myscript.py first 2 True\n\nArgument List: ['myscript.py', 'first', '2', 'True']\nNotice the following:\n\nThe first element in the list is the name of the script.\nArguments are available as strings.\n\nUnfortunately, we can’t trust that the user will always pass arguments in the correct order and using appropriate data types. We would still need to parse the arguments and make sure they are valid.\nFortunately, there is a built-in module that can help us do that.\n\n\n2.2 argparse\nargparse is a built-in module that makes it easy to write user-friendly command-line interfaces.\nOnce the script defines what arguments are required, argparse will figure out how to parse those out of sys.argv. The argparse module also automatically generates help and usage messages, and issues errors when users give the program invalid arguments.\n\nFor a basic tutorial of argparse, see this page.\n\nIn its simplest form, argparse must be imported and a parser must be instantiated:\n# myscript.py\n\nimport argparse\n\nparser = argparse.ArgumentParser()\nargs = parser.parse_args()\nprint(args)\nHowever, this will generate an error because we must tell argparse what argument flags and what data types to parse.\n$ python myscript.py first 2 True\n\nusage: myscript.py [-h]\nmyscript.py: error: unrecognized arguments: first 2 True\nTo “teach” argparse how to parse an argument we use the command parser.add_argument()\n# myscript.py\nimport argparse\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument(\"word\", type=str)\nparser.add_argument(\"number\", type=int)\nparser.add_argument(\"toggle\", type=bool)\n\nargs = parser.parse_args()\n\nprint(args)\nprint(args.word)\nprint(args.number)\nprint(args.toggle)\n$ python myscript.py first 2 True\n\nNamespace(word='first', number=2, toggle=True)\nfirst\n2\nTrue\nNotice how in the example above, all arguments were positional, which makes them mandatory. In other words, the order in which they are passed determines which variable they were being assigned to.\nIt’s also possible to make arguments optional. In this case, they must be specified with the correct “Flag”.\n# myscript.py\nimport argparse\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument(\"--word\", type=str)\nparser.add_argument(\"--number\", type=int)\nparser.add_argument(\"--toggle\", type=bool)\n\nargs = parser.parse_args()\n\nprint(args)\nprint(args.word)\nprint(args.number)\nprint(args.toggle)\n$ python myscript.py --word first --number 2\n\nNamespace(word='first', number=2, toggle=None)\nfirst\n2\nNone\n\n\n2.3 argparse References\n\nArticle 10 tips for passing arguments to Python script",
    "crumbs": [
      "Python scripting"
    ]
  },
  {
    "objectID": "notes/python-scripting/index.html#main__-top-level-environment",
    "href": "notes/python-scripting/index.html#main__-top-level-environment",
    "title": "Python scripting",
    "section": "3 __main__ & top-level environment",
    "text": "3 __main__ & top-level environment\nWhy include a if __name__ == \"__main__\": in your script?\nThere are two ways of executing Python code. Depending on how the code is executed, the global string variable __name__ will take one of two values:\n\nIf the script is run by the Python interpreter :\n\n__name__ has the value of __main__\n\nImporting script as a separate Python module:\n\n__name__ has the value of the module name.\n\n\nLet’s illustrate these two cases below.\n\nTL:DR\n\nOfficial docs:\n\n__main__— Top-level code environment\n\nSummary discussion at Stackoverflow:\n\nWhat does if name == “main”: do?\n\n\n\n\n3.1 Executing as a Script\nConsider the following script:\n# file my_script.py\n\nprint(\"Inside my_script.py, variable `__name__` is: \", __name__)\nExecuting this file with the Python interpreter:\n$ python my_script.py\n\n# Output\n# Inside my_script, variable `__name__` is: __main__\n\n\n3.2 Executing as a Module\nConsider a new file:\n# file your_script.py\n\nprint(\"Inside your_script.py, variable `__name__` is: \", __name__)\nNow modify my_script.py to import your_script.py:\n# file my_script.py\n\nimport your_script\n\nprint(\"Inside my_script.py, variable `__name__` is: \", __name__)\n\nWhen a file is imported as a module, it’s top-level code gets executed immediately.\n\nWhen we execute my_script.py again using the Python interpreter:\n$ python my_script.py\n\n# Output\n# Inside your_script, variable `__name__` is: your_script\n# Inside my_script, variable `__name__` is: __main__\nThe first print statement came from your_script.py when it was imported by my_script.py.\nNotice how inside your_script.py, the variable __name__ was the file name because it was being run as a module - from a different script (my_script.py).\n\n\n3.3 Top-level Code\nAll of the code that is at indentation level 0 gets executed is called the top-level.\n__main__ is the name of the environment where top-level code is run.\nIf a module is being run as a script (as in mys_cript.py above), then __name__ is instead set to the string \"__main__\".\nYou can test whether your script is being run directly or being imported by something else by testing what __name__ evaluates to:\n# another_script.py\n\ndef main():\n    # Include whatever code here.\n\nif __name__ == \"__main__\":\n    main()  # Will only run if executing file as a script.",
    "crumbs": [
      "Python scripting"
    ]
  },
  {
    "objectID": "notes/azure-features/index.html",
    "href": "notes/azure-features/index.html",
    "title": "Azure Features",
    "section": "",
    "text": "So far in this course, we have developed applications that are almost entirely local to our reTerminal (production) and workstation (development) environments.\nWe did not worry ourselves about data storage, application logic, authentication, etc. We could implement these things locally in Python, but we can also automate many of them using a PaaS framework.\nPlatform as a service (PaaS) is a complete development and deployment environment in the cloud:\n\nInfrastructure: servers, storage, and networking,\nPlatforms: development tools, business intelligence services (BI), database management systems, and more.\n\nIt’s really a reduced version of Software as a service (which you may be more familiar with), allowing the flexibility to write customized application/device software while benefitting from infrastructure and analysis services.\nSee the graphic below:\n\n\n\n\n\n\nFigure 1: Compasion of SaaS, PaaS and IaaS platforms. Figure from: SaaS vs PaaS vs IaaS, Microsoft.\n\n\n\nPaaS is designed to support the complete application life-cycle: building, testing, deploying, managing, and updating – while giving the developer flexibility to implement the actual services themselves.\nSee more in What is PaaS (azure.microsoft.com)",
    "crumbs": [
      "Azure Features"
    ]
  },
  {
    "objectID": "notes/azure-features/index.html#platform-as-a-service-paas",
    "href": "notes/azure-features/index.html#platform-as-a-service-paas",
    "title": "Azure Features",
    "section": "",
    "text": "So far in this course, we have developed applications that are almost entirely local to our reTerminal (production) and workstation (development) environments.\nWe did not worry ourselves about data storage, application logic, authentication, etc. We could implement these things locally in Python, but we can also automate many of them using a PaaS framework.\nPlatform as a service (PaaS) is a complete development and deployment environment in the cloud:\n\nInfrastructure: servers, storage, and networking,\nPlatforms: development tools, business intelligence services (BI), database management systems, and more.\n\nIt’s really a reduced version of Software as a service (which you may be more familiar with), allowing the flexibility to write customized application/device software while benefitting from infrastructure and analysis services.\nSee the graphic below:\n\n\n\n\n\n\nFigure 1: Compasion of SaaS, PaaS and IaaS platforms. Figure from: SaaS vs PaaS vs IaaS, Microsoft.\n\n\n\nPaaS is designed to support the complete application life-cycle: building, testing, deploying, managing, and updating – while giving the developer flexibility to implement the actual services themselves.\nSee more in What is PaaS (azure.microsoft.com)",
    "crumbs": [
      "Azure Features"
    ]
  },
  {
    "objectID": "notes/azure-features/index.html#iot-subsystems",
    "href": "notes/azure-features/index.html#iot-subsystems",
    "title": "Azure Features",
    "section": "2 IoT Subsystems",
    "text": "2 IoT Subsystems\nWe can breakdown an IoT solution into subsystems and explore how information flows between them:\n\nIoT Devices: The physical devices and sensors where data originates.\nCloud Gateway: The Cloud Gateway provides a cloud hub for secure connectivity, telemetry, event ingestion and device management (including command and control) capabilities.\nStream Processing: Processes large streams of data records, evaluates rules for those streams, and further routes the data.\nStorage: Storage can be divided into warm path (data that is required to be available for reporting and visualization immediately from devices), and cold path (data that is stored longer term and used for batch processing).\nUser Interface and Reporting: The user interface for an IoT application can be delivered on a wide array of device types, in native applications, and browsers.\nBusiness Process Integration: Facilitates executing actions based on insights garnered from device telemetry data during stream processing. Integration could include storage of informational messages, alarms, sending email or SMS, integration with CRM, and more.\n\n\n\n\n\n\n\nFigure 2: Diagram that shows the core subsystems of the Azure IoT reference architecture. Figure from: Subsystems of IoT architecture, Microsoft.\n\n\n\nWe will use Microsoft Azure to manage each sub-system individually.",
    "crumbs": [
      "Azure Features"
    ]
  },
  {
    "objectID": "notes/azure-features/index.html#azure-iot-services",
    "href": "notes/azure-features/index.html#azure-iot-services",
    "title": "Azure Features",
    "section": "3 Azure IoT Services",
    "text": "3 Azure IoT Services\nAzure offers many IoT-related services (see image below). In this course we’ll only explore a few.\n\n\n\n\n\n\nFigure 3: Azure IoT-related architecture. Data will flow from to/from your reTerminal (Devices), to/from the Azure IoT Hub (Ingestion & provisioning), to/from Azure Data Management (Warm path), to from your .NET mobile app (M&B integration). Figure from Microsoft Learn.",
    "crumbs": [
      "Azure Features"
    ]
  },
  {
    "objectID": "notes/azure-features/index.html#iot-hub-iot-central",
    "href": "notes/azure-features/index.html#iot-hub-iot-central",
    "title": "Azure Features",
    "section": "4 IoT Hub & IoT Central",
    "text": "4 IoT Hub & IoT Central\nMicrosoft Azure has two service offers for deploying and managing IoT systems: IoT Hub and IoT Central.\n\nThe page Overview: Connection options for Azure IoT device developers offers a great summary and comparison between IoT Hub and IoT Central.\n\n\n4.1 IoT Central\nAzure IoT Central is a software-as-a-service (SaaS) application that provides a complete platform for hosting IoT applications. Its main feature is a web UI that streamlines the lifecycle of creating and managing IoT applications.\nThe web UI simplifies the tasks of creating applications, and connecting and managing from a few up to millions of devices.\nThis service has a limited free plan (2 devices with 5000 messages per month). Pricing available here.\nSee official site for details or watch the walk-through for an example.\n\nWe will not use IoT Central in this course because we’ll connect the underlying Azure services ourselves.\n\n\n\n4.2 IoT Hub\nAzure IoT Hub is a platform-as-a-service (PaaS) application that also provides a platform for hosting IoT applications. IoT Hub acts as a central message hub for bi-directional communication between IoT applications and connected devices.\nIoT Hub offers greater control and customization over your application design, and more developer tool options for working with the service. However, it requires more development time and slightly more management complexity.\nThis service also offers a more generous free plan (500 devices with 8000 messages per day).\nSee official site for details and pricing.\n\nWe will use IoT Hub in this course.",
    "crumbs": [
      "Azure Features"
    ]
  },
  {
    "objectID": "notes/azure-features/index.html#azure-resource-groups-zones",
    "href": "notes/azure-features/index.html#azure-resource-groups-zones",
    "title": "Azure Features",
    "section": "5 Azure Resource Groups & Zones",
    "text": "5 Azure Resource Groups & Zones\nTo use Azure IoT Hub, we need to creaet Azure resource groups.\nAn Azure resource group is a “container” that holds related resources for an Azure solution (such as an IoT Hub).\nThe resource group can include all or a subset of the resources for the solution. You decide how to allocate resources to resource groups based on what makes sense for your organization. Generally, group resources share the same lifecycle (deploy, update, and delete).\n\n5.1 Azure Regions\nAzure resources and their groups must be deployed to a particular region.\nAzure operates in multiple datacenters around the world. These datacenters are grouped in to geographic regions.\n\n\n\n\n\n\nFigure 4: Azure infrastructure map. You can find a current list of all Azure regions at the Zzure geographies web tool. Figure from Map of Azure Regions, Thomas Poppelgaard, 2017.\n\n\n\nCanada has two Azure regions:\n\nCanada Central: located in Toronto (with 3 zones).\nCanada East: located in Quebec City (no zones).\n\nThe US has a few nearby regions as well, in particular:\n\nCentral US: allows the use of advanced IoT features still in testing.",
    "crumbs": [
      "Azure Features"
    ]
  },
  {
    "objectID": "notes/azure-features/index.html#azure-iot-dev-tools",
    "href": "notes/azure-features/index.html#azure-iot-dev-tools",
    "title": "Azure Features",
    "section": "6 Azure IoT Dev Tools",
    "text": "6 Azure IoT Dev Tools\nAzure offers several developer tools to create, manage and connect to the IoT Hub service. We’ll be using two: the Azure Portal, and the Azure CLI.\nThere’s also a few extensions worth noting.\n\n6.1 Azure Portal\nThe Azure Portal is a browser-based portal for IoT Hub and devices.\n\nDocumentation\n\nAzure portal documentation\n\nQuick Start\n\nWhat is the Azure portal?\n\nIoT Example\n\nCreate an IoT hub with Azure portal\n\n\n\n\n\n\n\n\n\nFigure 5: The Azure Portal at https://portal.azure.com is a web app that can access and modify all of your IoT Hub data/configurations/actions/etc.\n\n\n\n6.1.1 Metrics\nOne very useful feature of Azure Portal is the data analysis and presentation software that comes built-in.\nWithout any extra configuration, all IoT commands and actions are logged and stored on the Azure Portal, and visualizations of data usage can be easily displayed for verification/debugging/analysis.\n\n\n\n\n\n\n\nFigure 6: An example of the Azure Metrics portal page. You can easily verify that the daily number of messages you’re sending is below the Free Tier limit of 8000 by checking this page when using your IoT Hub devices.\n\n\nTo find the metrics page for a given Azure resource, do the following:\n\nIn the left navigation menu on the Azure portal, select All Resources.\nSelect the link on your IoT hub.\nSelect Monitoring in the left pane of your IoT Hub, and find the Metrics submenu.\nIn the Scope field, enter your IoT hub name.\nIn the Metric Namespace field, select IoT Hub Standard Metrics.\nIn the Metric field, select the desired metric.\nHover your mouse pointer over the chart to see detailed information at a given time.\n\n\n\n\n6.2 Azure CLI\nAzure CLI is a terminal tool to manage Azure services offered as Bash and PowerShell shells.\n\n\n\n\n\n\nWarning\n\n\n\nAn old version of this document recommended using the official installation instructions. Unfortunately, these methods (using apt) depend on unmaintained packages. Instead, install azure-cli in a python virtual environment as described below.\n\n\n\nDocumentation\n\nAzure CLI documentation\n\nInstallation\n\nazure-cli is a python package we can install in a virtual environment:\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you’re having issues installing azure-cli, make sure your system python is up to date. Revist the course notes for installing developer environment dependencies, and make sure your system packages and python libraries are up to date. Then, delete your existing virtual environment and recreate it with the updated python.\n\n# Create a venv if you do not have one already\n$ python -m venv .venv\n(.venv) $ source .venv/bin/activate\n# Install dependencies and azure-cli\n(.venv) $ pip install uamqp\n(.venv) $ pip install azure-cli\n\n\n\n\n\n\nWarning\n\n\n\nIf you are having errors installing uamqp, try the following:\n(.venv) $ export CFLAGS=\"-Wno-error=incompatible-function-pointer-types\"\n(.venv) $ export CMAKE_POLICY_VERSION_MINIMUM=3.5\n(.venv) $ pip install uamqp\n\n\nYou can later add these packages to a requirements.txt or pyproject.toml file for projects where you use these tools regularly.\n\nQuick Start\n\nGet started with Azure CLI\n\nIoT Example\n\nCreate an IoT hub with CLI\n\n\n\n6.2.1 Azure CLI IoT extension\nAzure CLI has a variety of installable extensions to manage different Azure services.\nThe Azure IoT extension gives us access to the az iot subcommand – a very convenient CLI for controlling IoT-related sevices.\n\nGitHub Repo\n\nazure-iot-cli-extension\n\nDocumentation\n\nList of az iot hub commands",
    "crumbs": [
      "Azure Features"
    ]
  },
  {
    "objectID": "notes/azure-features/index.html#some-tutorials-for-reference",
    "href": "notes/azure-features/index.html#some-tutorials-for-reference",
    "title": "Azure Features",
    "section": "7 Some tutorials for reference",
    "text": "7 Some tutorials for reference\nWe cover these tutorials in Lab 6, but here they are again for reference:\n\nQuickstart: Send telemetry from a device to an IoT hub and monitor it with the Azure CLI\nQuickstart: Send telemetry from an IoT Plug and Play device to Azure IoT Hub",
    "crumbs": [
      "Azure Features"
    ]
  },
  {
    "objectID": "notes/developer-environment/index.html",
    "href": "notes/developer-environment/index.html",
    "title": "Developer environment setup",
    "section": "",
    "text": "Photo by Tima Miroshnichenko",
    "crumbs": [
      "Developer environment setup"
    ]
  },
  {
    "objectID": "notes/developer-environment/index.html#overview",
    "href": "notes/developer-environment/index.html#overview",
    "title": "Developer environment setup",
    "section": "1 Overview",
    "text": "1 Overview\nThroughout this semester, we will make regular use of bash, python, git, and other 3rd party command line tools such as azure-cli and gh-cli.\nNo matter what hardware you have available at home, everyone should be comfortable completing coding assignments on their personal computers and on classroom computers.\nEveryone will need the following set up:\n\non a classroom computer:\n\na Debian WSL container with all class dependencies installed using apt\n\non personal computers:\n\nIf Windows: a Debian WSL container with all class dependencies installed using apt\nIf macOS: all class dependencies installed using brew\nIf Linux: all class dependencies installed using distribution package manager\n\n\nThe sections below show how to do that, and how to verify the installation, in each case.",
    "crumbs": [
      "Developer environment setup"
    ]
  },
  {
    "objectID": "notes/developer-environment/index.html#classroom-computers-linux-wsl",
    "href": "notes/developer-environment/index.html#classroom-computers-linux-wsl",
    "title": "Developer environment setup",
    "section": "2 Classroom computers: Linux WSL",
    "text": "2 Classroom computers: Linux WSL\nIf using a Windows machine (lab computers and/or personal) for this course, you will need to set up Windows Subsystem for Linux (WSL).\n\n2.1 Ensure necessary Windows software installed\nThese programs should already be installed on your Windows machine, but in case they are not:\n\nLink for installing git on Windows\nLink for installing Windows Terminal\n\n\n2.1.1 VS Code Extensions\nIf you have not already, install VSCode.\nThen, install the following extensions:\n\nRemote Development extension pack by Microsoft\nPython language support extension\nPython formatter/linter extension (ruff)\n\n\n\n\n2.2 Install Debian WSL\n\n\nPowershell\n\n# Verify that Debian is an available OS to install\nPS &gt; wsl --list --online # Debian should be one of the results\n\n# Install Debian\nPS &gt; wsl --install -d Debian\n\nYou will be prompted to create a username and password:\n\n\nPowershell\n\n# Recommended: All lower case. Something easy to type, e.g. your first name\nEnter new UNIX username:\n# Recommended: Don't overthink this, you can always change this later\nNew password:\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you forget the password for your WSL container, you can easily reset it.\nSee the Microsoft article Set up your linux username and password.\n\nAfter this, your installation is complete.\nSee the following links for more details if needed:\n\nInstall Debian on WSL\nChoosing Debian as the Linux distribution\nTroubleshooting WSL installation\n\n\n\n2.3 Configure terminal to use WSL\nFollow the steps in Set up Windows Terminal, particularly:\n\n\n\n\n\nWe’re going to spend a lot of time in terminal environments – you might as well enjoy using it. I find it motivating to use terminals that look and feel good to use. Image source\n\n\n\nEnsure your Debian WSL instance is the default profile\n\nThen, pin Windows Terminal to your taskbar, ideally as the first app\nUse Win+1 to open Windows Terminal automatically.\n\nChoose a theme\nChoose a color scheme\n\nCustomizing a color scheme\n\nPractise searching through terminal output using Ctrl+Shift+F\nMake sure you know how to copy/paste text in Windows Terminal\n\nYou can also use Ctrl+Shift+c and Ctrl+Shift+v to copy/paste in terminals\nYou can also use Ctrl+Insert and Shift+Insert to copy paste in terminals\n\nUse Ctrl+Shift+P to open the command palette to do almost any terminal config command (very similar to VSCode).\n\nThis is very useful for learning hotkeys for the following things:\n\nmaking terminal panes\nchanging focus\n\n\n\nSee Troubleshooting Windows Terminal for more details.\n\n\n2.4 Install dependencies\n\n2.4.1 Perform system update\n\n\nbash\n\n# Update system:\nsudo apt update && sudo apt upgrade -y\n\n\n\n2.4.2 Install python\n\n\nbash\n\n# Install python3 as the default python\nsudo apt install python3 python-is-python3\n\n# Verify default python version is &gt;= 3.9:\n# NOTE: python-is-python3 makes \"python\" the same as \"python3\"\npython --version\npython3 --version\n\n# Ensure pip is installed:\nsudo apt install python3-pip\npip --version\n\n# Ensure the python module venv is installed\nsudo apt install python3-venv\n\n# Ensure developer dependencies for python are installed\nsudo apt install python3-dev\n\n\n\n2.4.3 Set up git\nYou’ll need to do the following to set up git on both your WSL:\n\n\nbash\n\nsudo apt install git\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"youremail@domain.com\"\n\nSee Installing Git for more detail if needed.\n\n\n2.4.4 Install other needed tools\nWe’re going to need the following packages:\n\n\nbash\n\nsudo apt install man ssh wget ca-certificates rsync pass pass-extension-otp zbar-tools vim\n\nLast update to this command: May 19, 2025\n\n\n\n\n\n\nNote\n\n\n\nIf, when running sudo apt install, you have an error like this:\n\n\nbash\n\nE: Failed to fetch &lt;url&gt; 404 Not Found [IP: &lt;ip&gt;]\nE: Unable to fetch some archives, maybe run apt get update or try with --fix-missing?\n\nMake sure you update the system:\n\n\nbash\n\n# you can also run sudo apt upgrade -y, but it's not necessary all the time.\nsudo apt update\n\nThen, try the installation command again.\n\n\nI’ll keep this command updated throughout the semester as we encounter more packages we need.\n\n\n\n2.5 Backup container to OneDrive\nOnce the initial setup is complete, backups of the WSL container are easy to make.\nOnce a backup is made, it’s easy to:\n\nrecreate the exact same image on a new machine\nrestore your image in case the disk is wiped (this seems to be happening to our lab computers…)\n\n\n2.5.1 Backup command\nFirst, let’s ensure you have a folder to keep your backups.\nRecommendation: store WSL images on your college OneDrive account. That way, you can easily share your image with your personal computer, and restore your image automatically using any college computer.\n\n\nPowershell\n\nPS &gt; md -Force \"C:\\Users\\&lt;your-username&gt;\\OneDrive\\420-6P3-W25\"\n\nThen, we’ll use wsl --export to make a backup copy of your WSL container:\n\n\nPowershell\n\n# This can take around 5 minutes to finish.\nPS &gt; wsl --export Debian \"C:\\Users\\&lt;your-username\\OneDrive\\420-6P3-W25\\debian.tar\n\n\n\n2.5.2 Restore command\nOn a new machine (or on a machine with a freshly wiped hard drive…) you can --import the backup image you created:\n\n\nPowershell\n\n# This can take around 5 minutes to finish.\nPS &gt; wsl --import Debian .\\Debian \"C:\\Users\\&lt;your-username&gt;\\OneDrive\\debian.tar\"\n\n\n\n\n\n\n\nNote\n\n\n\nAfter restoring WSL, you will find that you are automatically logged in as root instead of your username.\nThe way to set a default user in a WSL container instance is to create a [user] entry in the container’s /etc/wsl.conf file:\nOpen your wsl instance and add the following entry to /etc/wsl.conf:\n\n\n/etc/wsl.conf\n\n[user]\ndefault=username\n\nExit your distro/instance, then run a wsl --terminate &lt;distroname&gt; from PowerShell.\nWhen you restart, the default user should be set to username.\nFor more detail: https://superuser.com/a/1627461",
    "crumbs": [
      "Developer environment setup"
    ]
  },
  {
    "objectID": "notes/developer-environment/index.html#personal-computers",
    "href": "notes/developer-environment/index.html#personal-computers",
    "title": "Developer environment setup",
    "section": "3 Personal computers",
    "text": "3 Personal computers\n\n3.1 Windows (WSL)\nAfter setting up WSL on a classroom computer, and backing up your WSL to OneDrive, the easiest way to set up WSL on your personal computer is to import your backup WSL image on your personal computer.\nNote that any changes made to either container, after the import, will not be automatically synchronized.\nIf you are making many customizations, you might want to keep your backup up-to-date.\n\n\n3.2 macOS / Linux\nOn a terminal on your computer, install the packages below.\nOn OS X we’ll use brew, on Linux you can use your system’s package manager:\n\n\nbash\n\n# Update system:\nbrew update && brew upgrade\n# Verify python version is &gt;= 3.9:\npython3 --version\n# Ensure pip is installed:\npython3 -m pip install --upgrade pip\n\n# Install other dependencies\nbrew install wget ca-certificates rsync pass pass-otp zbar vim\n\nLast update to this command: May 19, 2025\nAlso ensure you have installed VSCode and configured its extensions",
    "crumbs": [
      "Developer environment setup"
    ]
  },
  {
    "objectID": "notes/developer-environment/index.html#verify-environment",
    "href": "notes/developer-environment/index.html#verify-environment",
    "title": "Developer environment setup",
    "section": "4 Verify environment",
    "text": "4 Verify environment\nYour developer environment, whether on WSL, macOS, or Linux, should be able to run the following commands with the following results:\n\n\nbash\n\n# Verify python version is &gt;= 3.9 and pip is installed\npython3 --version\npip3 --version # or pip --version\n\n# Verify git config set up: ensure the output makes sense for you\ngit config user.name\ngit config user.email\n\n# For the following no specific version is required\n# but, these commands should not fail (show error message or exit error code 1)\nman -V\nssh -V\nrsync --version\npass --version\npass otp --version\nzbarimg --version\nvim --version",
    "crumbs": [
      "Developer environment setup"
    ]
  },
  {
    "objectID": "notes/iot-protocols/index.html",
    "href": "notes/iot-protocols/index.html",
    "title": "IoT Communication Protocols",
    "section": "",
    "text": "IoT devices typically exchange data with a cloud service using a specific communication protocol.\nThis lecture looks at common communication protocols used in IoT with a focus on MQTT.",
    "crumbs": [
      "IoT Communication Protocols"
    ]
  },
  {
    "objectID": "notes/iot-protocols/index.html#http",
    "href": "notes/iot-protocols/index.html#http",
    "title": "IoT Communication Protocols",
    "section": "1 HTTP",
    "text": "1 HTTP\nHTTP is a common protocol for the web, however, some of its characteristics make it not ideal for IoT applications, as the next section explains.\n\n1.1 Typical HTTP request\n\n\n Example of an HTTP Request   cheapsslsecurity.com.\n\n\n\n1.2 Typical HTTP Response\n\n\n Example of an HTTP Response   cheapsslsecurity.com.\n\nBelow is the HTTP response header for the posting of AIO data above (1 KB long)\n\n\n1.3 HTTP Characteristics\nThe following HTTP characteristics are relevant in the IoT context:\n\nRequest response protocol: client always has to request an update from the server.\nDesigned for large bandwidth, low delay.\nStateless: each request is considered as the new request. Server doesn’t recognise the user by default (cookies or tokens are required).\nResource identification: each request includes a URI (universal resource identifier).\nDNS lookup: 3-way identification handshake (overhead).\nBig protocol headers: uncompressed content transfer that is not the payload.\n\nIn HTTP, the client has to be constantly “pulling” the server for new information. Pull updates are either slow (check only every few minutes) or data/power intensive (check constantly).\n\n\n\ncustomer___partner_projects_adafruit_io_polling.png\n\n\n\n HTTP Client constantly pulling the server for updates   HTTP Protocol, Adafruit.",
    "crumbs": [
      "IoT Communication Protocols"
    ]
  },
  {
    "objectID": "notes/iot-protocols/index.html#pubsub-communication",
    "href": "notes/iot-protocols/index.html#pubsub-communication",
    "title": "IoT Communication Protocols",
    "section": "2 Pub/Sub Communication",
    "text": "2 Pub/Sub Communication\nThe most popular communication pattern for IoT systems is a publish/subscribe (pub/sub) messaging system done via some kind of broker.\nThis IoT system has two types of actors:\n\nOne or more Clients\nOne Broker\n\nIn the image below, a client device is publishing messages to the topic /telemetry while a client in the cloud is subscribed to the same topic.\n\n\n\nIoT device publishing telemetry on the /telemetry topic, and the cloud service subscribing to that topic\n\n\nIt’s very typical that any particular client is both the publisher and the subscriber.\n\n\n\nIoT devices connect to a broker and publish telemetry and subscribe to commands. Cloud services connect to the broker and subscribe to all telemetry and send commands to specific devices.\n\n\nFigures from: Broker and client relationship, IoT for Beginners, Microsoft.\n\n2.1 Clients\nClients are typically devices, applications or back-end services. They connect to the broker and publish telemetry as well as subscribe to commands.\n\nA cloud service is an example of a client.\nIt connects to the broker and subscribes to all the telemetry messages and publishes commands either to specific devices, or to groups of devices.\n\nThe client performs two operations:\n\nPublishes information: When the client sends the data to the server.\nSubscribes to information: When the client receives the data from the server.\n\n\n\n2.2 Broker\nThe Broker is typically running on a server and performs the following operations:\n\nAccepts messages from clients.\nProcesses subscribe and unsubscribe requests.\nForwards messages to specific clients according to subscription requests.\n\n\nMQTT is the most popular communication protocol for IoT devices",
    "crumbs": [
      "IoT Communication Protocols"
    ]
  },
  {
    "objectID": "notes/iot-protocols/index.html#mqtt",
    "href": "notes/iot-protocols/index.html#mqtt",
    "title": "IoT Communication Protocols",
    "section": "3 MQTT",
    "text": "3 MQTT\nMessage Queueing Telemetry Transport, i.e. MQTT, is a lightweight pub/sub protocol. It was originally designed to monitor oil pipelines where connectivity could not be guaranteed.\nOne or more clients connect to a single broker, who routes messages to the interested clients.\nMQTT manages messages asynchronously over a network socket.\nThe broker can hold and forward messages from client to client so if one gets disconnected, it will be able to fetch the message when it reconnects later.\n\nFacebook Messenger uses MQTT and is the World’s largest MQTT network.\nInfrastructure details here (nice read!).\n\n\n3.1 Topics & Wildcards\nMessages are routed using named topics, rather than being sent directly to an individual client. A client can publish to a topic, and any clients subscribed to that topic will receive the message.\nIn the example below, a client device is publishing some data (not shown) to the topic /telemetry. Immediately after the data is received by the broker, the broker routes the message to a client application previously subscribed to the same topic.\n\n\n\nIoT device publishing telemetry on the /telemetry topic, and the cloud service subscribing to that topic\n\n\n MQTT Topics   IoT for Beginners, Microsoft.\n\nTopics typically have a hierarchy to keep data organized and intuitive to access.\n\nFor example, a factory might want to track temperature from different HVAC units on different floor.\nThe topic hierarchy could be set up in the following way:\nsensors/&lt;FLOOR&gt;/temperature/&lt;HVAC_UNIT_ID&gt;\nHVAC units would publish data to the following topics:\n\nThe AC unit on the third floor: sensors/floor3/temperature/AC1\nThe boiler unit on the tenth floor: sensors/floor10/temperature/boiler\nThe heater unit on the fourth floor: sensors/floor4/temperature/heater\n\n\n3.1.1 Wildcards\nClients can subscribe to different levels of the topic hierarchy using wildcards.\nWhen subscribing to multiple topics two wildcard characters can be used:\n\n# (hash character) – multi level wildcard\n+ (plus character) -single level wildcard\n\nUsing the previous factory temperature example, an application could monitor multiple temperatures in the following way:\n\n\n\n\n\n\n\nTopic string\nSubscription result\n\n\n\n\nsensors/floor3/temperature/+\nAll of the temperature sensors on the 3rd floor\n\n\nsensors/+/temperature/+\nAll the temperature data on all floors\n\n\nsensors/#\nAll topics going on all floors\n\n\nsensors/floor10/#\nAll topics of the tenth floor\n\n\n\n\n\n\n3.2 Messages\nLike to HTTP, MQTT is built on top of TCP/IP and can also be used over websockets.\nMQTT messages can be as short as 2 bytes (eg. acknowledgement or disconnect messages), as big as 256Mb (eg. publishing a picture or firmware update).\nA typical telemetry message with payload data such as value=40 to a topic /assign1-temp has a size around 20 to 40 bytes.\n\nMessage headers and topic names are text encoded (UTF-8).\nMessage payload encoding is specific to the application (eg. plain text, JSON, binary, etc)\n\nMQTT connections can be public and open, or encrypted and secured using usernames and passwords, or certificates.\n\n\n3.3 QoS\nMessages can be sent with a quality of service (QoS) flag, which determines the guarantee of the message being received.\n\nQoS 0: At most once (Fire and Forget) - the message is sent only once and the client and broker take no additional steps to acknowledge delivery (fire and forget).\nQoS 1: At least once - the message is re-tried by the sender multiple times until acknowledgement is received (acknowledged delivery).\nQoS 2: Exactly once - the sender and receiver engage in a two-level handshake to ensure only one copy of the message is received (assured delivery).\n\nBelow is an example of QoS 1:\n\n\n\ncustomer___partner_projects_adafruit_io_puback.png\n\n\n\n Example of ‘At Least Once’ (QoS 1) delivery   MQTT QoS, Adafruit.\n\n\nBy default MQTT sessions are established with QoS 0 for all messages (clean sessions).\nThis means that if a client disconnects and then reconnects, it won’t receive messages sent during the disconnection.\n\nWhen setting QoS, there are two sides of message delivery:\n\nMessage delivery from the publishing client to the broker.\nMessage delivery from the broker to the subscribing client.\n\nThe client that publishes the message to the broker defines the QoS level of the message when it sends the message to the broker.\nThe broker transmits this message to subscribing clients using the QoS level that each subscribing client defines during the subscription process.\n\nIf the subscribing client defines a lower QoS than the publishing client, the broker transmits the message with the lower quality of service.\n\n\n\n3.4 Sessions: Clean or Persistent\nTypically, a client-broker connection is established with the session flag set to Clean with CleanSession = true (however, this is specific to the library implementation).\nIn this case the broker does not store anything for the client and purges all information from any previous subscriptions.\nIn a persistent session (CleanSession = false), the broker stores all subscriptions for the client and all missed messages for the client IF it subscribed with a QoS level 1 or 2.\n\n\n3.5 KeepAlive\nThe keep alive flag is a time interval in seconds that the client specifies and communicates to the broker when the connection established.\nThis is longest period of time that the broker and client can go without pinging each other. This method allows both sides to determine if the session is still available.\n\n\n3.6 Last Will & Testament\nWhen a client connects, it can provide the broker with a Last Will and Testament (LWT) message and topic.\nIf the client disconnects unexpectedly, the broker sends the LWT message on behalf of the client.\nThis message notifies other clients when a client disconnects unexpectedly.\n\n\n Last Will & Testament   Why HTTP isn’t the King of the IoT, Robert Bird, Akamai.\n\n\n\n3.7 Retained Messages\nA retained message is a normal MQTT message with the retained flag set to true.\nThe broker stores the last retained message and the corresponding QoS for that topic.\nEach client that subscribes to a topic pattern that matches the topic of the retained message receives the retained message immediately after they subscribe.\nThe broker stores only one retained message per topic.\n\nRetained messages help newly-subscribed clients get a status update immediately after they subscribe to a topic.\n\n\n\n Message with Retained Flag   Why HTTP isn’t the King of the IoT, Robert Bird, Akamai.",
    "crumbs": [
      "IoT Communication Protocols"
    ]
  },
  {
    "objectID": "notes/iot-protocols/index.html#http-vs-mqtt",
    "href": "notes/iot-protocols/index.html#http-vs-mqtt",
    "title": "IoT Communication Protocols",
    "section": "4 HTTP vs MQTT",
    "text": "4 HTTP vs MQTT\n\n\n\n\n\n\n\n\nCriteria\nMQTT\nHTTP\n\n\n\n\nComplexity\nSimple and lightweight\nMore complex and slightly heavier\n\n\nConsumption\nMore energy efficient\nLess energy efficient\n\n\nCommunication direction\nBi-directional (client or broker can initiate)\nUni-directional (client must initiate)\n\n\nArchitecture\nPublish / Subscribe model\nRequest / Response Model\n\n\nSession\nCan keep connection open\nConnection closes after every request\n\n\nDelivery assurance\nReliable message delivery with QoS (Quality of Service)\nNo QoS option\n\n\nHeader size\nSmaller (~2 bytes)\nComparatively large size (~8 bytes)",
    "crumbs": [
      "IoT Communication Protocols"
    ]
  },
  {
    "objectID": "notes/iot-protocols/index.html#mqtt-client-examples",
    "href": "notes/iot-protocols/index.html#mqtt-client-examples",
    "title": "IoT Communication Protocols",
    "section": "5 MQTT Client Examples",
    "text": "5 MQTT Client Examples\n\nExamples of graphical MQTT clients:\n\nMQTT X (supports cleanSession = False)\nMQTT Explorer\nVS Code Extension VSMqtt t\n\n\nIn this course you will write your own MQTT client. The next sections show an example of a simple MQTT client using Python that publishes and subscribes to messages.\n\n5.1 Public Brokers\nSince a broker is necessary, we can use the following publicly available MQTT brokers:\n\ntest.mosquitto.org by Eclipse Foundation.\nbroker.emqx.io by EMQX.\n\n\nFor the client implementation we will use the Eclipse Paho MQTT Python Client. It is a popular choice for many IoT Cloud services such as Adafruit IO.\n\nDocumentation for the paho-mqtt client is available at the Pypi page.\n\n\nHowever, there are lots of other implementation options.\n\nThis link compiles a list of open-source MQTT client and broker implementations you can use in your projects.\n\n\n\n5.2 Publisher Code\nThe first example will only publish random data to the broker every 3 seconds.\n\nInstall the paho-python library.\n\npip install paho-mqtt\n\nCreate your script file. A few notes:\n\n\nCreate a unique client ID to avoid conflicts with other users (this is a public broker).\n\nhttps://test.mosquitto.org/\nCreate a unique ID using the Online GUID generator\n\n\ncommon_key = '329adb05-8b85-4ebe-8309-15c564503a1f'\n\nTo have a unique client name and topic, we will combine the key from above with the desired name and topic.\n\nclient_name = common_key + 'publisher'\ntopic_name = common_key + '/temperature'\n\nImport paho-python and the built-in json library (we’ll encode our data as JSON).\n\nimport paho.mqtt.client as mqtt\nimport json\n\nAll network operations (incoming and outgoing data) are processed with the loop() method every 3 seconds.\n\nComplete code:\n# Publish to data topic (fire & forget)\n\nimport paho.mqtt.client as mqtt\nimport json\n\nimport time\nimport random\n\nrnd = random.Random()\n\ncommon_key = 'd5a4d5e6-d597-4bd4-8196-5f51d12345'\ntopic_name = common_key + '/temperature'\nclient_name = common_key + 'publisher'\n\nmqtt_client = mqtt.Client(client_name)\nmqtt_client.connect('test.mosquitto.org')\n\nprint(\"MQTT connected!\")\n\nwhile True:\n\n    # Random integer between -10 and 30\n    temp = rnd.randint(-10, 30)\n\n    # Prepare json payload\n    payload = json.dumps({'temperature': temp})\n    print(\"Sending telemetry \", payload)\n\n    # Send it off. QoS=0 and Retain=false\n    info = mqtt_client.publish(topic_name, payload)\n\n    # Process network event. Handles incoming/outgoing data\n    mqtt_client.loop()\n\n    time.sleep(3)\n\n\n5.3 Subscriber Code\nThe MQTT client below subscribes to data from the publisher using the library’s default values.\nA few notes:\n\nThe subscriber used the same unique key as the publisher in order to subscribe to the same topics. However, the subscriber has its own client name.\n\ncommon_key = 'd5a4d5e6-d597-4bd4-8196-5f51d12345'\ntopic_name = common_key + '/temperature'\nclient_name = common_key + 'subscriber'\n\nOnce the client receives a message from the broker, the method on_message is called. This is used as a hook to define our own callback function.\n\n# Callback for when a message is received\ndef message_received(mqtt_client, userdata, message):\nprint(f'Received message: {message.payload}')\n\n# Callback hook\nmqtt_client.on_message = message_received\n\nSimilarly to the publisher example, the network loop is processed every 1 second.\n\nThe complete subscriber code:\n# Subscribe to a topic with library's default values.\n\nimport paho.mqtt.client as mqtt\nimport json\n\nimport time\nimport random\n\ncommon_key = 'd5a4d5e6-d597-4bd4-8196-5f51d12345'\nclient_name = common_key + 'subscriber'\ntopic_name = common_key + '/temperature'\n\nmqtt_client = mqtt.Client(client_name)\nmqtt_client.connect('test.mosquitto.org')\nprint(\"MQTT connected!\")\n\n\n# Callback for when a message is received\ndef message_received(mqtt_client, userdata, message):\n    print(f'Received message: {message.payload}')\n\n\n# Callback hook\nmqtt_client.on_message = message_received\n\nmqtt_client.subscribe(topic_name)\n\nwhile True:\n\n    # Process network event. Handles incoming/outgoing data\n    mqtt_client.loop()\n\n    time.sleep(1)",
    "crumbs": [
      "IoT Communication Protocols"
    ]
  },
  {
    "objectID": "notes/iot-protocols/index.html#connectivity-issues",
    "href": "notes/iot-protocols/index.html#connectivity-issues",
    "title": "IoT Communication Protocols",
    "section": "6 Connectivity Issues",
    "text": "6 Connectivity Issues\nConnectivity is not always guaranteed. Things to think about when coding your application:\nWhat happens if:\n\nConnection is lost just prior to telemetry being published?\nClient connection is lost just prior to broker forwarding messages?\n\nHow to make sure that:\n\nThe broker has received the most up to date telemetry data?\nClient has the most recent command/data?",
    "crumbs": [
      "IoT Communication Protocols"
    ]
  },
  {
    "objectID": "notes/iot-protocols/index.html#references-resources",
    "href": "notes/iot-protocols/index.html#references-resources",
    "title": "IoT Communication Protocols",
    "section": "7 References & Resources",
    "text": "7 References & Resources\n\n\n\n\nAll the Internet of Things - Episode Two: Protocols, by Adafruit and Digikey\nLesson 4: connect your device to the Internet, IoT for Beginners by Microsoft\nMQTT Client and Broker and MQTT Server and Connection Establishment Explained by HiveMQ\n\n7.1 Diving Deeper\nMQTT Packet Format by OpenLab\nUnderstanding the MQTT Protocol Packet Structure by steves-internet-guide.com",
    "crumbs": [
      "IoT Communication Protocols"
    ]
  },
  {
    "objectID": "notes/python-collections/index.html",
    "href": "notes/python-collections/index.html",
    "title": "Python collection types and operations",
    "section": "",
    "text": "Image: https://realpython.com/python-lambda/",
    "crumbs": [
      "Python collection types and operations"
    ]
  },
  {
    "objectID": "notes/python-collections/index.html#overview",
    "href": "notes/python-collections/index.html#overview",
    "title": "Python collection types and operations",
    "section": "1 Overview",
    "text": "1 Overview\n\nLoops\nList comprehensions\nGenerators\nLambdas\nDictionaries",
    "crumbs": [
      "Python collection types and operations"
    ]
  },
  {
    "objectID": "notes/python-collections/index.html#loops",
    "href": "notes/python-collections/index.html#loops",
    "title": "Python collection types and operations",
    "section": "2 Loops",
    "text": "2 Loops\nThere are two types of loops in Python, for and while.\n\n2.1 The for loop\nFor loops iterate over a given sequence, or iterator. Here is an example:\n\n\nPython\n\nprimes = [2, 3, 5, 7]\nfor prime in primes:\n    print(prime)\n\nFor loops can iterate over a sequence of numbers using the range function. range returns an iterator object which can be looped using the following syntax:\n\n\nPython\n\n# Prints out the numbers 0,1,2,3,4\nfor x in range(5):\n    print(x)\n\n# Prints out 3,4,5\nfor x in range(3, 6):\n    print(x)\n\n# Prints out 3,5,7\nfor x in range(3, 8, 2):\n    print(x)\n\n\n\n2.2 break and continue statements\nbreak is used to exit a for loop or a while loop, whereas continue is used to skip the current block, and return to the “for” or “while” statement. A few examples:\n\n\nPython\n\n# Prints out only odd numbers - 1,3,5,7,9\nfor x in range(10):\n    # Check if x is even\n    if x % 2 == 0:\n        continue\n    print(x)\n\n# Prints out 0,1,2,3,4\n\ncount = 0\nwhile True:  # we have while loops in Python too.\n    print(count)\n    count += 1\n    if count &gt;= 5:\n        break",
    "crumbs": [
      "Python collection types and operations"
    ]
  },
  {
    "objectID": "notes/python-collections/index.html#list-comprehensions",
    "href": "notes/python-collections/index.html#list-comprehensions",
    "title": "Python collection types and operations",
    "section": "3 List Comprehensions",
    "text": "3 List Comprehensions\nList Comprehensions is a very powerful tool, which creates a new list based on another list, in a single, readable line.\nFor example, let’s say we need to create a list of integers which specify the length of each word in a certain sentence, but only if the word is not the word “the”.\n\n\nPython\n\nsentence = \"the quick brown fox jumps over the lazy dog\"\nwords = sentence.split()\nword_lengths = []\nfor word in words:\n    if word != \"the\":\n        word_lengths.append(len(word))\nprint(words)\nprint(word_lengths)\n\nUsing a list comprehension, we could simplify this process to this notation:\n\n\nPython\n\nsentence = \"the quick brown fox jumps over the lazy dog\"\nwords = sentence.split()\nword_lengths = [len(word) for word in words if word != \"the\"]\nprint(words)\nprint(word_lengths)",
    "crumbs": [
      "Python collection types and operations"
    ]
  },
  {
    "objectID": "notes/python-collections/index.html#generators",
    "href": "notes/python-collections/index.html#generators",
    "title": "Python collection types and operations",
    "section": "4 Generators",
    "text": "4 Generators\nGenerators are very easy to implement, but a bit difficult to understand.\nGenerators are used to create iterators, but with a different approach. Generators are simple functions which return an iterable set of items, one at a time, in a special way.\nWhen an iteration over a set of item starts using the for statement, the generator is run. Once the generator’s function code reaches a “yield” statement, the generator yields its execution back to the for loop, returning a new value from the set. The generator function can generate as many values (possibly infinite) as it wants, yielding each one in its turn.\nHere is a simple example of a generator function which returns 7 random integers:\n\n\nPython\n\nimport random\n\n\ndef lottery():\n    # returns 6 numbers between 1 and 40\n    for i in range(6):\n        yield random.randint(1, 40)\n\n    # returns a 7th number between 1 and 15\n    yield random.randint(1, 15)\n\n\nfor random_number in lottery():\n    print(\"And the next number is... %d!\" % (random_number))\n\nThis function decides how to generate the random numbers on its own, and executes the yield statements one at a time, pausing in between to yield execution back to the main for loop.",
    "crumbs": [
      "Python collection types and operations"
    ]
  },
  {
    "objectID": "notes/python-collections/index.html#lambda-functions",
    "href": "notes/python-collections/index.html#lambda-functions",
    "title": "Python collection types and operations",
    "section": "5 Lambda functions",
    "text": "5 Lambda functions\nNormally we define a function using the def keyword somewhere in the code and call it whenever we need to use it.\n\n\nPython\n\ndef sum(a, b):\n    return a + b\n\n\na = 1\nb = 2\nc = sum(a, b)\nprint(c)\n\nNow instead of defining the function somewhere and calling it, we can use python’s lambda functions, which are inline functions defined at the same place we use it. So we don’t need to declare a function somewhere and revisit the code just for a single time use.\nThey don’t need to have a name, so they also called anonymous functions. We define a lambda function using the keyword lambda.\n\n\nPython\n\nyour_function_name = lambda inputs: output\n\nSo the above sum example using lambda function would be,\n\n\nPython\n\na = 1\nb = 2\nsum = lambda x, y: x + y\nc = sum(a, b)\nprint(c)\n\nHere we are assigning the lambda function to the variable sum, and upon giving the arguments i.e. a and b, it works like a normal function.",
    "crumbs": [
      "Python collection types and operations"
    ]
  },
  {
    "objectID": "notes/python-collections/index.html#dictionaries",
    "href": "notes/python-collections/index.html#dictionaries",
    "title": "Python collection types and operations",
    "section": "6 Dictionaries",
    "text": "6 Dictionaries\n\nA dictionary is a data type similar to arrays, but works with keys and values instead of indexes. Each value stored in a dictionary can be accessed using a key, which is any type of object (a string, a number, a list, etc.) instead of using its index to address it.\nFor example, a database of phone numbers could be stored using a dictionary like this:\n\n\nPython\n\nphonebook = {}\nphonebook[\"John\"] = 938477566\nphonebook[\"Jack\"] = 938377264\nphonebook[\"Jill\"] = 947662781\nprint(phonebook)\n\nAlternatively, a dictionary can be initialized with the same values in the following notation:\n\n\nPython\n\nphonebook = {\"John\": 938477566, \"Jack\": 938377264, \"Jill\": 947662781}\nprint(phonebook)\n\n\n6.1 Iterating over dictionaries\nDictionaries can be iterated over, just like a list. However, a dictionary, unlike a list, does not keep the order of the values stored in it. To iterate over key value pairs, use the following syntax:\n\n\nPython\n\nphonebook = {\"John\": 938477566, \"Jack\": 938377264, \"Jill\": 947662781}\nfor name, number in phonebook.items():\n    print(\"Phone number of %s is %d\" % (name, number))\n\n\n\n6.2 Removing a value\nTo remove a specified index, use either one of the following notations:\n\n\nPython\n\nphonebook = {\"John\": 938477566, \"Jack\": 938377264, \"Jill\": 947662781}\ndel phonebook[\"John\"]\nprint(phonebook)\n\nor:\n\n\nPython\n\nphonebook = {\"John\": 938477566, \"Jack\": 938377264, \"Jill\": 947662781}\nphonebook.pop(\"John\")\nprint(phonebook)",
    "crumbs": [
      "Python collection types and operations"
    ]
  },
  {
    "objectID": "notes/serial-protocols/index.html",
    "href": "notes/serial-protocols/index.html",
    "title": "Serial protocols",
    "section": "",
    "text": "Many of the reTerminal’s General Purpose Input and Output (GPIO) pins also have specialized functions. These specialized functions typically include specific digital communication protocols:\n\nSerial\nSPI (serial peripheral interface)\nI2C (inter-integrated circuit)\nPWM (pulse-width modulation)\nPCM (pulse-code modulation)\n\nBelow is the pin out diagram illustrating the specialized pins:\n\n\n\nBreakout of 40-pin header for reTerminal\n\n\n\n GPIO and pin diagram of the reTerminal   - reTerminal Official Wiki, Seeed.",
    "crumbs": [
      "Serial protocols"
    ]
  },
  {
    "objectID": "notes/serial-protocols/index.html#serial-protocols",
    "href": "notes/serial-protocols/index.html#serial-protocols",
    "title": "Serial protocols",
    "section": "",
    "text": "Many of the reTerminal’s General Purpose Input and Output (GPIO) pins also have specialized functions. These specialized functions typically include specific digital communication protocols:\n\nSerial\nSPI (serial peripheral interface)\nI2C (inter-integrated circuit)\nPWM (pulse-width modulation)\nPCM (pulse-code modulation)\n\nBelow is the pin out diagram illustrating the specialized pins:\n\n\n\nBreakout of 40-pin header for reTerminal\n\n\n\n GPIO and pin diagram of the reTerminal   - reTerminal Official Wiki, Seeed.",
    "crumbs": [
      "Serial protocols"
    ]
  },
  {
    "objectID": "notes/serial-protocols/index.html#serial-communication",
    "href": "notes/serial-protocols/index.html#serial-communication",
    "title": "Serial protocols",
    "section": "2 Serial Communication",
    "text": "2 Serial Communication\nIn order for two devices to exchange information, they must share a common communication protocol.\nSerial interfaces stream their data, one bit at a time. These interfaces can operate with as little as one wire (for unidirectional communication), however they typically use 2 to 4 wires.\nSerial communication can be either: synchronous and asynchronous.\n\n2.1 Synchronous Serial\nA synchronous serial interface always pairs its data line(s) with a clock signal. Therefore, all devices on the same data bus share a common clock.\n\n\n\nSynonymous unidirectional serial communication with a clock line\n\n\n\n Serial interface unidirectionally transmitting one bit at every clock pulse   Serial Communication, Sparkfun.\n\n\nHow many wires are used in the image above? Can information flow in both directions?\n\nHaving a line dedicated to a clock makes for faster serial transfer, however, it also requires an extra wire between communicating devices.\nBelow are examples of synchronous digital protocols:\n\nSPI\nI2C\nUSB (uses clock-synchronization)\n\n\n\n2.2 Asynchronous Serial\nAsynchronous means that data is transferred without support from an external clock signal.\nThis transmission method minimizes wires and I/O pins, however, extra effort is put into reliably transferring and receiving data.\nAsynchronous serial communication is typically intended for only two devices to communicate\n\n\n\nWiring diagram of two serial communication devices\n\n\n\n Wiring diagram for two devices communicating with the Serial protocol   Serial Communication, Sparkfun.\n\n\nThis is the most common type of serial communication between devices.\nThe term “Serial” is commonly used to refer to Asynchronous Serial\n\nIn order to communicate reliably, both devices have to adhere to a number of rules such as: Data bits, Synchronization bits, Parity bits, and Baud rate.\nFor example, when using the serial monitor of the Arduino IDE, it’s necessary to properly select the Baud Rate so that both the client device (the Arduino) and the host (your PC) know the exactly clock frequency of the serial communication.\n\n\n\nSelecting the Baud rate in the Arduino serial monitor\n\n\n\n  Selecting the baud rate of the Arduino IDE’s serial monitor.",
    "crumbs": [
      "Serial protocols"
    ]
  },
  {
    "objectID": "notes/serial-protocols/index.html#uarts",
    "href": "notes/serial-protocols/index.html#uarts",
    "title": "Serial protocols",
    "section": "3 UARTs",
    "text": "3 UARTs\nA universal asynchronous receiver/transmitter (UART) is a block of circuitry responsible for implementing serial communication.\nA UART converts multiple parallel digital lines into two serial lines: Transmission (Tx) and Receiving (Rx) lines.\n\nThe Raspberry Pi has a built-in UART on GPIO14 (Tx) and GPIO15 (Rx).\n\nThe python library pySerial can be used to send and receive serial data to and from the Raspberry Pi:\nimport serial\nser = serial.Serial('/dev/ttyS0')  # open serial port (9600 default baud rate)\nprint(ser.name)         # check which port was really used\nser.write(b'hello')     # write a string\nser.close()             # close port\n\n3.1 Serial Demo\nThe demo below will show a reTerminal device devices communicating with a Raspberry Pi Pico over the asynchronous serial protocol.\nThe Rx port of the reTerminal is connected to the Tx port of the Pico and vice-versa.\nBelow is the code being run on the Pico using MicroPython:\nfrom time import sleep\nfrom machine import UART, Pin\n\nuart0 = UART(0, baudrate=9600, tx=Pin(0), rx=Pin(1))\nuart0.write('hello\\n')\n\nsleep(0.01)\n\nline = uart0.read()\nprint(f'My line: {line}')\nAnd this is the code running inside the reTerminal:\nimport serial\n\nwith serial.Serial('/dev/serial0', 9600) as ser:\n\n    while True:\n        line = ser.readline()   # read a '\\n' terminated line\n        print(f'Line: {line}')\n\n        if line == b'hello\\n':\n            break\n\n    ser.write(b'there\\n')\nOnce both scripts are run simultaneously, the signal observed would look the following:",
    "crumbs": [
      "Serial protocols"
    ]
  },
  {
    "objectID": "notes/serial-protocols/index.html#serial-peripheral-interface-spi",
    "href": "notes/serial-protocols/index.html#serial-peripheral-interface-spi",
    "title": "Serial protocols",
    "section": "4 Serial Peripheral Interface (SPI)",
    "text": "4 Serial Peripheral Interface (SPI)\nSerial Peripheral Interface (SPI) is commonly used to send data between microcontrollers and small peripherals (ei. shift registers, sensors, SD cards).\nIt uses separate clock and data lines, along with a select line to choose the device you wish to talk to.\nThe communication can happen between a controller device (controlling the terms of the communication) and one or multiple peripheral devices.\nThe following nomenclature is typically used:\n\nSCK: Clock Signal. Generated by the controller device.\nCOPI: Controller-Out Peripheral-In. Information flows from controller to peripheral device.\nCIPO: Controller-In Peripheral-Out. Information flows from peripheral to controller.\nCS: Chip Select. Every peripheral device has a unique connection to the controller. The controller uses this line to enable (wake-up) the peripheral when it wants to communicate by setting it low.\n\n\n\n\nWiring and bit exchange between SPI devices\n\n\n\n Wiring diagram for two devices communicating over SPI  Serial Peripheral Interface (SPI), Sparkfun.\n\nEach peripheral device connected to the controller will need a separate CS line. To talk to a particular peripheral, the controller makes that peripheral’s CS line low and keep the rest of them high.\n\n\n Multiple peripheral devices with unique CS lines talking to the same controller   Serial Peripheral Interface (SPI), Sparkfun.",
    "crumbs": [
      "Serial protocols"
    ]
  },
  {
    "objectID": "notes/serial-protocols/index.html#problematic-technical-terminology",
    "href": "notes/serial-protocols/index.html#problematic-technical-terminology",
    "title": "Serial protocols",
    "section": "5 Problematic Technical Terminology",
    "text": "5 Problematic Technical Terminology\nHistorically, the relationship between the Controller and Peripheral devices used to be called Master and Slave. This is incredibly problematic since the Master-Slave analogy is based on an extreme and violent power relationship between two individuals.\nThe Open Source Hardware Association (OSHA) has passed a resolution asking hardware manufactures to redefine SPI signal names. However, some legacy documentation and references still include the outdated terminology below.\nDeprecated signal names:\n\nMOSI – Master Out Slave In\nMISO – Master In Slave Out\nSS – Slave Select\nMOMI – Master Out Master In\nSOSI – Slave Out Slave In\n\nProblematic technical terminology is not unique to the SPI protocol and exists in many other technical domains. In 2022, Wire magazine published a nuanced article presenting multiple sides of the terminology debate: Tech Confronts Its Use of the Labels ‘Master’ and ‘Slave’.",
    "crumbs": [
      "Serial protocols"
    ]
  },
  {
    "objectID": "notes/serial-protocols/index.html#i2c",
    "href": "notes/serial-protocols/index.html#i2c",
    "title": "Serial protocols",
    "section": "6 I2C",
    "text": "6 I2C\nThe Inter-Integrated Circuit (I2C) Protocol is a protocol intended to allow multiple “peripheral” digital devices (chips) to communicate with one or more “controller” chip.\nI2C requires only two wires, however, those two wires can support up to 1008 peripheral devices.\nHardware required to implement I2C is more complex than SPI, but less than asynchronous serial. Data speeds are also faster than asynchronous serial but slower than SPI.\n\n\n\nWiring diagram of I2C devices connected\n\n\n\n Example wiring diagram for one controller and 3 peripheral devices   I2C, Wikipedia.\n\nIn I2C, each device has a unique identifier address (a hexadecimal number). When communication is initiated, the controller must announce the address of the target device.\nYou can check what are the I2C addresses of the peripherals attached to the reTerminal with the following bash command:\npi@raspberrypi:~ $ i2cdetect -y 1\n\n     0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f\n00:          -- -- -- -- -- -- -- -- -- -- -- -- --\n10: -- -- -- -- -- -- -- -- -- UU -- -- -- -- -- --\n20: -- -- -- -- -- -- -- -- -- UU -- -- -- -- -- --\n30: -- -- -- -- -- -- -- -- UU -- -- -- -- -- -- --\n40: -- -- -- -- -- UU -- -- -- -- -- -- -- -- -- --\n50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\n60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\n70: -- -- -- -- -- -- -- --\nThe output above shows peripheral devices with the hexadecimal addresses 0x19, 0x29, 0x38, 0x40.\nWhen communicating, messages are broken up into two types of frame:\n\nAn address frame, where the controller indicates the peripheral to which the message is being sent.\nOne or more data frames, which are 8-bit data messages passed from controller to peripheral or vice versa.\n\n\n\n Clock and data lines for I2C, showing address and data frames   I2C, Sparkfun.\n\nExamples of I2C devices used in this course:\n\nLCD driver of the reTerminal\nAHT20 I2C Temperature & Humidity Sensor\nreTerminal’s accelerometer.\nreTerminal’s light sensor\n\n\n6.1 I2C Python Library\nThe python library smbus2 supports I2C protocol.\nfrom smbus2 import SMBus\n\n# Open i2c bus 1\nwith SMBus(1) as bus:\n    # read one byte from address 80, offset 0\n    b = bus.read_byte_data(80, 0)\n    print(b)\n\n    # Write a byte to address 80, offset 0\n    data = 45\n    bus.write_byte_data(80, 0, data)\n\nEach peripheral device might require a series of initialization messages to be written to it. Typically you will use python libraries made to communicate with a specific peripheral device.\nAs an example, see the library for the AHT20 temperature sensor.",
    "crumbs": [
      "Serial protocols"
    ]
  },
  {
    "objectID": "notes/serial-protocols/index.html#pulse-width-modulation-pwm",
    "href": "notes/serial-protocols/index.html#pulse-width-modulation-pwm",
    "title": "Serial protocols",
    "section": "7 Pulse Width Modulation (PWM)",
    "text": "7 Pulse Width Modulation (PWM)\nPulse Width Modulation (PWM) is a type of digital signal. With PWM it’s possible to vary how much time the signal is high in an analog fashion.\nWhile the signal can only be high (usually 3.3V or 5V) or low (ground) at any time, we can change the proportion of time the signal is high compared to when it is low over a consistent time interval.\nThe duty cycle describes the amount of “ON time” as a percentage over an interval or period of time.\n\n\n\nDuty Cycle Examples.png\n\n\n\n Examples of duty cycles as a percentage of the total “High” signal   Pulse Width Modulation, Wikipedia.\n\n Signal with a constant amplitude (voltage) but duty cycle changing from 10% to 100%. MakerPortal.com\n\n7.1 Common PWM Applications\n\n7.1.1 Servo Motors\nFor servo motor positioning the width of the pulse indicates the position where the servo arm should be.\n Servo arm position changing according to the duty cycle of a PWM signal. 14Core.com\n\n\n7.1.2 LED Dimming\nLEDs are make to work with constant voltage (approximately 2 volts). It is not possible to dim their brightness by lowering the voltage (like in incandescent light bulb).\nHowever, with PWM, it is possible to change the amount of “ON time” to give the illusion that the LED is dimmer.\n Signal duty cycle affecting LED brightness. Pyroelectro.com\nIn reality, the LED is simply blinking so fast that the human eye cannot notice it.\n\n\n\n\n\n7.2 PWM Python Library\nThe python library GPIO Zero offers good support more a number of PWM devices:\n\nLED with variable brightness\nServo\n\n# LED intensity modulated with PWM\n\nfrom gpiozero import PWMLED\nfrom time import sleep\n\nled = PWMLED(17)\n\nwhile True:\n    led.value = 0  # off\n    sleep(1)\n    led.value = 0.5  # half brightness\n    sleep(1)\n    led.value = 1  # full brightness\n    sleep(1)\n# Servo position set with PWM\n\nfrom gpiozero import Servo\nfrom time import sleep\n\nservo = Servo(17)\n\nwhile True:\n    servo.min()\n    sleep(2)\n    servo.mid()\n    sleep(2)\n    servo.max()\n    sleep(2)",
    "crumbs": [
      "Serial protocols"
    ]
  },
  {
    "objectID": "notes/serial-protocols/index.html#references",
    "href": "notes/serial-protocols/index.html#references",
    "title": "Serial protocols",
    "section": "8 References",
    "text": "8 References\nSerial Communication Tutorial, Sparkfun.\nSerial Peripheral Interface (SPI), Sparkfun\nPulse Width Modulation, Sparkfun\nServo Control, Wikipedia\nRaspberry Pi: Python Libraries for I2C, SPI, UART by Sebastian via medium.com\nRaspberry Pi UART Communication using Python and C by ElectronicWings.com",
    "crumbs": [
      "Serial protocols"
    ]
  },
  {
    "objectID": "notes/serial-protocols/index.html#diving-deeper",
    "href": "notes/serial-protocols/index.html#diving-deeper",
    "title": "Serial protocols",
    "section": "9 Diving Deeper",
    "text": "9 Diving Deeper\nIf you want to know more about how USB uses clock synchronization even thought there is no clock wire:",
    "crumbs": [
      "Serial protocols"
    ]
  },
  {
    "objectID": "notes/index.html",
    "href": "notes/index.html",
    "title": "All course notes",
    "section": "",
    "text": "Table viewCard viewGrid view\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Modified - Oldest\n      \n      \n        Modified - Newest\n      \n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nSubtitle\n\n\n\nModified\n\n\n\n\n\n\n\n\nJan 20\n\n\nCourse hardware\n\n\nList of hardware available for the course as well as their respective specifications and documentation. \n\n\nMay 19\n\n\n\n\n\n\nJan 20\n\n\nDeveloper environment setup\n\n\nInstructions for setting up classroom and personal computers for programming in this course, including: setting up a Debian container on WSL; restoring your WSL container from backup quickly; installing needed dependencies on WSL, Mac/OSX, and Linux; verifying that needed dependencies have been installed. \n\n\nMay 19\n\n\n\n\n\n\nJan 31\n\n\nBash essentials\n\n\nTips for using the bash interactive shell CLI effectively. The core bash commands everyone should know. How to combine commands using pipes and redirects. \n\n\nMay 19\n\n\n\n\n\n\nFeb 10\n\n\nGitHub basics\n\n\nBasics of cloning a repository hosted on Github and creating branches. Using a personal access token for authentication on GitHub outside of VSCode. Using the pass password manager to store access tokens safely. \n\n\nMay 19\n\n\n\n\n\n\nFeb 17\n\n\nBash scripting\n\n\nTips for scripting in bash effectively. Managing environment and permissions in bash. The capabilities and limitations of programming in bash. \n\n\nMay 19\n\n\n\n\n\n\nFeb 24\n\n\nReterminal setup\n\n\nGuide to setting up reterminal for the first time. \n\n\nMay 19\n\n\n\n\n\n\nMar 10\n\n\nreTerminal built-in devices\n\n\nInstalling initial reTerminal packages. Reading and controlling the reTerminal hardware interfaces, e.g. touchscreen, LEDs, light sensor, screen backlight, etc.\n\n\nMay 19\n\n\n\n\n\n\nMar 10\n\n\nPython package management\n\n\nHow to use install system python packages using apt. How to install project python libraries using pip. Using virtual environments to manage python dependencies. \n\n\nMay 19\n\n\n\n\n\n\nMar 14\n\n\nGPIOs: Inputs and Outputs\n\n\nUnderstanding the General Purpose Input and Output (GPIO) pins our reTerminal makes available to us. Voltage, digital vs. analog, python libraries for GPIO.\n\n\nMay 19\n\n\n\n\n\n\nMar 14\n\n\nSerial protocols\n\n\nA deeper dive into the protocols used to communicate between GPIO and the kernel\n\n\nMay 19\n\n\n\n\n\n\nMar 14\n\n\nSignals: Analog and digital\n\n\nComparing the two paradigms for measuring electronic information and understanding how each are used for distributed applications.\n\n\nMay 19\n\n\n\n\n\n\nApr 4\n\n\nPython project management\n\n\nPython modules and packages. Python project file structure. Managing project dependencies. Configuring python project metadata. requirements.txt and pyproject.toml. Configuring python linters and formatters. \n\n\nMay 19\n\n\n\n\n\n\nApr 4\n\n\nAsynchronous progamming in Python\n\n\nUsing the asyncio library to control concurrentprocesses in Python.\n\n\nMay 19\n\n\n\n\n\n\nApr 4\n\n\nPython collection types and operations\n\n\nUseful python collection types and operations\n\n\nMay 19\n\n\n\n\n\n\nApr 4\n\n\nPython OOP\n\n\nBasics of Object Oriented Programming in Python.\n\n\nMay 19\n\n\n\n\n\n\nApr 4\n\n\nPython scripting\n\n\nTips and tricks for effective scripting in Python \n\n\nMay 19\n\n\n\n\n\n\nApr 13\n\n\nDebugging Reterminal Issues\n\n\nSometimes, the reTerminal display drivers fail. Here is an (incomplete) set of debugging steps you can take to try fixing the issue. \n\n\nMay 19\n\n\n\n\n\n\nApr 19\n\n\nAzure Features\n\n\nAn overview of the main feautres of Azure we will use for implementing an IoT system. Background information on “Platform as a Service” (PaaS), Azure IoT Hub vs IoT Central, Azure Resource Groups, Intro to Azure Dev Tools (Azure Portal, Azure CLI). \n\n\nMay 19\n\n\n\n\n\n\nApr 19\n\n\nAzure Portal Setup\n\n\nCreating an Azure account, managing your Azure account budget, Azure IoT Hub documentation, creating an Azure IoT Hub resource, and more. \n\n\nMay 19\n\n\n\n\n\n\nApr 21\n\n\nAzure SDKs in Python and C#\n\n\nAzure Libraries in Python and C# Installation instructions for WSL, how to use the libraries in your code. \n\n\nMay 19\n\n\n\n\n\n\nApr 21\n\n\nAzure CLI Cheatsheet\n\n\nReference for common Azure commands\n\n\nMay 19\n\n\n\n\n\n\nApr 28\n\n\nDevice to Cloud Communication\n\n\nHow to use Azure for bidrectional device-cloud communication\n\n\nMay 19\n\n\n\n\n\n\nApr 28\n\n\nLinux package management\n\n\nInstalling and configuring Linux packages on WSL / Raspberry Pi Debian using the apt package manager\n\n\nMay 19\n\n\n\n\n\n\nApr 28\n\n\nElectronics Basics\n\n\nReview of Ohm’s Law, fundamental concepts for understanding electronic circuits and interfaces.\n\n\nMay 19\n\n\n\n\n\n\nApr 28\n\n\nIntro to the Internet of Things\n\n\nFundamental concepts and applications behind the ‘Internet of Things’ paradigm.\n\n\nMay 19\n\n\n\n\n\n\nApr 28\n\n\nDevice Twins\n\n\nHow to retrieve and configure IoT Devices using Device Twin metadata\n\n\nMay 19\n\n\n\n\n\n\nApr 28\n\n\nAzure EventHubs\n\n\nMore notes on how to send and receive events in C#\n\n\nMay 19\n\n\n\n\n\n\nApr 28\n\n\nIoT Communication Protocols\n\n\nHow communication protocols can be optimized for device-cloud communication, i.e. why not everything should use HTTP\n\n\nMay 19\n\n\n\n\n\n\nApr 28\n\n\nMicrocomputers\n\n\nComparing the architectural paradigms for microcomputers to better understand our Pis.\n\n\nMay 19\n\n\n\n\n\n\nMay 5\n\n\nDevice calibration\n\n\nNotes for getting your sensors/actuators to work. \n\n\nMay 19\n\n\n\n\n\n\nMay 5\n\n\nPython extras\n\n\nPython tips and tricks that there wasn’t time to cover explicitly in the course, but are useful to know. Logging in Python with the logging module. Unit testing in python with pytest. \n\n\nMay 19\n\n\n\n\n\n\nMay 8\n\n\nMessage Routing\n\n\nHow to specify custom endpoints for azure messages\n\n\nMay 19\n\n\n\n\n\n\nMay 8\n\n\nAzure Blob Storage\n\n\nSetting up blob storage on the Azure Portal \n\n\nMay 19\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Modified - Oldest\n      \n      \n        Modified - Newest\n      \n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nCourse hardware\n\n\nList of hardware available for the course as well as their respective specifications and documentation. \n\n\n\nhardware\n\n\n\n\n\n\nJan 20, 2025\n\n3 min\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDeveloper environment setup\n\n\nInstructions for setting up classroom and personal computers for programming in this course, including: setting up a Debian container on WSL; restoring your WSL container from backup quickly; installing needed dependencies on WSL, Mac/OSX, and Linux; verifying that needed dependencies have been installed. \n\n\n\nbash\n\nlinux\n\n\n\n\n\n\nJan 20, 2025\n\n7 min\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBash essentials\n\n\nTips for using the bash interactive shell CLI effectively. The core bash commands everyone should know. How to combine commands using pipes and redirects. \n\n\n\nbash\n\n\n\n\n\n\nJan 31, 2025\n\n8 min\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nGitHub basics\n\n\nBasics of cloning a repository hosted on Github and creating branches. Using a personal access token for authentication on GitHub outside of VSCode. Using the pass password manager to store access tokens safely. \n\n\n\ngit\n\nbash\n\n\n\n\n\n\nFeb 10, 2025\n\n6 min\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBash scripting\n\n\nTips for scripting in bash effectively. Managing environment and permissions in bash. The capabilities and limitations of programming in bash. \n\n\n\nbash\n\n\n\n\n\n\nFeb 17, 2025\n\n21 min\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nReterminal setup\n\n\nGuide to setting up reterminal for the first time. \n\n\n\nhardware\n\nlinux\n\n\n\n\n\n\nFeb 24, 2025\n\n18 min\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nreTerminal built-in devices\n\n\nInstalling initial reTerminal packages. Reading and controlling the reTerminal hardware interfaces, e.g. touchscreen, LEDs, light sensor, screen backlight, etc.\n\n\n\nhardware\n\nlinux\n\nbash\n\n\n\n\n\n\nMar 10, 2025\n\n5 min\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPython package management\n\n\nHow to use install system python packages using apt. How to install project python libraries using pip. Using virtual environments to manage python dependencies. \n\n\n\npython\n\n\n\n\n\n\nMar 10, 2025\n\n11 min\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nGPIOs: Inputs and Outputs\n\n\nUnderstanding the General Purpose Input and Output (GPIO) pins our reTerminal makes available to us. Voltage, digital vs. analog, python libraries for GPIO.\n\n\n\nhardware\n\nsignals\n\n\n\n\n\n\nMar 14, 2025\n\n4 min\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nSerial protocols\n\n\nA deeper dive into the protocols used to communicate between GPIO and the kernel\n\n\n\nhardware\n\nsignals\n\n\n\n\n\n\nMar 14, 2025\n\n10 min\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nSignals: Analog and digital\n\n\nComparing the two paradigms for measuring electronic information and understanding how each are used for distributed applications.\n\n\n\nhardware\n\n\n\n\n\n\nMar 14, 2025\n\n9 min\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPython project management\n\n\nPython modules and packages. Python project file structure. Managing project dependencies. Configuring python project metadata. requirements.txt and pyproject.toml. Configuring python linters and formatters. \n\n\n\npython\n\n\n\n\n\n\nApr 4, 2025\n\n8 min\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAsynchronous progamming in Python\n\n\nUsing the asyncio library to control concurrentprocesses in Python.\n\n\n\npython\n\n\n\n\n\n\nApr 4, 2025\n\n4 min\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPython collection types and operations\n\n\nUseful python collection types and operations\n\n\n\npython\n\n\n\n\n\n\nApr 4, 2025\n\n5 min\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPython OOP\n\n\nBasics of Object Oriented Programming in Python.\n\n\n\npython\n\n\n\n\n\n\nApr 4, 2025\n\n7 min\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPython scripting\n\n\nTips and tricks for effective scripting in Python \n\n\n\npython\n\n\n\n\n\n\nApr 4, 2025\n\n4 min\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDebugging Reterminal Issues\n\n\nSometimes, the reTerminal display drivers fail. Here is an (incomplete) set of debugging steps you can take to try fixing the issue. \n\n\n\nlinux\n\nbash\n\n\n\n\n\n\nApr 13, 2025\n\n4 min\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAzure Features\n\n\nAn overview of the main feautres of Azure we will use for implementing an IoT system. Background information on “Platform as a Service” (PaaS), Azure IoT Hub vs IoT Central, Azure Resource Groups, Intro to Azure Dev Tools (Azure Portal, Azure CLI). \n\n\n\niot\n\n\n\n\n\n\nApr 19, 2025\n\n7 min\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAzure Portal Setup\n\n\nCreating an Azure account, managing your Azure account budget, Azure IoT Hub documentation, creating an Azure IoT Hub resource, and more. \n\n\n\niot\n\n\n\n\n\n\nApr 19, 2025\n\n6 min\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAzure SDKs in Python and C#\n\n\nAzure Libraries in Python and C# Installation instructions for WSL, how to use the libraries in your code. \n\n\n\npython\n\niot\n\n\n\n\n\n\nApr 21, 2025\n\n3 min\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAzure CLI Cheatsheet\n\n\nReference for common Azure commands\n\n\n\nbash\n\niot\n\n\n\n\n\n\nApr 21, 2025\n\n5 min\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDevice to Cloud Communication\n\n\nHow to use Azure for bidrectional device-cloud communication\n\n\n\niot\n\n\n\n\n\n\nApr 28, 2025\n\n7 min\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nLinux package management\n\n\nInstalling and configuring Linux packages on WSL / Raspberry Pi Debian using the apt package manager\n\n\n\nlinux\n\n\n\n\n\n\nApr 28, 2025\n\n6 min\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nElectronics Basics\n\n\nReview of Ohm’s Law, fundamental concepts for understanding electronic circuits and interfaces.\n\n\n\nhardware\n\nsignals\n\n\n\n\n\n\nApr 28, 2025\n\n5 min\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to the Internet of Things\n\n\nFundamental concepts and applications behind the ‘Internet of Things’ paradigm.\n\n\n\niot\n\n\n\n\n\n\nApr 28, 2025\n\n5 min\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDevice Twins\n\n\nHow to retrieve and configure IoT Devices using Device Twin metadata\n\n\n\niot\n\n\n\n\n\n\nApr 28, 2025\n\n9 min\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAzure EventHubs\n\n\nMore notes on how to send and receive events in C#\n\n\n\niot\n\n.NET\n\n\n\n\n\n\nApr 28, 2025\n\n8 min\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nIoT Communication Protocols\n\n\nHow communication protocols can be optimized for device-cloud communication, i.e. why not everything should use HTTP\n\n\n\nsignals\n\niot\n\n\n\n\n\n\nApr 28, 2025\n\n14 min\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMicrocomputers\n\n\nComparing the architectural paradigms for microcomputers to better understand our Pis.\n\n\n\nhardware\n\n\n\n\n\n\nApr 28, 2025\n\n12 min\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDevice calibration\n\n\nNotes for getting your sensors/actuators to work. \n\n\n\nhardware\n\nbash\n\npython\n\n\n\n\n\n\nMay 5, 2025\n\n2 min\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPython extras\n\n\nPython tips and tricks that there wasn’t time to cover explicitly in the course, but are useful to know. Logging in Python with the logging module. Unit testing in python with pytest. \n\n\n\npython\n\n\n\n\n\n\nMay 5, 2025\n\n4 min\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMessage Routing\n\n\nHow to specify custom endpoints for azure messages\n\n\n\niot\n\n\n\n\n\n\nMay 8, 2025\n\n3 min\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAzure Blob Storage\n\n\nSetting up blob storage on the Azure Portal \n\n\n\niot\n\n\n\n\n\n\nMay 8, 2025\n\n4 min\n\nMay 19, 2025\n\n\n\n\n\nNo matching items\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Modified - Oldest\n      \n      \n        Modified - Newest\n      \n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nCourse hardware\n\n\nList of hardware available for the course as well as their respective specifications and documentation.\n\n3 min\n\n\nhardware\n\n\n\n\nJan 20, 2025\n\n\n\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\nDeveloper environment setup\n\n\nInstructions for setting up classroom and personal computers for programming in this course, including: setting up a Debian container on WSL; restoring your WSL container from backup quickly; installing needed dependencies on WSL, Mac/OSX, and Linux; verifying that needed dependencies have been installed.\n\n7 min\n\n\nbash\n\nlinux\n\n\n\n\nJan 20, 2025\n\n\n\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\n\n\nBash essentials\n\n\nTips for using the bash interactive shell CLI effectively. The core bash commands everyone should know. How to combine commands using pipes and redirects.\n\n8 min\n\n\nbash\n\n\n\n\nJan 31, 2025\n\n\n\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\nGitHub basics\n\n\nBasics of cloning a repository hosted on Github and creating branches. Using a personal access token for authentication on GitHub outside of VSCode. Using the pass password manager to store access tokens safely.\n\n6 min\n\n\ngit\n\nbash\n\n\n\n\nFeb 10, 2025\n\n\n\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\n\n\nBash scripting\n\n\nTips for scripting in bash effectively. Managing environment and permissions in bash. The capabilities and limitations of programming in bash.\n\n21 min\n\n\nbash\n\n\n\n\nFeb 17, 2025\n\n\n\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\n\n\nReterminal setup\n\n\nGuide to setting up reterminal for the first time.\n\n18 min\n\n\nhardware\n\nlinux\n\n\n\n\nFeb 24, 2025\n\n\n\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\nreTerminal built-in devices\n\n\nInstalling initial reTerminal packages. Reading and controlling the reTerminal hardware interfaces, e.g. touchscreen, LEDs, light sensor, screen backlight, etc.\n\n5 min\n\n\nhardware\n\nlinux\n\nbash\n\n\n\n\nMar 10, 2025\n\n\n\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\nPython package management\n\n\nHow to use install system python packages using apt. How to install project python libraries using pip. Using virtual environments to manage python dependencies.\n\n11 min\n\n\npython\n\n\n\n\nMar 10, 2025\n\n\n\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\nGPIOs: Inputs and Outputs\n\n\nUnderstanding the General Purpose Input and Output (GPIO) pins our reTerminal makes available to us. Voltage, digital vs. analog, python libraries for GPIO.\n\n4 min\n\n\nhardware\n\nsignals\n\n\n\n\nMar 14, 2025\n\n\n\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\nSerial protocols\n\n\nA deeper dive into the protocols used to communicate between GPIO and the kernel\n\n10 min\n\n\nhardware\n\nsignals\n\n\n\n\nMar 14, 2025\n\n\n\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\nSignals: Analog and digital\n\n\nComparing the two paradigms for measuring electronic information and understanding how each are used for distributed applications.\n\n9 min\n\n\nhardware\n\n\n\n\nMar 14, 2025\n\n\n\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\nPython project management\n\n\nPython modules and packages. Python project file structure. Managing project dependencies. Configuring python project metadata. requirements.txt and pyproject.toml. Configuring python linters and formatters.\n\n8 min\n\n\npython\n\n\n\n\nApr 4, 2025\n\n\n\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\nAsynchronous progamming in Python\n\n\nUsing the asyncio library to control concurrentprocesses in Python.\n\n4 min\n\n\npython\n\n\n\n\nApr 4, 2025\n\n\n\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\nPython collection types and operations\n\n\nUseful python collection types and operations\n\n5 min\n\n\npython\n\n\n\n\nApr 4, 2025\n\n\n\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\nPython OOP\n\n\nBasics of Object Oriented Programming in Python.\n\n7 min\n\n\npython\n\n\n\n\nApr 4, 2025\n\n\n\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\nPython scripting\n\n\nTips and tricks for effective scripting in Python\n\n4 min\n\n\npython\n\n\n\n\nApr 4, 2025\n\n\n\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\nDebugging Reterminal Issues\n\n\nSometimes, the reTerminal display drivers fail. Here is an (incomplete) set of debugging steps you can take to try fixing the issue.\n\n4 min\n\n\nlinux\n\nbash\n\n\n\n\nApr 13, 2025\n\n\n\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\nAzure Features\n\n\nAn overview of the main feautres of Azure we will use for implementing an IoT system. Background information on “Platform as a Service” (PaaS), Azure IoT Hub vs IoT Central, Azure Resource Groups, Intro to Azure Dev Tools (Azure Portal, Azure CLI).\n\n7 min\n\n\niot\n\n\n\n\nApr 19, 2025\n\n\n\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\nAzure Portal Setup\n\n\nCreating an Azure account, managing your Azure account budget, Azure IoT Hub documentation, creating an Azure IoT Hub resource, and more.\n\n6 min\n\n\niot\n\n\n\n\nApr 19, 2025\n\n\n\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\nAzure SDKs in Python and C#\n\n\nAzure Libraries in Python and C# Installation instructions for WSL, how to use the libraries in your code.\n\n3 min\n\n\npython\n\niot\n\n\n\n\nApr 21, 2025\n\n\n\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\nAzure CLI Cheatsheet\n\n\nReference for common Azure commands\n\n5 min\n\n\nbash\n\niot\n\n\n\n\nApr 21, 2025\n\n\n\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\nDevice to Cloud Communication\n\n\nHow to use Azure for bidrectional device-cloud communication\n\n7 min\n\n\niot\n\n\n\n\nApr 28, 2025\n\n\n\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\nLinux package management\n\n\nInstalling and configuring Linux packages on WSL / Raspberry Pi Debian using the apt package manager\n\n6 min\n\n\nlinux\n\n\n\n\nApr 28, 2025\n\n\n\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\nElectronics Basics\n\n\nReview of Ohm’s Law, fundamental concepts for understanding electronic circuits and interfaces.\n\n5 min\n\n\nhardware\n\nsignals\n\n\n\n\nApr 28, 2025\n\n\n\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\nIntro to the Internet of Things\n\n\nFundamental concepts and applications behind the ‘Internet of Things’ paradigm.\n\n5 min\n\n\niot\n\n\n\n\nApr 28, 2025\n\n\n\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\nDevice Twins\n\n\nHow to retrieve and configure IoT Devices using Device Twin metadata\n\n9 min\n\n\niot\n\n\n\n\nApr 28, 2025\n\n\n\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\nAzure EventHubs\n\n\nMore notes on how to send and receive events in C#\n\n8 min\n\n\niot\n\n.NET\n\n\n\n\nApr 28, 2025\n\n\n\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\nIoT Communication Protocols\n\n\nHow communication protocols can be optimized for device-cloud communication, i.e. why not everything should use HTTP\n\n14 min\n\n\nsignals\n\niot\n\n\n\n\nApr 28, 2025\n\n\n\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\nMicrocomputers\n\n\nComparing the architectural paradigms for microcomputers to better understand our Pis.\n\n12 min\n\n\nhardware\n\n\n\n\nApr 28, 2025\n\n\n\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\nDevice calibration\n\n\nNotes for getting your sensors/actuators to work.\n\n2 min\n\n\nhardware\n\nbash\n\npython\n\n\n\n\nMay 5, 2025\n\n\n\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\nPython extras\n\n\nPython tips and tricks that there wasn’t time to cover explicitly in the course, but are useful to know. Logging in Python with the logging module. Unit testing in python with pytest.\n\n4 min\n\n\npython\n\n\n\n\nMay 5, 2025\n\n\n\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\nMessage Routing\n\n\nHow to specify custom endpoints for azure messages\n\n3 min\n\n\niot\n\n\n\n\nMay 8, 2025\n\n\n\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\nAzure Blob Storage\n\n\nSetting up blob storage on the Azure Portal\n\n4 min\n\n\niot\n\n\n\n\nMay 8, 2025\n\n\n\n\nMay 19, 2025\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\n Back to topReuseCC BY-NC 4.0. ©2022-25 Mauricio Buschinelli & Michael Haaf.\n(View License)",
    "crumbs": [
      "All course notes"
    ]
  },
  {
    "objectID": "notes/azure-device-twins/index.html",
    "href": "notes/azure-device-twins/index.html",
    "title": "Device Twins",
    "section": "",
    "text": "For each device that you connecte to IoT Hub, Azure IoT Hub maintains a device twin:\nDevice twins store device-related information that:\nNote that Device Twins are not appropriate for high-frequency communication such as sending telemetry data from device to cloud. For this, use D2C messages (see the course notes about device-cloud communication).",
    "crumbs": [
      "Device Twins"
    ]
  },
  {
    "objectID": "notes/azure-device-twins/index.html#device-twin-anatomy",
    "href": "notes/azure-device-twins/index.html#device-twin-anatomy",
    "title": "Device Twins",
    "section": "1 Device Twin Anatomy",
    "text": "1 Device Twin Anatomy\nThe device twin JSON document has the following structure:\n\n1.1 Device identity properties\n\nRead-only properties from the corresponding device identity.\n\n\n\n1.2 Tags\n\nThe solution back end can read from and write to. Tags are not visible to device apps.\n\n\n\n1.3 Desired properties\n\nUsed along with reported properties to synchronize device configuration or conditions.\nThe solution back end can set desired properties, and the device app can read them.\nThe device app can also receive notifications of changes in the desired properties.\n\n\n\n1.4 Reported properties\n\nUsed along with desired properties to synchronize device configuration or conditions. The device app can set reported properties, and the solution back end can read and query them.\n\n\n\n\nScreenshot of device twin properties\n\n\n\n Structure of a Device Twin Json document   Understand and use device twins in IoT Hub.\n\nNotice how the back-end has read/write access to the Desired properties but only has read access to the Reported properties.\nThe inverse is true for the device app.\nThe following example shows a device twin JSON document:\n{\n    \"deviceId\": \"devA\",\n    \"etag\": \"AAAAAAAAAAc=\",\n    \"status\": \"enabled\",\n    \"statusReason\": \"provisioned\",\n    \"statusUpdateTime\": \"0001-01-01T00:00:00\",\n    \"connectionState\": \"connected\",\n    \"lastActivityTime\": \"2015-02-30T16:24:48.789Z\",\n    \"cloudToDeviceMessageCount\": 0,\n    \"authenticationType\": \"sas\",\n    \"x509Thumbprint\": {\n        \"primaryThumbprint\": null,\n        \"secondaryThumbprint\": null\n    },\n    \"version\": 2,\n    \"tags\": {\n        \"$etag\": \"123\",\n        \"deploymentLocation\": {\n            \"building\": \"43\",\n            \"floor\": \"1\"\n        }\n    },\n    \"properties\": {\n        \"desired\": {\n            \"telemetryConfig\": {\n                \"sendFrequency\": \"5m\"\n            },\n            \"$metadata\" : {...},\n            \"$version\": 1\n        },\n        \"reported\": {\n            \"telemetryConfig\": {\n                \"sendFrequency\": \"5m\",\n                \"status\": \"success\"\n            },\n            \"batteryLevel\": 55,\n            \"$metadata\" : {...},\n            \"$version\": 4\n        }\n    }\n}",
    "crumbs": [
      "Device Twins"
    ]
  },
  {
    "objectID": "notes/azure-device-twins/index.html#high-level-flow",
    "href": "notes/azure-device-twins/index.html#high-level-flow",
    "title": "Device Twins",
    "section": "2 High-level Flow",
    "text": "2 High-level Flow\nThe use of Device Twins typically involves the back-end application trying to update the state of a device.\nIn this case we have the following information flow:\n\nA desired property is set by a back-end application into the IoT Hub.\nThe desired property is read by a device.\nThe device processes the desired property.\nThe device sets a reported property into the IoT Hub.\nThe device’s reported property is read by the back-end application.\n\n\nNotice that a tag can be set by a back-end application and is never sent to the device. Tags are used to organize and query devices.\n\n\n\n\nInformation flow for a desired property change\n\n\n\n Information flow for a desired property change from a back-end app   Tutorial: Configure your devices from a back-end service.",
    "crumbs": [
      "Device Twins"
    ]
  },
  {
    "objectID": "notes/azure-device-twins/index.html#desired-properties-example",
    "href": "notes/azure-device-twins/index.html#desired-properties-example",
    "title": "Device Twins",
    "section": "3 Desired Properties Example",
    "text": "3 Desired Properties Example\nConsider the property telemetryConfig which is used to configure telemetry collection on the device.\n\n3.1 Information Flow\nStep 1: The solution back-end sets the desired property with the desired configuration value. Below is the portion of the document with the desired property:\n\"desired\": {\n    \"telemetryConfig\": {\n        \"sendFrequency\": \"5m\"\n    },\n    ... other properties and metadata if applicable ...\n},\nStep 2a: If the device is connected, it is notified of the change immediately.\nStep 2b: If the device is not connected, it must follow the device reconnection flow to receive the full desired properties document and to subscribe to new notifications.\nStep 3: The device parses the desired property and triggers an action according to the application logic.\nStep 4: The device reports the updated configuration as a reported property (or an error condition using the status property). Below is the portion of the document with the reported property:\n\"reported\": {\n    \"telemetryConfig\": {\n        \"sendFrequency\": \"5m\",\n        \"status\": \"success\"\n    }\n    ... other properties and metadata if applicable ...\n}\nStep 5: The back-end reads and tracks the results of the configuration operation.\n\nNote that a back-end service can set and track device twin properties across many devices at once by querying device twins (more on querying later).\n\n\n\n3.2 Device Implementation\n\nAn excellent tutorial is provided in Get started with device twins (Python) - Create the device app\nAdditionally, there are two examples in the Github repository (use v2 branch):\nazure-iot-sdk-python/azure-iot-device/samples/sync-samples\n\nget_twin.py\nupdate_twin_reported_properties.py\n\n\n\n3.2.1 Implementation Summary\n\nInstall the Azure IoT Hub Device SDK for Python.\n\npip install azure-iot-device\n\nImport the class IoTHubDeviceClient.\n\nfrom azure.iot.hub import IoTHubDeviceClient\n\nInstantiate a IoTHubDeviceClient from the Device connection string and connect to the IoT Hub.\n\ndevice_client = IoTHubDeviceClient.create_from_connection_string(conn_str)\ndevice_client.connect()\n\nDefine a callback function for newly received Device Twin updates (patches).\n\ndef twin_patch_handler(twin_patch):\n    print(f\"Twin patch received: {twin_patch}\")\n\ndevice_client.on_twin_desired_properties_patch_received = twin_patch_handler\n\nGet the Device Twin document from the device instance and parse it if connecting for the first time (see the device reconnection flow).\n\ntwin = device_client.get_twin()\n\nParse your twin update by inspecting the requested properties.\n\n\nThis is were the magic happens 🧙\n⚠ Remember that any property starting with “$” is set by Azure. You cannot set or modify them. Trying to do so will result in an error.\n\n\nPrepare your Reported Properties dictionary and send it to the IoT Hub\n\nreported_patch = {\"telemetry_interval\": 20}\ndevice_client.patch_twin_reported_properties(reported_patch)\n\nDisconnect the client.\n\ndevice_client.shuthdown()\n\n\n\n3.3 Back-end Implementation\n\nAn excellent tutorial is provided in # Get started with device twins (.NET) - Create a service app\n\n\n3.3.1 Shared Access Policy\nThe connection string for the Device Twins back-end operations requires a specific shared access policy that has the permissions to:\n\nRegistry Read\nService Connect\n\n\nThis is not your Event Hub compatible endpoint connection string.\n\nSee image below for setting up a new shared access polity:\n\n\n\n\n3.4 .NET Service Client for Device Twin Updates\nThe .NET Azure IoT SDK includes two clients for device twin operations:\n\nDigitalTwinClient (recommended but slightly more complex)\nRegistryManager\n\nSDK references:\n\n\n\n.NET Class\nDocumentation\nGitHub Samples\n\n\n\n\nDigitalTwinClient\nDigitalTwinClient Class\nservice/samples/ solutions/DigitalTwinClientSamples/Thermostat/\n\n\nRegistryManager\nRegistryManager Class\nservice/samples/ how to guides/RegistryManagerSample\n\n\n\nNotes:\n\nThe Twin class has the properties Twin.Properties.Desired and Twin.Properties.Reported, which returns a TwinCollection Class.\n\nTwinCollection class has helper methods to access and manipulate Device Twin properties.",
    "crumbs": [
      "Device Twins"
    ]
  },
  {
    "objectID": "notes/azure-device-twins/index.html#references",
    "href": "notes/azure-device-twins/index.html#references",
    "title": "Device Twins",
    "section": "4 References",
    "text": "4 References\nMost of the content in this lesson was extracted from:\n\nUnderstand and use device twins in IoT Hub by Microsoft Docs.\nGet started with device twins (Python) by Microsoft Docs.\n\n\n4.1 Video Reference",
    "crumbs": [
      "Device Twins"
    ]
  },
  {
    "objectID": "notes/course-hardware/index.html",
    "href": "notes/course-hardware/index.html",
    "title": "Course hardware",
    "section": "",
    "text": "Image source",
    "crumbs": [
      "Course hardware"
    ]
  },
  {
    "objectID": "notes/course-hardware/index.html#reterminal",
    "href": "notes/course-hardware/index.html#reterminal",
    "title": "Course hardware",
    "section": "1 reTerminal",
    "text": "1 reTerminal\nThe reTerminal is a development board based on the Raspberry Pi Compute Module 4 (CM4) manufactured by Seeed Studio.\n\nSee reTerminal Wiki page for the complete specs and documentation\n\nMost of the information from the following sections was scraped from (“Getting Started with reTerminal | Seeed Studio Wiki” 2023)\n\n1.1 Features\n\nIntegrated modular design with high stability and expandability\nPowered by Raspberry Pi Computer Module 4 with 4GB RAM & 32GB eMMC\n5-Inch IPS capacitive multi-touch screen at 1280 x 720 and 293 PPI\nWireless connectivity with dual-band 2.4GHz/5GHz Wi-Fi and Bluetooth 5.0 BLE\nHigh-speed expansion interface and rich I/O for more expandability\nCryptographic co-processor with secure hardware-based key storage\nBuilt-in modules such as accelerometer, light sensor and RTC\nGigabit Ethernet Port and Dual USB 2.0 Type-A ports\n40-Pin header for IoT applications\n\n\n\n1.2 Specifications\nSee Specifications on the reTerminal wiki webpage.\n\n\n1.3 Hardware Overview\n\n1.3.1 Chassis\n\n\n\npir\n\n\n\n\n1.3.2 Motherboard\n\n\n\npir\n\n\n\n\n1.3.3 Block Diagram​\n\n\n\npir\n\n\n\n\n1.3.4 Pinout Diagram​\n\n\n\nPlease carefully pay attention to the orientation of the reTerminal in the above diagram. The LCD and the onboard buttons are on the right side whereas the back of reTerminal is on the left side. Also the whole device is flipped upside down.\n\n\n\n\n\n1.4 Power Supply\n\n\n\n\n\nRPI USB-C POWER SUPPLY BLACK US\n\n\nThe reTerminal requires a power supply that can provide a minimum of 3 Amps. The official Raspberry Pi USB-C Power Supply in included in the kit.",
    "crumbs": [
      "Course hardware"
    ]
  },
  {
    "objectID": "notes/course-hardware/index.html#compute-module-4",
    "href": "notes/course-hardware/index.html#compute-module-4",
    "title": "Course hardware",
    "section": "2 Compute Module 4",
    "text": "2 Compute Module 4\n\n\n\n\n\nRaspberry PI CM 4\n\n\nThe Compute Module 4 (CM4) made by the Raspberry Pi Foundation is powering the reTerminal.\nNotable features:\n\nProcessor: Broadcom BCM2711 quad-core Cortex-A72 (ARM v8) 64-bit SoC @ 1.5GHz\n\n\nSee CM4 datasheet for details.\n\nSee Difference Between ARM64, ARMel, and ARMhf for more info on the different ARM architectures.\n\n2.1 Grove Base Hat for Raspberry Pi\n\n\n\n\n\nGrove Base Hat for GPIO connections\n\n\nIn a typical Raspberry Pi, sensors would be connected via the 40-pin GPIO.\nTo facilitate connections of the Grove sensors, this “Hat” (term for an add-on board of the Raspberry Pi) includes the following types of connection:\n\n6 Digital\n4 Analog\n3 I2C\n1 PWM\n1 UART\n\n\nSee Grove base hat Wiki for details.",
    "crumbs": [
      "Course hardware"
    ]
  },
  {
    "objectID": "notes/course-hardware/index.html#sensors",
    "href": "notes/course-hardware/index.html#sensors",
    "title": "Course hardware",
    "section": "3 Sensors",
    "text": "3 Sensors\n\n3.1 AHT20 I2C Temperature & Humidity\n\n\n\n\n\nAHT20 I2C temperature/humidity sensor\n\n\nSee AHT20 I2C Industrial Grade Temperature & Humidity Sensor wiki for details.\n\nTemperature measurement range -40 ~ 85°C, Humidity measurement range 0 ~ 100% RH.\nDigital output, Grove I2C interface.\n\n\n\n3.2 AHT20 Libraries\nThe main module for this sensor is provided by Seeed in this Github repository and can be installed with the grove.py library.\nFollow official Step by step installation for python 3 (see below). Don’t use the one-click installation or it will install to the wrong location\ngit clone [https://github.com/Seeed-Studio/grove.py](https://github.com/Seeed-Studio/grove.py)\ncd grove.py\nsudo pip3 install .\nAlternatively, it’s also possible to use Adafruit’s adafruit-circuitpython-ahtx0 library to communicate with the sensor (see library’s Pypi page). However, to instantiate the provided sensor class, you will need to pass it an I2C bus instance. To instantiate an I2C bus instance, install and use the adafruit-extended-bus library (see Pypi page).",
    "crumbs": [
      "Course hardware"
    ]
  },
  {
    "objectID": "notes/course-hardware/index.html#actuators-motors",
    "href": "notes/course-hardware/index.html#actuators-motors",
    "title": "Course hardware",
    "section": "4 Actuators & Motors",
    "text": "4 Actuators & Motors\n\n4.1 LED Socket\n\n\n\n\n\nLED\n\n\nLED in a removable socket and potentiometer for power adjustment. LED can be swapped with different colors.\nSee LED wiki page for details.\n\n\n4.2 Cooling Fan\n\n\n\n\n\nCooling Fan\n\n\n5V Cooling Fan 40mm x 10mm with 2-pin JST connector.\n\nSee product page here.\n\n\n\n4.3 Relay\n\n\n\n\n\nRelay switch\n\n\nA digital switch. Controls the on/off flow of electricity with a small digital signal.\n\nOperate voltage: 3.3V-5V\nInput current: 100mA\nRated load: 5A@250VAC 5A@30VDC\n\nSee relay wiki page for details.",
    "crumbs": [
      "Course hardware"
    ]
  },
  {
    "objectID": "notes/course-hardware/index.html#cabling",
    "href": "notes/course-hardware/index.html#cabling",
    "title": "Course hardware",
    "section": "5 Cabling",
    "text": "5 Cabling\nThe following cables are included in the base kit:\n\nGrove Universal 4 Pin Buckled 5cm Cable.\nGrove Universal 4 Pin Buckled 20cm Cable.\nGrove 4 pin Female Jumper to Grove 4 pin Cable\nGrove 4 pin Male Jumper to Grove 4 pin Cable\n40-pin flat ribbon cable 20cm (female-female).\n2-pin JST SM Plug, one end open.",
    "crumbs": [
      "Course hardware"
    ]
  },
  {
    "objectID": "notes/reterminal-devices/index.html",
    "href": "notes/reterminal-devices/index.html",
    "title": "reTerminal built-in devices",
    "section": "",
    "text": "A diagram showing how the electrostatic field changes caused by touch is processed by devices like the reTerminal. Image source",
    "crumbs": [
      "reTerminal built-in devices"
    ]
  },
  {
    "objectID": "notes/reterminal-devices/index.html#programmable-interfaces",
    "href": "notes/reterminal-devices/index.html#programmable-interfaces",
    "title": "reTerminal built-in devices",
    "section": "1 Programmable interfaces",
    "text": "1 Programmable interfaces\nThis section is based on the official documentation for the reTerminal: Hardware and Interfaces Usage\nAll programmable data can be passed in file streams that can be read and/or written to.\nFor example, keyboard inputs and communication over web-sockets are all read as a file streams.\nThe reTerminal has 3 programmable LED’s and a light sensor that can be controlled like a regular file.\n\n\n\n\n\nreTerminal interface overview. Image source\n\n\nYou can see, there are 3 programmable LEDs in the reTerminal:\n\nSTA light can be turned on as red or green.\nUSR light can only be turned on as green.\n\n\n\n\n\n\nThe reTerminal LEDs and their corresponding filenames. Image source\n\n\nThe lights can be controlled at the OS level by editing files in the /sys/class/leds/ directory. Use ls -al to list the files in this directory:\n\n\nbash\n\nusername@hostname:/sys/class/leds/usr_led0 $ ls -al\ntotal 0\ndrwxr-xr-x 3 root root    0 Jan 25 20:33 .\ndrwxr-xr-x 8 root root    0 Jan 25 20:33 ..\n-rw-r--r-- 1 root root 4096 Jan 26 22:02 brightness\n\n\n\n.\nThe brightness file inide the usr_led0 controls the brightness of LED0. But, because only root has write permissions to this file, we will likely run into permissions errors if we try to edit the value directly:\n$ nano /sys/class/leds/usr_led0/brightness\nPermission denied\n\n$ echo 255 &gt; /sys/class/leds/usr_led0/brightness\nPermission denied\n\n$ sudo echo 255 &gt; /sys/class/leds/usr_led0/brightness\nPermission denied\nThere are a few possible approaches to this problem:\n\n1.1 Use sudo + text-editor\nYou can open a text-editor with root permissions using sudo:\n# nano text editor\nsudo nano /sys/class/leds/usr_led0/brightness\n\n# or, you can use vi/vim text editor\nsudo vim /sys/class/leds/usr_led0/brightness\nEdit the brightness file to a value between 0-255 using the editor. When you save, you should see the change immediately.\n\n\n1.2 Use sudo + su\nUsing an editor is perfectly reasonable for a one-off change, but annoying if you want to make the change more often.\nWe can use the echo value &gt; /path/to/file pattern, but only if we have permissions to write to /path/to/file – we can obtain these permissions if we run the entire command as the root user.\n\nNOTE: Running commands as the root user can have unintended consequences, ranging from annoying/tedious to fix, to completely devastating/permanently ruinous for your machine. You should avoid being root wherever possible (see Use sudo + tee section below for how to avoid it for this problem).\n\nYou can enter a root shell instance using the command su (switch user), or by using sudo -i or sudo -s:\nuser@hostname $ sudo su\nroot@hostname #\n\nuser@hostname $ sudo -i\nroot@hostname #\n\nuser@hostname $ sudo -s\nroot@hostname #\nYour shell should now display root@hostname:~#.\nTurn on the LED with maximum brightness\n# echo 255 &gt; brightness\nTurn off the LED\n# echo 0 &gt; brightness\nSimilarly, you can control usr_led1 and usr_led2 and even the buzzer on /sys/class/leds/usr_buzzer\nWhen done working as root, you can exit the root shell and return to your user shell using the exit command (or Ctrl-D as a hotkey):\nroot@hostname # exit\nuser@hostname $\n\n\n1.3 Use sudo + tee\nIf we want to avoid using a text editor, AND avoid logging into a root shell instance, it would be great if something like this worked:\n$ sudo echo 255 &gt; /sys/class/leds/usr_led0/brightness\nBut it doesn’t! Can you understand why? Consider which part of the instruction sudo applies to. Unfortunately, there is no way to sudo &gt; or sudo filename since sudo executes commands as the root user, NOT filenames/redirects.\nWhat if there were a command that we could put after the redirect, and sudo that command? What would that command need to do? It’s time to introduce a “new” command: tee.\nTo be clear, tee is a 50 year old program and an absolute classic – it is as ubiquitous/essential as the other OG unix programs like ls, cat, echo. We will see why in the following example.\nTry running man tee in your reTerminal. You should see something like the following:\nTEE(1)\n\nNAME\n       tee - read from standard input and write to standard output and files\n\nSYNOPSIS\n       tee [OPTION]... [FILE]...\n\nDESCRIPTION\n       Copy standard input to each FILE, and also to standard output.\nWe have here a command that will read from standard input, and write that same content from standard input into BOTH standard output AND files – the files we can specify as arguments.\n$ echo 255 | /sys/class/leds/usr-led0/brightness\n# this command is missing `tee` and `sudo`.... find where to put them, and you've learned a very important unix pattern!\n\n\n1.4 Luminosity Sensor\nThe digital light sensor can read the surrounding light levels.\n\nEnter the following directory\n\ncd /sys/bus/iio/devices/iio:device0\n\nRead the following file to obtain the light intensity value in Lux\n\ncat in_illuminance_input\nOutput:\npi@raspberrypi:/sys/bus/iio/devices/iio:device0 $ cat in_illuminance_input\n2719\nNote: We don’t need to be root to read this file. Its permissions are set to let all users read it, even-though it belongs to the root user:\nrw-r--r-- 1 root root 4096 Jan 30 22:16 in_illuminance_input",
    "crumbs": [
      "reTerminal built-in devices"
    ]
  },
  {
    "objectID": "notes/reterminal-devices/index.html#python-library-for-reterminal",
    "href": "notes/reterminal-devices/index.html#python-library-for-reterminal",
    "title": "reTerminal built-in devices",
    "section": "2 Python Library for reTerminal",
    "text": "2 Python Library for reTerminal\nSeeed Studio provided a python library to access most of the sensors and actuators of the reTerminal.\nInstall the library seeed-python-reterminal (see official Github repo) using pip:\nNOTE: See Package Management in Python for a clear explanation of how to use venv and pip.\n# NOTE: run this with a venv activated! \npip install seeed-python-reterminal\n\n# NOTE: you will also need this dependency\npip install RPi-GPIO\nNow you can import it to a test script (eg. buzz.py)\nimport seeed_python_reterminal.core as rt\nimport time\n\nprint(\"BUZZER ON\")\nrt.buzzer = True\ntime.sleep(1)\n\nprint(\"BUZZER OFF\")\nrt.buzzer = False\nThis will sound the buzzer of the reTerminal for 1 second.\nTo run the script:\nsudo $(which python) buzz.py\nAlternatively, first elevate your shell, then execute the script normally:\nuser@hostname:~ $ sudo -i\nroot@hostname:~# python buzz.py\n\nNote: this library is simply a wrapper to the OS operations we did in the previous section.\n\nSee the official seeed-python-reterminal Github repo for API reference on how to control:\n\nLED’s\nAccelerometer\nProgrammable Buttons\nLight Sensor",
    "crumbs": [
      "reTerminal built-in devices"
    ]
  },
  {
    "objectID": "notes/iot-concepts/index.html",
    "href": "notes/iot-concepts/index.html",
    "title": "Intro to the Internet of Things",
    "section": "",
    "text": "The Internet of Things (IoT) is comprised of equipment, machines, products, and devices that are connected to the cloud and periodically collect, send and/or receive data.\n\n\n\n\n\nThe Interconnection of communicating devices defines the “Internet of Things”\n\n\n\nIoT refers to the entire network of:\n\nphysical devices,\ntools,\nequipment, and\nsmart objects\n\nThese objects have the capability to collect and share data about:\n\nhow they are used\nthe environment around them\n\nIoT devices can also receive commands or updates via the cloud.",
    "crumbs": [
      "Intro to the Internet of Things"
    ]
  },
  {
    "objectID": "notes/iot-concepts/index.html#internet-of-things-iot",
    "href": "notes/iot-concepts/index.html#internet-of-things-iot",
    "title": "Intro to the Internet of Things",
    "section": "",
    "text": "The Internet of Things (IoT) is comprised of equipment, machines, products, and devices that are connected to the cloud and periodically collect, send and/or receive data.\n\n\n\n\n\nThe Interconnection of communicating devices defines the “Internet of Things”\n\n\n\nIoT refers to the entire network of:\n\nphysical devices,\ntools,\nequipment, and\nsmart objects\n\nThese objects have the capability to collect and share data about:\n\nhow they are used\nthe environment around them\n\nIoT devices can also receive commands or updates via the cloud.",
    "crumbs": [
      "Intro to the Internet of Things"
    ]
  },
  {
    "objectID": "notes/iot-concepts/index.html#why-collect-all-this-data",
    "href": "notes/iot-concepts/index.html#why-collect-all-this-data",
    "title": "Intro to the Internet of Things",
    "section": "2 Why collect all this data?",
    "text": "2 Why collect all this data?\nBy analyzing the data collected by these connected objects, users and organisations can make informed decisions in near real-time and automate tightly integrated processes. This avoids decision making based on limited or out-dated data.\nThis data can create quick feedback loops for automation. Data from one object can trigger actions or control several other objects with timing and precision that could not be achieved by humans.",
    "crumbs": [
      "Intro to the Internet of Things"
    ]
  },
  {
    "objectID": "notes/iot-concepts/index.html#iot-examples",
    "href": "notes/iot-concepts/index.html#iot-examples",
    "title": "Intro to the Internet of Things",
    "section": "3 IoT Examples",
    "text": "3 IoT Examples\nThe use of IoT can be organised in two fields of application:\n\nIndustrial IoT (IIoT)\nConsumer IoT\n\n\n\n\n\n\nExamples of industrial and consumer IoT applications. Image source: “The Internet of Things and its Benefit to U.S. Water Customers”\n\n\n\n3.1 Consumer IoT\nExamples of consumer IoT are:\n\nHome Security: Sensors, alarms, cameras, lights, and microphones provide 24/7 security and surveillance. All of which can be controlled from a smart phone.\nSmart Home: Smart lighting, heating and sound systems can be automated to detect human present or receive instructions remotely. Voice Assistants can also control house appliances or and provide information.\nWearables: Smartwatches and fitness trackers can monitor blood pressure or heart rate, and physical activity and suggest workouts, breaks or medical assistance.\nAppliances: Smart fridges can keep track of food quantities and expiry dates and create shopping lists accordingly. Smart outlet plugs can measure electrical consumption and turn other appliances on/off.\n\n\n\n3.2 Industrial IoT (IIoT)\nMost people tend to first think of IoT as consumer devices. However, their main value added happens in industrial applications.\n\nEquipment Maintenance: Rolls-Royce’s Total Care provides a suite of predictive maintenance and repair services for its jet engines, including monitoring engine health, and modifying engines to increase reliability and durability.\nFarming: John Deere is building intelligence into its large tractors and sprayers through sensors that make the machines into mobile platforms capable of self-driving.\nHeavy Machinery & Transportation: Rio Tinto, a global mining company uses a remote command center to orchestrate the actions of huge drills, excavators, trains, and trucks across multiple mining sites.\nRenewable Energies: Iberdrola and Siemens are using IoT to optimize the operation and maintenance of offshore wind power farms, the lifetime extension of wind turbines and the reducing the cost of energy.\nFacilities Management: Monitoring of buildings, infrastructure, and other spaces, allow to improve energy efficiency, space utilization, productivity, and safety. These insights may help:\n\nsave money by automating lighting or optimizing heating and cooling cycles.\nIncrease employee or occupant satisfaction by keep equipment running or ensuring that supplies are stocked.",
    "crumbs": [
      "Intro to the Internet of Things"
    ]
  },
  {
    "objectID": "notes/iot-concepts/index.html#high-level-components-of-an-iot-system",
    "href": "notes/iot-concepts/index.html#high-level-components-of-an-iot-system",
    "title": "Intro to the Internet of Things",
    "section": "4 High-level Components of an IoT System",
    "text": "4 High-level Components of an IoT System\nIoT systems are compromised of five high-level components:\n\nSensors:\n\nCollect data from the device’s environment (eg. velocity, GPS coordinates, temperature, etc).\nSensors might use different communication protocols.\n\nIoT Devices are computing devices that have:\n\nAbility to securely register with the cloud;\nConnectivity options for sending and receiving data with the cloud.\n\nCloud Gateway / Hub:\n\nSecurely ingests and sends data or commands to devices;\nOffers device management (provisioning) services (add, remove, update, group, etc).\n\nData Processing:\n\nData validation;\nIntegrate with business processes (check thresholds, send alerts, trigger action)\nPlace data in Storage;\nRetrieve data from Storage for deeper / long-term analysis;\n\nUser Interface:\n\nData visualization;\nTrigger manual actions;\nConfigure automatic actions;\nFacilitate device management.\n\n\n\n\n\n\n\nIcons of IoT High level components",
    "crumbs": [
      "Intro to the Internet of Things"
    ]
  },
  {
    "objectID": "notes/iot-concepts/index.html#iot-cloud-service-providers",
    "href": "notes/iot-concepts/index.html#iot-cloud-service-providers",
    "title": "Intro to the Internet of Things",
    "section": "5 IoT Cloud Service Providers",
    "text": "5 IoT Cloud Service Providers\nSince IoT objects typically move around, connectivity and data collection need to happen over wide geographical areas.\nMost companies do not want to deal with setting-up the physical infrastructure and software required to have this level of distributed connectivity and reliability (servers, databases, backup systems, networking, container orchestration, etc).\nIt is typically far more practical and cost-effective to use the services of an established IoT Cloud Service Provider.\nThe three main Cloud Service Providers in the IoT space are:\n\nAmazon Web Services (AWS)\nMicrosoft Azure\nGoogle Cloud\n\nThere are many other Cloud providers such as IBM Cloud IoT and Oracle IoT.\n\nIn this course we will focus on Microsoft Azure. However, all three platforms offer similar functionality.",
    "crumbs": [
      "Intro to the Internet of Things"
    ]
  },
  {
    "objectID": "notes/iot-concepts/index.html#references",
    "href": "notes/iot-concepts/index.html#references",
    "title": "Intro to the Internet of Things",
    "section": "6 References",
    "text": "6 References\n\nWhat is IoT by Microsoft Azure.\nHow Industrial Internet of Things are impacting our lives by Bluechip Infocorp Pvt Ltd via Medium.\nAn Introduction to the Internet of Things by Pier Paolo Ippolito via FreeCodeCamp Blog.",
    "crumbs": [
      "Intro to the Internet of Things"
    ]
  },
  {
    "objectID": "notes/bash-scripting/index.html",
    "href": "notes/bash-scripting/index.html",
    "title": "Bash scripting",
    "section": "",
    "text": "Image source: tudoubaba.net",
    "crumbs": [
      "Bash scripting"
    ]
  },
  {
    "objectID": "notes/bash-scripting/index.html#bash-theory",
    "href": "notes/bash-scripting/index.html#bash-theory",
    "title": "Bash scripting",
    "section": "1 Bash theory",
    "text": "1 Bash theory\nBash (and other shell languages) isn’t really a programming language – it is a command interpreter that’s best used for organizing the inputs and outputs of programs written in other languages. Historically this would be the C programming language. In our class, we will be writing “real” programs in Python.\nBut I digress. Despite the fact that Bash isn’t a programming language like C or Python, you can still program in it. What do you need to know to write Bash programs?\nThese notes will cover the basic programming concepts you need to know to get the most out of writing scripts in Bash.\n\n1.1 Variables\nThis section was adapted from (“Variable Substitution” n.d.)\nIn Bash, there is no concept of “types” – this is not a compiled language where different amounts of memory need to be reserved for different types of data.\nRather, all variables in Bash are simply value-placeholders. That is:\n\nThe name of a variable is a placeholder for its value, which is the data that it holds.\nReferencing/retrieving the value of a variable is called variable substitution.\n\nFor example, if variable1 is the name of a variable, then $variable1 is a reference to its value, the data it contains.\nAn equivalent syntax for variable substitution that is generally more robust is to add curly braces, as in: ${variable1}\nRunning a command such as echo $variable1 or echo ${variable1} will cause the value referenced by variable1 to be substituted and passed to the echo command.\nHere’s an example of variable assignment and substitution:\n\n\nbash\n\n# Create a variable with the name \"a\" and the value \"375\"\n$ a=375\n\n# Create a variable with the name \"hello\" and the value of the variable \"a\" (375)\n$ hello=$a\n\n# Not a variable reference, just the string \"hello\" ...\n$ echo hello\nhello\n\n# This *is* a variable substitution\n$ echo $hello\n375\n\n# This is another way to do a variable substitution\necho ${hello}\n375\n\n\n\n\n\n\n\n\nNote\n\n\n\nNo spaces are permitted on either side of = sign when initializing variables.\n\n\n1.1.1 Variable names\nWhat can you name variables? Here are the rules:\n\nVariable names must start with a letter (not a number or any other character)\nVariable names cannot contain whitespace or punctuation\n\n\n\n1.1.2 Command substitution\nWe can also assign the results of a command to a variable:\n\n\nbash\n\n# The \"%x %r %Z\" string is an example of a date format string.\n# See `man date` for examples of other date format strings\nright_now=\"$(date +\"%x %r %Z\")\"\n\nThe characters $( ) tell the shell: “substitute the results of the enclosed command”. This technique is known as command substitution.\nIn the above example script, the variable right_now gets assigned the result of calling the date command with with the argument \"%x %r %Z\" which outputs the current date and time.\nLike variable substitutions, it is a good idea to wrap command substitutions in double quotes to prevent unwanted word splitting in case the result of the expansion contains whitespace characters – see the (#Quoting) section below.\n\n\n1.1.3 Environment variables\nThis section was adapted from (“Writing Shell Scripts - Lesson 4: Variables” n.d.) and (“Export Man Page - Linux - SS64.com” n.d.)\nAny time a shell session is initialized, some variables are already set by startup files.\nTo see all the variables that are in the environment, use the printenv command:\n\n\nbash\n\n$ printenv\nSHELL=/bin/bash\n...\n\nYou can also view environment variables in a bash session using echo:\n\n\nbash\n\n$ echo $SHELL\n/bin/bash\n\nYou can create your own environment variables using the export command.\n\n\nbash\n\n$ MYDEPT=Sales\n$ echo $MYDEPT\nSales\n$ export MYDEPT\n\n# In one line:\nexport SOMEOTHERVAR=Value\n\n\n\n\n1.2 Quoting\nThis section was adapted from (“Quoting” n.d.)\nQuoting means just that, bracketing a string in quotes. This has the effect of protecting special characters in the string from reinterpretation or expansion by the shell or shell script.\n\nA character is special if it has an interpretation other than its literal meaning. For example, the asterisk * represents a wild card character in globbing and Regular Expressions.\n\n\n\nbash\n\n$ ls -l [Vv]*\n-rw-rw-r--    1 bozo  bozo       324 Apr  2 15:05 VIEWDATA.BAT\n-rw-rw-r--    1 bozo  bozo       507 May  4 14:25 vartrace.sh\n-rw-rw-r--    1 bozo  bozo       539 Apr 14 17:11 viewdata.sh\n\n$ ls -l '[Vv]*'\nls: [Vv]*: No such file or directory\n\n$ ls -l '[vv]*'\nls: [vv]*: no such file or directory\n\n\n1.2.1 Quoting variables\nWhen referencing a variable, it is generally advisable to enclose its name in double quotes, like so:\n\n\nbash\n\n$ local var=\"string-with-special-characters#*;&gt;,\"\n$ echo \"$var\"\nstring-with-special-characters#*;&gt;,\n\nThis prevents reinterpretation of all special characters within the quoted string, with the following exceptions:\n\n$ (used for variable dereferencing)\n\\ (escape character)\n\nKeeping $ as a special character within double quotes permits referencing a quoted variable ($variable), that is, replacing the variable with its value in the resulting string.\nUsing double quotes also prevents word splitting. An argument enclosed in double quotes presents itself as a single word, even if it contains whitespace separators.\n\n\nbash\n\nList=\"one two three\"\n\nfor a in $List     # Splits the variable in parts at whitespace.\ndo\n  echo \"$a\"\ndone\n# one\n# two\n# three\n\necho \"---\"\n\nfor a in \"$List\"   # Preserves whitespace in a single variable.\ndo #     ^     ^\n  echo \"$a\"\ndone\n# one two three\n\n\n\n1.2.2 Escaping\nEscaping is a method of quoting single characters. The escape \\ preceding a character tells the shell to interpret that character literally.\n\n\nbash\n\n$ echo \"Hello world\"\nHello world\n\n$ echo \"Hello \\\"world\\\"\"\nHello \"world\"\n\nYou can see more examples and information on escaping here.\n\n\n1.2.3 Single quotes\nSingle quotes ' operate similarly to double quotes, but do not permit referencing variables, since the special meaning of $ is turned off.\nWithin single quotes, every special character except ' gets interpreted literally. Consider single quotes (‘full quoting’) to be a stricter method of quoting than double quotes (“partial quoting”).\nSince even the escape character \\ gets a literal interpretation within single quotes, trying to enclose a single quote within single quotes will not yield the expected result.\n\n\n\n1.3 Functions\nThis section was adapted from (“Writing Shell Scripts - Lesson 6: Shell Functions” n.d.)\nAs programs get longer and more complex, they become more difficult to design, code, and maintain. As with any large endeavor, it is often useful to break a single, large task into a series of smaller tasks. We can do this with functions.\nA couple of important points about functions in bash:\n\nThey must be defined before they can be used.\nSecond, the function body (the portions of the function between the { and } characters) must contain at least one valid command.\n\nHere is an example:\n\n\nbash\n\n# The function definition must come before any function calls\nsystem_info()\n{\n  # At least one valid command is required in a function body\n  echo \"function system_info\"\n}\n\n# No brackets are included in the function call\nsystem_info\n\nRunning the above lines will define the system_info() function, then call it, simply result in the echo command running.\n\n1.3.1 Positional parameters\nFunctions in bash support positional parameters by default. This allows us to do things like specify the name of the output file on the command line, as well as set a default output file name if no name is specified.\nPositional parameters are a series of special variables ($0 through $9) that contain the contents of the command line.\nFor example, let’s change the earlier function definition above slightly:\n\n\nbash\n\nsystem_info()\n{\n  echo \"$1\"\n  echo \"$2\"\n  echo \"$3\"\n}\n\nThen, let’s see what happens if we were to use this function:\n\n\nbash\n\n$ system_info Hello world !\nHello\nworld\n!\n\n$ system_info \"Hello world!\" \"How are you?\" \"I am good, thanks!\"\nHello world!\nHow are you?\nI am good, thanks!\n\n\n\n1.3.2 Detecting positional parameters\nOften, we will want to check to see if we have command line arguments on which to act. There are a couple of ways to do this. First, we could simply check to see if $1 contains anything like so:\n\n\nbash\n\nif [ \"$1\" != \"\" ]; then\n    echo \"Positional parameter 1 contains something\"\nelse\n    echo \"Positional parameter 1 is empty\"\nfi\n\nSecond, the shell maintains a variable called $# that contains the number of items on the command line in addition to the name of the command ($0).\n\n\nbash\n\n# -gt means \"greater than\" in Bash, and -lt is \"less than\".\nif [ $# -gt 0 ]; then\n    echo \"Your command line contains $# arguments\"\nelse\n    echo \"Your command line contains no arguments\"\nfi\n\n\n\n1.3.3 Naming positional parameters with local\nIn a function that has many positional parameters, it can be difficult to keep track of what each $1 $2, etc. should mean.\nA common practise in bash functions is to create named variables within a function using local:\n\n\nbash\n\nsystem_info()\n{\n  local param1=\"$1\"\n  local param2=\"$2\"\n  local param3=\"$3\"\n}\n\nlocal is a bash builtin that can only be used within a function; it makes the variable name have a visible scope restricted to that function.\n\n\n\n1.4 Conditionals\nThis section was adapted from (“Writing Shell Scripts - Lesson 8: Flow Control - Part 1” n.d.)\nMost programs need to make decisions and perform different actions depending on various conditions. In bash, there are two main things to know for achieving conditional logic:\n\nHow the shell evaluates the success or failure of a command (Exit status)\nHow the shell can control the flow of execution in our program.\n\nThese two things are elaborated in the following sections.\n\n1.4.1 Exit status\nCommands (including the scripts and shell functions we write) issue a value to the system when they terminate, called an exit status. This value, which is an integer in the range of 0 to 2551, indicates the success or failure of the command’s execution. By convention, a value of zero indicates success and any other value indicates failure.\n1 Most of these numbers aren’t used – 0 (success) and 1 (failure) are most common. You can see a useful discussion of where to find more information about exit codes on stackoverflow.The shell provides a parameter $? that we can use to examine the exit status of the previously run command. Here we see it in action:\n\n\nbash\n\n$ ls -d /usr/bin\n/usr/bin\n\n# The last command terminated sucessfully, so we have a zero exit code when calling $?\n$ echo $?\n0\n\n$ ls -d /bin/usr\nls: cannot access /bin/usr: No such file or directory\n\n# The last command did not terminate successfully, so we have a non-zero exit code when calling $?\n$ echo $?\n2\n\nSome commands use different exit status values to provide diagnostics for errors, while many commands simply exit with a value of one when they fail. man pages often include a section entitled “Exit Status,” describing what codes are used. However, a zero always indicates success.\nThe shell provides two extremely simple builtin commands that do nothing except terminate with either a zero or one exit status. The true command always executes successfully and the false command always executes unsuccessfully:\n\n\nbash\n\n$ true\n$ echo $?\n0\n$ false\n$ echo $?\n1\n\nWe can use these commands to see how the if statement works. What the if statement really does is evaluate the success or failure of commands:\n\n\nbash\n\n$ if true; then echo \"It's true.\"; fi\nIt's true.\n\n$ if false; then echo \"It's true.\"; fi\n$\n\nThe command echo “It’s true.” is executed when the command following if executes successfully, and is not executed when the command following if does not execute successfully.\n\n\n1.4.2 exit\nWe can (and should!) set the exit status of our own scripts when they finish. To do this, use the exit command. The exit command causes the script to terminate immediately and set the exit status to whatever value is given as an argument.\nFor example: exit 0 exits our script and sets the exit status to 0 (success), whereas exit 1 exits your script and sets the exit status to 1 (failure).\n\n\n1.4.3 if\nThe if command is fairly simple on the surface; it makes a decision based on the exit status of a command. The if command’s syntax looks like this:\n\n\nbash\n\nif commands; then\n    commands\n[elif commands; then\n    commands...]\n[else\n    commands]\nfi\n\nTypically, you will see the if command combined with the test command, seen below.\n\n\n1.4.4 test\nThe test command is used most often with the if command to perform true/false decisions.\nThe command is unusual in that it has two different syntactic forms:\n\n\nbash\n\n# First form\ntest expression\n\n# Second form, which is far more common\n# Note: the word \"test\" does not appear, but this is in fact a \"test\" command!\n[ expression ]\n\n\n\nNotice the spaces between the [ ] braces and the expression – the whitespace is required.\nThe test command works simply. If the given expression is true, test exits with a status of zero; otherwise it exits with a status of 1.\nThe neat feature of test is the variety of expressions we can create. Here is an example:\n\n\nbash\n\nif [ -f .bash_profile ]; then\n    echo \"You have a .bash_profile. Things are fine.\"\nelse\n    echo \"Yikes! You have no .bash_profile!\"\nfi\n\nIn this example, we use the expression -f .bash_profile. This expression asks, “Is .bash_profile a file?” If the expression is true, then test exits with a zero (indicating true) and the if command executes the command(s) following the word then. If the expression is false, then test exits with a status of one and the if command executes the command(s) following the word else.\nHere is a partial list of the conditions that test can evaluate. Since test is a shell builtin, use help test to see a complete list:\n\n\n\nExpression\nDescription\n\n\n\n\n-d file\nTrue if file is a directory.\n\n\n-e file\nTrue if file exists.\n\n\n-f file\nTrue if file exists and is a regular file.\n\n\n-L file\nTrue if file is a symbolic link.\n\n\n-r file\nTrue if file is a file readable by you.\n\n\n-w file\nTrue if file is a file writable by you.\n\n\n-x file\nTrue if file is a file executable by you.\n\n\nfile1 -nt file2\nTrue if file1 is newer than file2.\n\n\nfile1 -ot file2\nTrue if file1 is older than file2.\n\n\n-z string\nTrue if string is empty.\n\n\n-n string\nTrue if string is not empty.\n\n\nstring1 = string2\nTrue if string1 equals string2.\n\n\nstring1 != string2\nTrue if string1 does not equal string2.\n\n\n\n\n\n1.4.5 A quick note on syntax\nNote that the above example can be written in a few ways:\n\n\nbash\n\n# Preferred form\nif [ -f .bash_profile ]; then\n    echo \"You have a .bash_profile. Things are fine.\"\nelse\n    echo \"Yikes! You have no .bash_profile!\"\nfi\n\n# Alternate form\nif [ -f .bash_profile ]\nthen echo \"You have a .bash_profile. Things are fine.\"\nelse echo \"Yikes! You have no .bash_profile!\"\nfi\n\nThe semicolon ; is a command separator. Using it allows us to put more than one command on a line.\nFor example: $ clear; ls will clear the screen, then execute the ls command.\nWe use the semicolon as we did to allow us to put the word then on the same line as the if command, because it’s easier to read that way.",
    "crumbs": [
      "Bash scripting"
    ]
  },
  {
    "objectID": "notes/bash-scripting/index.html#code-reuse",
    "href": "notes/bash-scripting/index.html#code-reuse",
    "title": "Bash scripting",
    "section": "2 Code reuse",
    "text": "2 Code reuse\nThe primary goal of writing a program in any language is to cryztalize useful logic in a reusable form – that is, to write a program. The outcome is that you don’t need to repeat yourself once you’ve solved a problem once.\nIn bash there are two main avenues we will take to achieve this goal:\n\ncreate function libraries\ncreate executable scripts\n\nThe following sections elaborate each technique.\n\n2.1 Function library with source\nThis section was adapted from (“Linux Command Line Adventure: Source” n.d.)\nMost programming languages permit programmers to specify external files to be included within their programs. This is often used to add “boilerplate” code to programs for such things as defining standard constants and referencing external library function definitions.\nBash has a builtin command, source, that implements this feature. This section will cover the ways it can make our scripts more powerful and easier to maintain.\nsource reads a specified file and executes the commands within it using the current shell. It works both with the interactive command line and within a script. Using the command line for example, we can reload the .bashrc file by executing the following command:\n$ source ~/.bashrc\nNote that the source command can be abbreviated by a single dot character like so:\n$ . ~/.bashrc\nWhen source is used on the command line, the commands in the file are treated as if they are being typed directly at the keyboard. In a shell script, the commands are treated as though they are part of the script.\nsource is a natural way to share functions and variables across many bash programs. For example, it makes sense to have a shared function to display error messages:\n\n\nbash\n\nerror_msg() {\n  printf \"%s\\n\" \"$1\" &gt;&2\n}\n\nTo share these functions across other scripts, we could build a library of functions and source that library. As an example, we could put all the common code in a file called ~/bash-scripts.sh and add the following code to both scripts to source that file:\n\n\nbash\n\nFUNCLIB=~/bash-scripts.sh\n\nif [[ -r \"$FUNCLIB\" ]]; then\n    source \"$FUNCLIB\"\nelse\n    echo \"Cannot read function library!\" &gt;&2\n    exit 1\nfi\n\nIf you put source statements like this in your ~/.bashrc, your functions will always be available each time you open a new terminal instance.\n\n\n2.2 Scripts\nIn the simplest terms, a shell script is a file containing a series of commands. The shell reads this file and carries out the commands as though they have been entered directly on the command line.\nSay we have the following file, example.sh:\n\n\nexample.sh\n\n#!/bin/bash\necho \"This is an example script\"\n\nWe can use the bash program to run this script:\n$ bash example.sh\nThis is an example script\nThis is fairly similar to the source command we saw earlier. The difference is a bit subtle:\n\nsource &lt;library-name&gt; will run within our current shell instance (preserving variables and functions)\nbash &lt;script-name&gt; will launch a new shell instance, which will NOT preserve variables and functions\n\nThe impetus to use one method or the other varies by purpose:\n\nwrite a library and use source to create re-useable functions to be used on the command line or in other scripts\nwrite a script to do a specific task\n\nThere is overlap between these two purposes, so don’t overthink it too much. You will often find yourself writing scripts that should be libraries, and vice versa – you can always make these changes to your programs whenever you like.\n\n2.2.1 Shebangs\nThe first line of any script, in ANY programming language (bash, sh, or, as we will soon see, `python) should start with a shebang:\n\n\nbash-script.sh\n\n#!/bin/bash\n\n# the first line beginning with #! is a shebang!\n\nLet’s break down the components of the shebang to better understand it:\n\n# – a comment\n! – in this context, a special character indicating that this line should be executed by the program that loads this file\n/bin/bash – the path to the interpreter for the code written in this file. Note that /bin/bash is an absolute path.\n\nWe will soon see that the most portable method for specifying a shebang is by providing the path to using the env program, like so:\n#!/usr/bin/env bash     # a bash script shebang\n#!/usr/bin/env sh       # a sh script shebang\n#!/usr/bin/env perl     # a perl script shebang\n#!/usr/bin/env python   # a python script shebang\nFor now, all that matters is that you provide a path to the bash program on your system. You can see valid options by running whereis bash:\n$ whereis bash\nbash: /bin/bash\n\n\n2.2.2 Permissions\nThis section was adapted from (“How-To: Set Permissions in Bash - Linux - SS64.com” n.d.)\nIn order to run a script without specifying an interpreter, you need to make the script executable, which is a permission that you can set on the file itself.\nThe sections below show how to view and set file permissions in linux filesystems.\n\n2.2.2.1 View permissions with ls\nThe ouptut of ls -l will show the current permissions for files and folders:\n-rwxr--rw- 1 user user 0 Jan 19 12:59 file1.txt\nThe letters rwx stand for Read/Write/Execute permission. These rights are shown three times, first for the Owner, then the Group and lastly Others (world)\n\n\n2.2.2.2 Edit permissions with chmod\nThe command to modify permissions is chmod. There are two ways to modify permissions, with numbers or with letters.\nCheck out this this chmod documentation for a really great interactive demo.\n\n2.2.2.2.1 Numeric\n\nchmod 400 file - Read by owner\nchmod 040 file - Read by group\nchmod 004 file - Read by world\nchmod 200 file - Write by owner\nchmod 020 file - Write by group\nchmod 002 file - Write by world\nchmod 100 file - execute by owner\nchmod 010 file - execute by group\nchmod 001 file - execute by world\n\nTo combine these, just add the numbers together:\n\nchmod 444 file - Allow read permission to owner and group and world\nchmod 777 file - Allow everyone to read, write, and execute file\n\n\n\n2.2.2.2.2 Symbolic\nchmod also accepts symbolic arguments for permission changes, where:\n\nrwx: read/write/execute\nugo: user/group/world\n\nSome examples:\n\nDeny execute permission to everyone: $ chmod a-x file\nAllow read permission to everyone: $ chmod a+r file\nMake a file readable and writable by the group and others: $ chmod go+rw file\nMake a shell script executable by the user/owner: $ chmod u+x myscript.sh\nAllow everyone to read, write, and execute the file and turn on the set group-ID: $ chmod =rwx,g+s file\n\nSome files are configured to have very restrictive permissions to prevent unauthorized access. Changing these permissions can create security problems.\nTo change or edit files that are owned by root, sudo chmod must be used. Note that changing permissions incorrectly can quickly make your system unusable! Please be careful when using sudo!\n$ sudo chmod o+x /usr/local/bin/somefile\n\n\n\n2.2.2.3 Recursive Permission Changes\nchmod -R will change all the permissions of each file and folder under a specified directory at once.\nFor example, $ chmod 777 -R /path/to/Dir will grant read/write/execute permissions to all users for ALL files in /path/to/Dir.\nTo assign reasonably secure permissions to files and folders/directories, it’s common to give files a permission of 644, and directories a 755 permission, using the find command and a pipe we can target just files or just folders as in the following examples.\n$ sudo find /path/to/Dir -type f -print0 | xargs -0 sudo chmod 644`\n$ sudo find /path/to/Dir -type d -print0 | xargs -0 sudo chmod 755\nAgain if using sudo be careful, in particular watch for extra spaces in your command/path.\n\n\n2.2.2.4 Changing Ownership and Group membership\nA file’s owner can be changed using the chown command.\n$ sudo chown kate file1.txt\nA file’s group can also be changed using the chown command.\n$ sudo chown :mygroup file1.txt\nchown can also change the owner and group in a single command:\n$ sudo chown tux:mygroup file1.txt\n\n\n\n2.2.3 Style\nYou can see the following resources for style guides for bash coding:\n\n“Unofficial Shell Scripting Stylesheet” by the Linux Documentation Project\n“Shell Style Guide” by Google developers",
    "crumbs": [
      "Bash scripting"
    ]
  },
  {
    "objectID": "notes/bash-scripting/index.html#resources",
    "href": "notes/bash-scripting/index.html#resources",
    "title": "Bash scripting",
    "section": "3 Resources",
    "text": "3 Resources\nThis section was adapted from https://linuxcommand.org/lc3_resources.php\nAside from everything covered in these notes, you can refer to the following resources:\n\nBash Builtins - commands built into the shell itself\nThe GNU Coreutils - the essential utilities included with most Linux distributions. These are divided into three groups:\n\nFile Utilities\nText Utilities\nShell Utilities\n\nOther Commands - other commonly-used Linux utilities",
    "crumbs": [
      "Bash scripting"
    ]
  },
  {
    "objectID": "notes/device-calibrating/index.html",
    "href": "notes/device-calibrating/index.html",
    "title": "Device calibration",
    "section": "",
    "text": "These notes group together various tips and tricks for calibrating and coding the sensors and actuators for your project.\n\n\n\n\nMake a file very similar to the one I made for the aht20, suggestions:\n\nname the python file after the device, for e.g., tmg39931.py for the the TMG39931.\nmake a Mock class very similar to the MockGrove... class I made, e.g. MockTMG39931\n\n(for now) have it inherit the same Grove... class, since it’s also an I2C-using module, the behaviour is similar enough for us to be happy, we can replace it later\nEnsure you use the correct default bus (1) and address (depends on the device)\ndefine a read method that returns a tuple with random numbers for all the readings\n\nMake a Sensor class for each reading in a similar style to my Temperature and Humidity sensors\n\nthe device type should be the same (Grove...), so it can be the same type as your Mock class.\nthe measurement should be a new Measurement type you create for that device.\n\nUse your new Sensor classes the same way I used the Temperature and Humidity ones, that is, make sure the device controller gets them when the system is initiated\nYou can now do everything you need to do to meet 100% of the A2/M4 requirements\n\n\n\n\n\n\n\n\nUse one of the three ports labelled “I2C” on your raspberry pi, plug the sensor in\non your reterminal, run i2cdetect -y 1. The devices’ address should be “in use” once the device is plugged in\nSee if you can get readings from the device using python. Try to find an example using the device in python\n\n\n\n\n\n\n\n\nMake a new device class (e.g. TMG39931 class in your device file e.g. tmg39931.py. It’s going to be really similar to the Grove... class in the grove.py library, except:\n\n__init__ : the same, but use the correct default address and bus.\nread: same idea, but customize according to the previous steps where you got readings from the device.\n\nMake your Mock class inherit from your new class instead\nMake your Sensor classes use your new class as the device instead",
    "crumbs": [
      "Device calibration"
    ]
  },
  {
    "objectID": "notes/device-calibrating/index.html#overview",
    "href": "notes/device-calibrating/index.html#overview",
    "title": "Device calibration",
    "section": "",
    "text": "These notes group together various tips and tricks for calibrating and coding the sensors and actuators for your project.\n\n\n\n\nMake a file very similar to the one I made for the aht20, suggestions:\n\nname the python file after the device, for e.g., tmg39931.py for the the TMG39931.\nmake a Mock class very similar to the MockGrove... class I made, e.g. MockTMG39931\n\n(for now) have it inherit the same Grove... class, since it’s also an I2C-using module, the behaviour is similar enough for us to be happy, we can replace it later\nEnsure you use the correct default bus (1) and address (depends on the device)\ndefine a read method that returns a tuple with random numbers for all the readings\n\nMake a Sensor class for each reading in a similar style to my Temperature and Humidity sensors\n\nthe device type should be the same (Grove...), so it can be the same type as your Mock class.\nthe measurement should be a new Measurement type you create for that device.\n\nUse your new Sensor classes the same way I used the Temperature and Humidity ones, that is, make sure the device controller gets them when the system is initiated\nYou can now do everything you need to do to meet 100% of the A2/M4 requirements\n\n\n\n\n\n\n\n\nUse one of the three ports labelled “I2C” on your raspberry pi, plug the sensor in\non your reterminal, run i2cdetect -y 1. The devices’ address should be “in use” once the device is plugged in\nSee if you can get readings from the device using python. Try to find an example using the device in python\n\n\n\n\n\n\n\n\nMake a new device class (e.g. TMG39931 class in your device file e.g. tmg39931.py. It’s going to be really similar to the Grove... class in the grove.py library, except:\n\n__init__ : the same, but use the correct default address and bus.\nread: same idea, but customize according to the previous steps where you got readings from the device.\n\nMake your Mock class inherit from your new class instead\nMake your Sensor classes use your new class as the device instead",
    "crumbs": [
      "Device calibration"
    ]
  },
  {
    "objectID": "notes/circuitry/index.html",
    "href": "notes/circuitry/index.html",
    "title": "Electronics Basics",
    "section": "",
    "text": "This course is at the boundary of code and physical objects. While we access the GPIOs of our reTerminals using filesystems and code, ultimately the digital signals are being delivered with electricity along conductive physical components.\nHaving a basic understanding of electronics will help you properly connect and debug digital devices without damaging them.",
    "crumbs": [
      "Electronics Basics"
    ]
  },
  {
    "objectID": "notes/circuitry/index.html#why-study-electronics",
    "href": "notes/circuitry/index.html#why-study-electronics",
    "title": "Electronics Basics",
    "section": "",
    "text": "This course is at the boundary of code and physical objects. While we access the GPIOs of our reTerminals using filesystems and code, ultimately the digital signals are being delivered with electricity along conductive physical components.\nHaving a basic understanding of electronics will help you properly connect and debug digital devices without damaging them.",
    "crumbs": [
      "Electronics Basics"
    ]
  },
  {
    "objectID": "notes/circuitry/index.html#gpio-physical-limits",
    "href": "notes/circuitry/index.html#gpio-physical-limits",
    "title": "Electronics Basics",
    "section": "2 GPIO Physical Limits",
    "text": "2 GPIO Physical Limits\nThe reTerminal has the following electrical constraints (see official wiki):\n\nAn individual GPIO pin in the reTerminal can only safely draw 16mA current.\nGPIO pins can only handle 3.3V voltage.\nThe power pins (3.3V and 5V) can handle a maximum of 2A current.\n\nWhat do these numbers mean? Why does it matter for us? What can we do about it?",
    "crumbs": [
      "Electronics Basics"
    ]
  },
  {
    "objectID": "notes/circuitry/index.html#units-of-measurement-volt-amp-and-ohm",
    "href": "notes/circuitry/index.html#units-of-measurement-volt-amp-and-ohm",
    "title": "Electronics Basics",
    "section": "3 Units of Measurement: Volt, Amp, and Ohm",
    "text": "3 Units of Measurement: Volt, Amp, and Ohm\nBellow are the basic units that are used to measure electricity:\n\n\n\nQuantity\nSymbol\nUnit\nAbbreviation\n\n\n\n\nCurrent\nI or i\nAmpere (“Amps”)\nA\n\n\nVoltage\nV or E\nVolts\nV\n\n\nResistance\nR\nOhm\nΩ\n\n\n\nAn electric circuit is formed when a conductive path is created to allow electric charge to continuously move.\n\n3.1 Current\nThe movement of electric charge (electrons) through the conductors of a circuit is called a current, and it is often referred to in terms of “flow,” just like the flow of a liquid through a hollow pipe.\n\n\n3.2 Voltage\nThe force pushing the electrons to “flow” in a circuit is called voltage.\nVoltage is a specific measure of potential energy that is always relative between two points. This is comparable to the pressure pushing water from a reservoir into a pipe.\n\nVoltage relates to the amount of potential energy between two points.\nWithout reference to two relative points, the term “voltage” has no meaning.\n\nVoltage is added to a circuit by a voltage source, such as a batter or a power supply unit attached to a wall.\nBelow are the two symbols typically used to identify a voltage source in a circuit:\n\n\n\n3.3 Resistance\nCurrent moves through the conductors with some degree of friction, or opposition to motion. This opposition to motion is more properly called resistance.\nThe amount of current in a circuit depends on the amount of voltage and the amount of resistance in the circuit to oppose current flow.\n\nJust like voltage, resistance is a quantity relative between two points.\nVoltage and resistance are often stated as being “between” or “across” two points in a circuit.\n\nBelow are the two symbols used for a resistor:\n\n\n\nWhat is resistor",
    "crumbs": [
      "Electronics Basics"
    ]
  },
  {
    "objectID": "notes/circuitry/index.html#ohms-law",
    "href": "notes/circuitry/index.html#ohms-law",
    "title": "Electronics Basics",
    "section": "4 Ohm’s law",
    "text": "4 Ohm’s law\nOhm’s law describes the relationship between voltage, current and resistance.\n\nThe amount of current through a metal conductor is directly proportional to the voltage across it.\nV = IR\n\nConsider the following circuit:\n\nThe proper circuit diagram will be:\n\nIf V1=9V and R1 = 200Ω, what is the current running through the circuit?\nRearranging Ohm’s law we get:\nI=V/R\nTherefore:\nI = 9V / 200Ω = 0.045A (amps) or 45mA (milli amps).\nWe can measure the current by using a multimeter in “ammeter” mode.\nThe multimeter must be added in series with the circuit. This means that the wiring must “go thorough” the multimeter.",
    "crumbs": [
      "Electronics Basics"
    ]
  },
  {
    "objectID": "notes/circuitry/index.html#voltage-drop-across-components",
    "href": "notes/circuitry/index.html#voltage-drop-across-components",
    "title": "Electronics Basics",
    "section": "5 Voltage Drop Across Components",
    "text": "5 Voltage Drop Across Components\nConsider the circuit below:\n\n\nWe must select the resistor value in order to maximize the life of the LED.\nThe following requirements apply:\n\nThe ideal current across the LED is of 20mA.\nThe voltage drop across the LED (Vled) is of 2.2V.\nThe voltage source is 12V.\n\nUsing Ohm’s law:\nR1 = Vr / Ir (Vr and Ir are the voltage and current across the resistor)\nHow do we find Vr and Ir?\nNote that because there is a LED that is also “consuming” some voltage, R1 is no longer the same as V1.\n\nThe total voltage drop across all components of the circuit must equal the source voltage.\nThis is know as Kirchhoff’s Law\n\nThis means that the voltage dropped at the LED and the resistor must add up to the same value as the source voltage.\nV1 = Vled + Vr\nVled was provided as a design constraint: Vled = 2.2V. Therefore we can easily calculate Vr:\nVr = V1 - Vled = 12V - 2.2V = 9.8V\nIt’s possible to measure the voltage drop across the resistor by using a multimeter that measures the voltage difference before and after the resistor.\n\nNotice how the multimeter is not interrupting the circuit. It is rather “sampling” the circuit at two points.\n\n\n\nNow that we know the voltage drop across the resistor, we are only missing the current flowing through it (Ir)\n\nBecause the circuit only has a single path, the current leaving the source is the same as the current flowing through all components.\n\nSince we are optimizing for the LED lifespan, the target current is 20mA or 0.02A.\nWe now have all necessary information to calculate the resistance value:\nR1 = Vr / Ir = 9.8V / 0.02A = 490Ω",
    "crumbs": [
      "Electronics Basics"
    ]
  },
  {
    "objectID": "notes/circuitry/index.html#resources-references",
    "href": "notes/circuitry/index.html#resources-references",
    "title": "Electronics Basics",
    "section": "6 Resources & References",
    "text": "6 Resources & References\nOhm’s Law - How Voltage, Current, and Resistance Relate by All About Circuits\nVoltage Divider Circuits by All About Circuits",
    "crumbs": [
      "Electronics Basics"
    ]
  },
  {
    "objectID": "notes/azure-d2c/index.html",
    "href": "notes/azure-d2c/index.html",
    "title": "Device to Cloud Communication",
    "section": "",
    "text": "Communication for IoT systems should be bi-directional:\nAzure offers a variety of ways of exchanging information in both directions. In this lesson we’ll look at the specifics of how these messaging systems work.",
    "crumbs": [
      "Device to Cloud Communication"
    ]
  },
  {
    "objectID": "notes/azure-d2c/index.html#azure-iot-protocols",
    "href": "notes/azure-d2c/index.html#azure-iot-protocols",
    "title": "Device to Cloud Communication",
    "section": "1 Azure IoT Protocols",
    "text": "1 Azure IoT Protocols\nIoT Hub allows devices to use the following protocols for device-side communications:\n\nMQTT (including over WebSockets)\nAMQP (including over WebSockets)\nHTTPS\n\nBy now you are well familiar with MQTT and HTTPS.\nAMQP is not covered in this course, however, it is also a popular protocol for IoT communication. It is used for most of Azure IoT internal communication.\nAMQP is more complex and has slightly more over head than MQTT, thus consuming more bandwidth and device memory. However, it does not requires a broker (although one can be used) and has extra features for security and flow control.\n\nThe choice of protocol is a design choice that is implemented when using libraries from the Azure IoT SDKs (ex.: when instantiating a client or a connection object).\nFor example, the Azure IoT SDK for python uses MQTT by default.\n\nIf you would like to know more about AMQP and how it differs from MQTT, see the video on “Diving Deeper”.",
    "crumbs": [
      "Device to Cloud Communication"
    ]
  },
  {
    "objectID": "notes/azure-d2c/index.html#device-to-cloud-d2c-communication",
    "href": "notes/azure-d2c/index.html#device-to-cloud-d2c-communication",
    "title": "Device to Cloud Communication",
    "section": "2 Device to Cloud (D2C) Communication",
    "text": "2 Device to Cloud (D2C) Communication\nIoT Hub exposes three options for sending information from the device to the cloud or solution back end:\n\nDevice-to-cloud messages for time series telemetry and alerts.\nFile uploads for media files and large telemetry batches uploaded by intermittently connected devices.\nDevice twin’s reported properties for reporting device state information such as configuration or last known state. Course notes on Device twins",
    "crumbs": [
      "Device to Cloud Communication"
    ]
  },
  {
    "objectID": "notes/azure-d2c/index.html#cloud-to-device-c2d-communication",
    "href": "notes/azure-d2c/index.html#cloud-to-device-c2d-communication",
    "title": "Device to Cloud Communication",
    "section": "3 Cloud to Device (C2D) Communication",
    "text": "3 Cloud to Device (C2D) Communication\nIoT Hub provides three options for devices to receive information and commands from the cloud or back-end app:\n\nCloud-to-device messages for one-way notifications to the device.\nDirect methods follow a request-response pattern and are meant for communications that require immediate confirmation of their result. For example, interactive control of the device, such as turning on a fan.\nTwin’s desired properties for long-running commands intended to put the device into a certain desired state. For example, set the telemetry send interval to 30 minutes. Course notes on Device twins",
    "crumbs": [
      "Device to Cloud Communication"
    ]
  },
  {
    "objectID": "notes/azure-d2c/index.html#end-points",
    "href": "notes/azure-d2c/index.html#end-points",
    "title": "Device to Cloud Communication",
    "section": "4 End-points",
    "text": "4 End-points\nIoT Hub also exposes multiple end-points that act as data input and output to various other actors.\nFor example, all messages received from a device are also forwarded to a “device to cloud message” end-point. An external client such as VS Code or Azure CLI can connect to this end-point and read the D2C messages sent to IoT Hub.\n\nYou can think of IoT Hub as a broker and the end-points as topics. A client can subscribe to a topic and receive forwarded messages.\nBy default, messages are routed to a built-in endpoint.\n\nIoT Hub allows data retention in the built-in end-points for a maximum of 7 days (1 day by default).\nFor a list of all built-in end-points see IoT Hub endpoints reference.\nThe Azure IoT SDKs article describes the various ways to access these endpoints.",
    "crumbs": [
      "Device to Cloud Communication"
    ]
  },
  {
    "objectID": "notes/azure-d2c/index.html#message-anatomy",
    "href": "notes/azure-d2c/index.html#message-anatomy",
    "title": "Device to Cloud Communication",
    "section": "5 Message Anatomy",
    "text": "5 Message Anatomy\nAs previously mentioned, IoT Hub messages can be used for both D2C or C2D communication.\nAn IoT Hub message consists of:\n\nA predetermined set of system properties.\nA set of application properties: A dictionary of string properties that the application can define and access, without needing to deserialize the message body.\nAn opaque binary body (the payload / application data).\n\n(Property names and values can only contain ASCII alphanumeric characters).\n Below is an example of a message as they are received by the IoT Hub.\n\nNote that creating and manipulation of the message on the device-side will look different. Many properties are automatically set on message by the SDK.\n\n{\n  \"message\": {\n\n    \"systemProperties\": {\n      \"contentType\": \"application/json\",\n      \"contentEncoding\": \"UTF-8\",\n      \"iothub-message-source\": \"deviceMessages\",\n      \"iothub-enqueuedtime\": \"2017-05-08T18:55:31.8514657Z\"\n    },\n\n    \"appProperties\": {\n      \"processingPath\": \"{cold | warm | hot}\",\n      \"verbose\": \"{true, false}\",\n      \"severity\": 1-5,\n      \"testDevice\": \"{true | false}\"\n    },\n\n    \"body\": \"{\\\"Weather\\\":{\\\"Temperature\\\":50}}\"\n  }\n}\nSystem properties are added for all users and identify content of the message. Users can selectively add application properties to the message.\nFor a list of system and application properties that can be read or set on messages, see:\n\nSystem Properties of D2C IoT Hub messages\nApplication Properties of D2C IoT Hub messages\nSystem Properties of C2D IoT Hub messages\n\n\n5.1 Application Properties in Python\nTo send device-to-cloud messages the method IoTHubDeviceClient.send_message(message) is used. This method can receive a string or a Message object. Typically we’ve been passing a json string with the telemetry payload and the Message object is created automatically for you (as per the docs).\nYou can, however, have more control over the properties of the Message object to be sent. As mention in Application Properties of D2C IoT Hub messages, there are many properties you can set yourself, including custom application properties. See the docs for the Message class.\nSee the example in azure-iot-sdk-python/azure-iot-device/samples/sync-samples/send_message.py to understand how to set message properties.\n\nThe messages described above (D2C and C2D communication) are typically created at the device or application with the help of a SDK (Software Development Kit) library.\n\n\nSee notes on Azure IoT SDKs for references to libraries and examples provided by Azure.",
    "crumbs": [
      "Device to Cloud Communication"
    ]
  },
  {
    "objectID": "notes/azure-d2c/index.html#direct-methods",
    "href": "notes/azure-d2c/index.html#direct-methods",
    "title": "Device to Cloud Communication",
    "section": "6 Direct Methods",
    "text": "6 Direct Methods\nIoT Hub gives you the ability to invoke direct methods on devices from the cloud. For example, to trigger a relay or ask the device to initiate a upload process.\nSimilarly to HTTP, direct methods represent a request-reply interaction with a device. The request sender needs to know immediately if the request succeeded or failed. For example, turning on a light from a phone.\nThe life-cycle of a Direct Method is as follows:\n\nA service application (such as a back-end or mobile app) sends a Direct Method request to the device. This is typically done using a SDK library running on the application or “manually” via a HTTPS call to IoT Hub.\n\nFor example using a C# app, see the sample C# repository we used in Lab 6, particularly azure-iot-sdk-csharp/iothub/service/samples/getting started/InvokeDeviceMethod.\nFor example using the azure cli, see the Azure CLI cheatsheet notes: az iot invoke-device-method\n\nThe device receives the Direct Method, processes it and prepares a response.\n\nThe IotHubDeviceClient from the azure.iot.device.aio library is used in python device code for asynchronous applications like ours.\nYou can see an example using it in the sample python repository we used in Lab 6, particularly azure-iot-sdk-python/samples/async-hub-scenarios/receive_direct_method.py sample code.\n\nThe response is sent back to the sender of the Direct Method.\nThe application inspects the response using a call-back function and decides what to do next.\n\nFor details see Understand and invoke direct methods from IoT Hub\nFor examples of how to use Direct Methods, see Quickstart: Control a device connected to an IoT hub.",
    "crumbs": [
      "Device to Cloud Communication"
    ]
  },
  {
    "objectID": "notes/azure-d2c/index.html#diving-deeper",
    "href": "notes/azure-d2c/index.html#diving-deeper",
    "title": "Device to Cloud Communication",
    "section": "7 Diving Deeper 🤿",
    "text": "7 Diving Deeper 🤿\nIf you would like to have a “bigger picture” view of how the different Azure IoT services fit together, watch the video below by Blaize Stewart.\n\n\nTo learn more about AMQT and how it differs from MQTT, see the presentation below by Ken Giusti.",
    "crumbs": [
      "Device to Cloud Communication"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "This companion site for the 420-6P3 course includes:\n\n\nLecture slides / documents\nExercises\nReferences & Resources\n\n\nWebsite accessible at john-abbott-college.github.io/6P3-Notes\n\n\nThis course will introduce students to the principles of the Internet of Things (IoT). Students will use Linux and Python to program a Raspberry Pi in order to read data from sensors and control actuators.\nBy the end of the course, students build and deploy an IoT product that is securely connected to the Azure ecosystem in order to collect and analyse telemetry data as well as respond to remote commands.\n\n\n\n\nExperience using Object Oriented Programming to create applications in any language.\nFamiliarity using the Bash shell and basic Linux CLI.\nFamiliarity with Python.\nA Raspberry Pi with various sensors and actuators (see Course Hardware)\nA Microsoft Azure account: create a free Azure for Students account.\n\n\n\n\nThis webpage is written in Markdown using the Quarto framework. The website is hosted via GitHub Pages\nSource code is open source and available on GitHub.\n\n\nCreate a local copy of these notes:\n\nInstall Quarto for your system\nClone the course GitHub repository.\nPreview the website:\nquarto preview .\n\n\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License. See the Copyright statement on the course webpage."
  },
  {
    "objectID": "index.html#description",
    "href": "index.html#description",
    "title": "Home",
    "section": "",
    "text": "This course will introduce students to the principles of the Internet of Things (IoT). Students will use Linux and Python to program a Raspberry Pi in order to read data from sensors and control actuators.\nBy the end of the course, students build and deploy an IoT product that is securely connected to the Azure ecosystem in order to collect and analyse telemetry data as well as respond to remote commands."
  },
  {
    "objectID": "index.html#requirements",
    "href": "index.html#requirements",
    "title": "Home",
    "section": "",
    "text": "Experience using Object Oriented Programming to create applications in any language.\nFamiliarity using the Bash shell and basic Linux CLI.\nFamiliarity with Python.\nA Raspberry Pi with various sensors and actuators (see Course Hardware)\nA Microsoft Azure account: create a free Azure for Students account."
  },
  {
    "objectID": "index.html#source-code",
    "href": "index.html#source-code",
    "title": "Home",
    "section": "",
    "text": "This webpage is written in Markdown using the Quarto framework. The website is hosted via GitHub Pages\nSource code is open source and available on GitHub.\n\n\nCreate a local copy of these notes:\n\nInstall Quarto for your system\nClone the course GitHub repository.\nPreview the website:\nquarto preview .\n\n\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License. See the Copyright statement on the course webpage."
  },
  {
    "objectID": "notes/linux-package-management/index.html",
    "href": "notes/linux-package-management/index.html",
    "title": "Linux package management",
    "section": "",
    "text": "The Raspberry Pi OS (previously called Raspbian OS) used in the reTerminal is a variation of the Debian OS, with optimizations for the Raspberry Pi hardware.\n\nDebian OS is one of the most commonly used Linux distributions, especially as the “base” for derivative distributions. There are over 100 derivatives of Debian, many of which are themselves very popular distributions. These include:\n\nDesktop distributions like Ubuntu, Linux Mint, elementaryOS;\nSpecial-purpose distributions Kali Linux and Backbox (penetration testing)\nServer/Hypervisor distributions like Proxmox, Ubuntu server,\nMany others, you can find a more complete list here\n\n\nDebian-derived distributions share the following:\n\nthe .deb package format\nthe dpkg package manager and its frontend apt\n\nFor that reason, many of the commands and configuration for Raspberry Pi OS can be taken directly from Debian’s documentation.",
    "crumbs": [
      "Linux package management"
    ]
  },
  {
    "objectID": "notes/linux-package-management/index.html#raspberry-pi-os-and-debian",
    "href": "notes/linux-package-management/index.html#raspberry-pi-os-and-debian",
    "title": "Linux package management",
    "section": "",
    "text": "The Raspberry Pi OS (previously called Raspbian OS) used in the reTerminal is a variation of the Debian OS, with optimizations for the Raspberry Pi hardware.\n\nDebian OS is one of the most commonly used Linux distributions, especially as the “base” for derivative distributions. There are over 100 derivatives of Debian, many of which are themselves very popular distributions. These include:\n\nDesktop distributions like Ubuntu, Linux Mint, elementaryOS;\nSpecial-purpose distributions Kali Linux and Backbox (penetration testing)\nServer/Hypervisor distributions like Proxmox, Ubuntu server,\nMany others, you can find a more complete list here\n\n\nDebian-derived distributions share the following:\n\nthe .deb package format\nthe dpkg package manager and its frontend apt\n\nFor that reason, many of the commands and configuration for Raspberry Pi OS can be taken directly from Debian’s documentation.",
    "crumbs": [
      "Linux package management"
    ]
  },
  {
    "objectID": "notes/linux-package-management/index.html#apt-debian-package-manger",
    "href": "notes/linux-package-management/index.html#apt-debian-package-manger",
    "title": "Linux package management",
    "section": "2 Apt: Debian Package Manger",
    "text": "2 Apt: Debian Package Manger\nThe command apt is the default package manager of Debian.\n\nA package manager is a software tool responsible for automating the installation, removal, configuration and removal of computer programs.\n\nManaging packages could technically be done manually, however, it is very time consuming and error prone: tracking package dependencies and compatibility with the current system.\n\n2.1 Common apt commands\nThere are many apt commands you will find yourself using all the time on a Debian-based OS like Raspberry Pi. It’s worth knowing in a bit more detail how they work.\nNote: Online, you will often see apt-get used instead of apt. apt is a newer package manager API meant to replace the older API apt-get. However, they both do more or less the same things the same ways, and apt is a backwards compatible replacement for apt-get in all cases. See Debian docs for details.\nDuring Lab 1, you updated, upgraded and installed some packages in the Raspberry Pi OS using apt:\n\napt: the difference between update and upgrade\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\nsudo apt update\nUpdate list of available packages\n\n\nsudo apt upgrade\nUpgrade the system by installing/upgrading packages\n\n\n\nCombined, these two commands update and upgrade all of the currently operating system packages on your system:\n$ sudo apt update && sudo apt upgrade -y\nThere are a few other apt commands worth knowing.\n\napt: a variety of other commands I find myself using now and again\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\napt list\nlist packages installed on your system based on package names.\n\n\napt search &lt;query&gt;\nReturns all repository packages search in package descriptions\n\n\napt show &lt;package&gt;\nshow package details\n\n\napt install &lt;package&gt;\ninstall packages\n\n\napt reinstall &lt;package&gt;\nreinstall packages\n\n\napt remove &lt;package&gt;\nremove packages\n\n\n\nYou can see details about all of these commands and more by running apt --help. Note: documentation will often leave out sudo since the need to run sudo depends on the machine and the user.\n\nIn general, commands that read data (apt list, apt search, etc.) will generally not require sudo, since most folderst that apt will touch are universally readable.\napt commands that write data (apt install, etc.) to a folder not owned by the current user – which is most folders outside of /home/username, but particularly common package install locations like /usr, /bin, etc. – these commands will require sudo to gain the required write permission\n\n\n\n2.2 Apt source repositories explained\nThis section has been adapted directly from the Debian documentation: see SourcesList\nApt downloads packages from one or more software repositories (sources) and installs them onto your computer.\nA repository is generally a network server, such as the official DebianStable repository. Local directories or CD/DVD are also accepted.\nThe specific repositories (package sources) configured on your machine affect: - What software packages are available for download - What versions of packages are available - Who packages the software\n\n2.2.1 Commonly used package sources\n\nDebianStable: official Debian repository for the current release\nStableProposedUpdates: official Debian repository for upcoming point releases (security and important bug fixes every ~2 months)\nStableUpdates: official Debian repository for changes that cannot wait for the next point release, packages are also added to StableProposedUpdates for inclusion in the next point release\nDebianSecurity: official Debian repository for frequent security updates\nDebianBackports: more recent versions of some packages, compatible with DebianStable.\nDebianUnstable: rolling development version containing the latest packages\nDebianExperimental: development version containing the experimental/alpha/beta/untested packages\n\n\n\n2.2.2 Editing Software Sources\nBeing able to change the repositories used by your package management system is a powerful feature but this power comes with some responsibility.\nUsers are cautioned that it is possible to break your system (in a way that could be difficult or impossible to cleanly fix) by adding third-party repositories, or repositories for a Debian version that does not match your current version - these repository create a risk of conflicting package versions, creating what’s sometimes called a “Franken-Debian” system.\nThe whole concept behind a Debian stable release is that the Debian developers have picked a set of software and their versions that function nicely together. While this software is patched to fix security issues, the software is frequently not the latest version. It takes some experience to know how the repos may be changed without risk of breaking your system. Users of all levels are advised to change repos cautiously.",
    "crumbs": [
      "Linux package management"
    ]
  },
  {
    "objectID": "notes/python-project-management/index.html",
    "href": "notes/python-project-management/index.html",
    "title": "Python project management",
    "section": "",
    "text": "Now that we’re making larger applications with multiple dependecies from different sources, as well as multiple modules, code organization starts to be a noticable problem. Managing a “project” (for now, a directory containing your source code for a given task) becomes much easier with tools and configuration.\nWe’ll look at:\n\nImport statements\nRelative imports\nConfiguration of python projects",
    "crumbs": [
      "Python project management"
    ]
  },
  {
    "objectID": "notes/python-project-management/index.html#overview",
    "href": "notes/python-project-management/index.html#overview",
    "title": "Python project management",
    "section": "",
    "text": "Now that we’re making larger applications with multiple dependecies from different sources, as well as multiple modules, code organization starts to be a noticable problem. Managing a “project” (for now, a directory containing your source code for a given task) becomes much easier with tools and configuration.\nWe’ll look at:\n\nImport statements\nRelative imports\nConfiguration of python projects",
    "crumbs": [
      "Python project management"
    ]
  },
  {
    "objectID": "notes/python-project-management/index.html#modules-vs-packages",
    "href": "notes/python-project-management/index.html#modules-vs-packages",
    "title": "Python project management",
    "section": "2 Modules vs Packages",
    "text": "2 Modules vs Packages\nNotes adapted from (“Making a Python Package — The Joy of Packaging 0.1 Documentation” n.d.)\nFor programmers coming from other languages, how to organize your project files and how to import modules and classes can be confusing.\nLet’s clarify the terminology and distinction between a module vs a package.\n\n2.1 Modules\nA module usually corresponds to a single file: something.py A python “module” is a single namespace, with a collection of values:\n\nfunctions\nconstants\nclass definitions\n\n\n\n2.2 Packages\nA “package” is essentially a module, except it can have other modules (and indeed other packages) inside it.\nA package usually corresponds to a directory with a file in it called __init__.py and any number of python files or other package directories:\na_package\n    __init__.py\n    module_a.py\n    a_sub_package/\n        __init__.py\n    module_b.py\n__init__.py can be empty or it can have arbitrary code.\nThe code will be run when the package is imported (just like a module).\nModules inside packages are not automatically imported. So, with the above structure:\nimport a_package\nwill run the code in a_package/__init__.py\nAny names defined in the __init__.py will be available in: a_package.a_name",
    "crumbs": [
      "Python project management"
    ]
  },
  {
    "objectID": "notes/python-project-management/index.html#project-file-structure",
    "href": "notes/python-project-management/index.html#project-file-structure",
    "title": "Python project management",
    "section": "3 Project file structure",
    "text": "3 Project file structure\nThere are two conventions for structuring a python project: flat layout and src layout.\nFlat layout is a bit more intuitive, and most small projects use it. Src layout has a lot of benefits for larger projects. The two are compared below.\n\nflat layoutsrc layout\n\n\nexample adapted from (“Src Layout Vs Flat Layout - Python Packaging User Guide” n.d.)\n\n\nexample flat layout\n\n.\n├── .gitignore\n├── README.md\n├── pyproject.toml\n├── some_package/\n│   ├── __init__.py\n│   └── module.py\n└── tools/\n    └── some_python_script.py\n# More directories can be added; tests, data, utilities, etc.\n\nThe “flat layout” refers to organising a project’s files in a folder or repository, such that the various configuration files and import packages are all in the top-level directory.\n\n\nexample adapted from (“Src Layout Vs Flat Layout - Python Packaging User Guide” n.d.)\n\n\nexample src layout\n\n.\n├── .gitignore\n├── README.md\n├── pyproject.toml\n├── src/\n│    └── some_package/\n│       ├── __init__.py\n│       └── module.py\n└── tools/\n    └── some_python_script.py\n# More directories can be added; tests, data, utilities, etc.\n\nThe “src layout” deviates from the flat layout by moving the code that is intended to be importable (i.e. import some_package, also known as import packages) into a subdirectory. This subdirectory is typically named src/, hence “src layout”.",
    "crumbs": [
      "Python project management"
    ]
  },
  {
    "objectID": "notes/python-project-management/index.html#project-configuration",
    "href": "notes/python-project-management/index.html#project-configuration",
    "title": "Python project management",
    "section": "4 Project configuration",
    "text": "4 Project configuration\nThis section explains configuring the following in a python project:\n\ndependencies through requirements.txt\nlinting/formatting through dotfiles like ruff.toml\nall of the above with pyproject.toml\n\n\n4.1 Dependencies with requirements.txt\nProject dependencies can be listed in a file for portability. By convention, the file requirements.txt contains all the dependencies of a project created with a virtual environment.\n\nThis is analogous to package.json for Node.Js projects\n\nTo generate a requirements.txt for an existing project:\n$ pip freeze &gt; requirements.txt\nTo install packages from a requirements.txt:\n$ pip install -r requirements.txt\nThe content of a requirements.txt file is a list of arguments that can be provided to the pip install command. Any syntax that is valid for pip install is valid for the requirements.txt file:\n\n\nsample-requirements.txt\n\n# Install the latest version of gpiozero published on pypi.org\ngpiozero\n\n# You can pin the dependency to a specific version published on pypi.org\n# This is a good practise to make sure your project doesn't stop working when\n# breaking changes are introduced to the package you're depending on.\ngpiod==2.3.0\n\n# Some packages aren't kept up to date on pypi.org, but can be installed from git directly\ngit+https://github.com/Seeed-Studio/Seeed_Python_RPi\n\n# You can have requirements files automatically install the dependencies of other requirements files\n# This is a nice way to separate requirements for different purposes.\n# Note the `-r` flag to *recursively* install packages listed in the file.\n-r other-requirements.txt\n\nSee Requirements File Format from the pip documentation for more details.\n\n\n4.2 Formatting/linting with ruff\nThere are many formatters and linters for Python (see the links for examples and explanations).\nFor Assignment 1, we used ruff to both format and lint our source code.\nTo install ruff in a virtual environment:\n(.venv) $ pip install ruff\nYou can customize the behavior of tools like ruff command line arguments, as well as configuration files. For ruff, the file is ruff.toml.\nSee the ruff documentation: “Configuring Ruff”.\n\n\n4.3 pyproject.toml\nFor more advanced projects, more advanced configurations than just project dependencies need to be declared: linters, formatters, the build system for packaging the project, project metadata, and more.\nThe pyproject.toml file is the recommended configuration file for packaging python projects. It can replace the functionality of requirements.txt. See (“Writing Your Pyproject.toml - Python Packaging User Guide” n.d.) for more detail – some of the notes below are adapted from it directly.\nThese notes are adapted from (“The Complete Guide to Pyproject.toml” 2024), a great blogpost explaining the pyproject.toml file.\n\n4.3.1 Basics\nThe only required information in the pyproject.toml file is the name of your project. This is entered under a [project] field in the .toml file.\nIf/when your project is distributed on https://pypi.org/, this field will be the name of the project there.\n\n\nComparison of project names is case insensitive and treats runs of underscores, hyphens, and/or periods equivalently. For example, if you register a project named cool-stuff, users will be able to download it or declare a dependency on it using any of the following spellings: Cool-Stuff, cool.stuff, COOL_STUFF, CoOl__-.-__sTuFF\n\n\nsample-pyproject.toml\n\n[project]\nname = \"sample-python-project\"\n\nThe project name must consist of ASCII letters, digits, underscores “_”, hyphens “-” and periods “.” It must not start or end with an underscore, hyphen or period.\nThere are many more metadata properties that can be set in the [project] section of the config file. See the Other project metadata section for more.\n\n\n4.3.2 Replacing requirements.txt\nIn the assignments, we separated developer and production dependencies into two different files. We can replace that functionality with a single file using pyproject.toml.\n\n\nsample-pyproject.toml\n\n[project]\n# project fields go here\n\n# We can replace requirements.txt with the \"dependencies\" field of the [project] section\ndependencies = [\n  # Note the comma: \"dependencies\" has a list-like syntax.\n  \"gpiozero\",\n  \"gpiod == 2.3.0\",\n  # Note that the syntax for installing from a repository is slightly different\n  # You must specify the package name first, then provide the protocol/url for the repository.\n  \"seeed-python-rpi @ git+https://github.com/Seeed-Studio/Seeed_Python_RPi\"\n]\n\n# We can replace requirements-dev.txt with the \"optional-dependencies\" subfield of the [project] section.\n# You can make up your own categories: here, I have made \"test\", \"lint\", and \"dev\" categories.\n[project.optional-dependencies]\ntest = [\n    \"pytest\",\n]\nlint = [\n    \"ruff\"\n]\ndev = [\n    # Note that the name of the project (sample-python-project) must match exactly for this syntax to work.\n    \"sample-python-project[test,lint]\",\n]\n\n\n\n4.3.3 Replacing configuration files e.g. ruff.toml\nThe pyproject.toml config file unifies the configuration for most python tooling.\nIn Assignment 1, we used ruff for linting and formatting. All of its configuration was placed in a ruff.toml file.\nWe can move that configuration to pyproject.toml:\n\n\nsample-pyproject\n\n[tool.ruff]\nline-length = 120\nindent-width = 4\n\nThere are many other options you can specify, for example, some useful options I use:\n\n\nsample-pyproject.toml\n\n[tool.ruff.format]\nquote-style = \"double\"\nindent-style = \"space\"\nline-ending = \"auto\"\ndocstring-code-format = true\n\n# This specifies the linting issues that ruff will catch when `ruff check` is run\n[tool.ruff.lint]\nselect = [\n    \"F\",   # pyflakes\n    \"E\",   # pycodestyle\n    \"I\",   # isort\n    \"ANN\", # flake8 type annotations\n    \"RUF\", # ruff-specific rules\n]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\nYou can see more options in the ruff documentation.\n\n\n4.3.4 Project metadata\nWhen reading a project on GitHub, you can find out important information about the code from the project README.md\nIf your code is packaged to be used by others, for instance, by packaging it and publishing it on pypi.org, the metadata about the project (author, project description, etc.) can be specified in pyproject.toml.\nSee setting project metadata formore explanation and an example.\nThe official python documentation also shows examples:\n\nSetting project authors\nSetting project license\nSetting project version\nA full example\n\n\n\n4.3.5 Advanced features\nHere are a few features you can easily add to your projects, if you have the time or interest!\n\nSpecifying entrypoints for the project\nCreating executable scripts\nPackaging your project for distribution\n\nI recommend using setuptools",
    "crumbs": [
      "Python project management"
    ]
  },
  {
    "objectID": "notes/python-project-management/index.html#tips-and-tricks",
    "href": "notes/python-project-management/index.html#tips-and-tricks",
    "title": "Python project management",
    "section": "5 Tips and Tricks",
    "text": "5 Tips and Tricks\nThis section has a few tips and tricks for understanding python projects better\n\n5.1 Relative Import Statements\nAdapted from StackOverflow discussion (Aya 2013)\nThere are a few tricky aspects for using import statements in python projects.\nConsider the project layout below:\nmain.py\nmypackage/\n    __init__.py\n    mymodule.py\n    myothermodule.py\nIn mymodule.py :\n\n\nmymodule.py\n\n# Exported function\ndef get_temperature():\n    return 23.45\n\nif __name__ == '__main__':\n    print(get_temperature())\n\nIn myothermodule.py :\n\n\nmyothermodule.py\n\n# Relative import using `.` for current directory, `..` for parent directory.\nfrom .mymodule import get_temperature\n\n# Exported function\ndef announce_weather():\n    return f\"Outside temperature is {get_temperature()}\"\n\nif __name__ == '__main__':\n    announce_weather()\n\nIn main.py :\n\n\nmain.py\n\nfrom mypackage.myothermodule import announce_weather\n\ndef main():\n    print(announce_weather())\n\nif __name__ == '__main__':\n    main()\n\nThis works when running main.py or mypackage/mymodule.py, but fails with mypackage/myothermodule.py, due to the relative import:\nfrom .mymodule import as_int\nException has occurred: ImportError\nattempted relative import with no known parent package\nSolution\nWhen running myothermodule.py as a stand-alone script, run Python using the -m option to run the file as a module. This will execute all the __init__.py files and load the module dependency.\npython -m mypackage.myothermodule\n\n# Note the \".\", NOT a \"/\"\n# the following WILL NOT work:\npython -m mypackage/myothermodule\nThis will allow you to write modules that contain main() functions you can test separately – we took advantage of this pattern in Assignment 1.",
    "crumbs": [
      "Python project management"
    ]
  },
  {
    "objectID": "notes/python-extras/index.html",
    "href": "notes/python-extras/index.html",
    "title": "Python extras",
    "section": "",
    "text": "These notes cover useful python skills, particularly for finalizing your team’s project.",
    "crumbs": [
      "Python extras"
    ]
  },
  {
    "objectID": "notes/python-extras/index.html#overview",
    "href": "notes/python-extras/index.html#overview",
    "title": "Python extras",
    "section": "",
    "text": "These notes cover useful python skills, particularly for finalizing your team’s project.",
    "crumbs": [
      "Python extras"
    ]
  },
  {
    "objectID": "notes/python-extras/index.html#logging",
    "href": "notes/python-extras/index.html#logging",
    "title": "Python extras",
    "section": "2 Logging",
    "text": "2 Logging\nThese notes were adapted from (“Logging in Python – Real Python” n.d.)\nSee this article from Real Python for a complete explanation.\nImportant notes:\n\nInstantiating the logger in each file\nFormatting log output\nLogging to a file\nUsing handlers to customize format and output",
    "crumbs": [
      "Python extras"
    ]
  },
  {
    "objectID": "notes/python-extras/index.html#unit-testing",
    "href": "notes/python-extras/index.html#unit-testing",
    "title": "Python extras",
    "section": "3 Unit testing",
    "text": "3 Unit testing\n\nThese notes were adapted from:\n\n(“Effective Python Testing With Pytest – Real Python” n.d.)\n(“Pytest Documentation” n.d.)\n\n\nThere are two main libraries for unit tests in python:\n\nunittest: included in the standard python library\npytest: a very popular third party library for writing unit tests in python\n\nI’ve used both before, and they’re both great. In this course, we’ll be using pytest. There’s a few reasons for that, which mostly aren’t that important, but in general, compared to unittest, pytest requires far less boilerplate code to accomplish the same things. For example:\n\nunittestpytest\n\n\nTo create one unit test that always passes, and one that always fails, using unittest:\n\n\nunittest\n\nfrom unittest import TestCase\n\nclass TryTesting(TestCase):\n    def test_always_passes(self):\n        self.assertTrue(True)\n\n    def test_always_fails(self):\n        self.assertTrue(False)\n\nThe following steps were necessary:\n\nImport the TestCase class from unittest\nCreate TryTesting, a subclass of TestCase\nWrite a method in TryTesting for each test\nUse one of the self.assert methods from unittest.TestCase to make assertions\n\nThis boilerplate code is required for every single test.\n\n\nTo create one unit test that always passes, and one that always fails, using pytest:\n\n\npytest\n\ndef test_always_passes():\n    assert True\n\ndef test_always_fails():\n    assert False\n\nThe following steps were necessary:\n\ndefine functions beginning with the name test_\nuse Python’s built-in assert keyword directly\n\nMuch less boilerplate code required!\n\n\n\n\n3.1 Installing pytest\nIn your virtual environment, use pip to install pytest:\n$ pip install pytest\n\n\n3.2 Creating and running tests\nCreate a new file called test_sample.py, containing a function, and a test:\n\n\ntest_sample.py\n\ndef func(x):\n    return x + 1\n\n\ndef test_answer():\n    assert func(3) == 5\n\nRun the test:\n$ pytest\nSee pytest: Getting Started documentation for more details.\nSee my example_system unit tests for examles relevant to our project.\n\n\n3.3 Dependency injection with pytest fixutres\npytest is designed to follow a variant of the “functional” testing paradigm called Arrange-Act-Assert:\n\nArrange, or set up, the conditions for the test\nAct by calling some function or method\nAssert that some end condition is true\n\npytest enables the creation of fixtures, that is, test objects, that can be used across tests to simplify the Arrange step:\n\n\nfixture_demo.py\n\nimport pytest\n\n# This annotation marks the function as a fixture generator\n@pytest.fixture\ndef example_fixture():\n    return 1\n\n# Make tests use that fixture by providing the name as a parameter of the test function:\ndef test_with_fixture(example_fixture):\n    assert example_fixture == 1\n\nYou can see how I use fixtures in the example_system conftest.py files of the sample test code in the final-project-upstream.\n\n3.3.1 conftest.py\nYou’ll notice in the example_system tests that I define fixtures in files named conftest.py:\n\nfinal-project-upstream/iot_subsystems/tests/example_system/\n├── conftest.py\n├── integration\n│   ├── conftest.py\n│   ├── __init__.py\n│   └── test_system.py\n└── unit\n    ├── __init__.py\n    ├── test_aht20.py\n    └── test_fan.py\n\nThis is a pytest convention for sharing fixtures across multiple test files, allowing you to keep your test files concise and to avoid needing to recreate dependencies. This is very useful for our project, where we have many custom classes we are creating and mocking.\nYou can see more about conftest.py in the pytest documentation\n\n\n\n3.4 using pytest on asynchronous code\nIn our final project, there are quite a few asynchronous functions to test.\nSee the example_system.py unit tests from the final-project-upstream code for an example.\n\n\n3.5 using logot to test logs\npytest does include a fixture called caplog for testing if logs were written.\nI found that the 3rd party package logot was much simpler to use.\nSee the example_system.py unit tests from the final-project-upstream code for an example:\n\nInstalling logot\n\nEnsure logot is compatible with pytest\n\nBasics of using logot\nUsing logot with `pytest\nUsing logot on asynchronous tests\nUsing “message matching” to match log messages with placeholders\n\n\n\n3.6 Further pytest reference\nI found the following documentation really useful when I was making the example_system/ starter code tests:\n\nHow to use pytest on the command line, choosing which files to test\nConventions for Python test discovery: file, module, class and function names\nBest practises for test file structure layout\nHow to use fixtures\n\nMore explanation of what fixtures are\nMore detailed reference for fixtures\n\nHow to adjust the behavior of mock test classes using “monkeypatch”",
    "crumbs": [
      "Python extras"
    ]
  },
  {
    "objectID": "notes/debug-reterminal/index.html",
    "href": "notes/debug-reterminal/index.html",
    "title": "Debugging Reterminal Issues",
    "section": "",
    "text": "So far, there have been two primary causes for the reTerminal screen to stop working:\nIn the first case, the reTerminal boots just fine, but the screen doesn’t work. In the second and third cases, the reTerminal fails to boot at all.\nFor each of these cases, I have a set of troubleshooting steps you can take to resolve these issues.",
    "crumbs": [
      "Debugging Reterminal Issues"
    ]
  },
  {
    "objectID": "notes/debug-reterminal/index.html#if-you-can-ssh-into-your-reterminal",
    "href": "notes/debug-reterminal/index.html#if-you-can-ssh-into-your-reterminal",
    "title": "Debugging Reterminal Issues",
    "section": "1 If you can ssh into your reTerminal",
    "text": "1 If you can ssh into your reTerminal\nMost likely it is the case that your reterminal is still connecting to the internet and accessible over ssh, even when the screen is not working.\n\nDouble check that you can see your reterminal in tailscale by running tailscale status on your developer machine. ssh into the device using this IP address.\n\nIf for some reason your device is offline on tailscale, try to determine its local ip address using your router webpage (at home). ssh into reterminal using this ip address\n\nOnce you have an ssh connection, follow the steps from the course notes: Display driver fix scripts",
    "crumbs": [
      "Debugging Reterminal Issues"
    ]
  },
  {
    "objectID": "notes/debug-reterminal/index.html#if-you-cannot-ssh-into-your-reterminal",
    "href": "notes/debug-reterminal/index.html#if-you-cannot-ssh-into-your-reterminal",
    "title": "Debugging Reterminal Issues",
    "section": "2 If you cannot ssh into your reTerminal",
    "text": "2 If you cannot ssh into your reTerminal\n\n2.1 Finding the problem\n\nAsk the teacher for a micro-usb to HDMI connector\nPlug the HDMI end into a workstation monitor, and the micro-usb end into your reTerminal (in the port labelled HDMI)\nPlug your reTerminal into a power supply to boot the reTerminal\nThe monitor will show logs of your reTerminal booting. Observe these logs closely. You are looking for specific error messages.\nWrite down the specific error messages. Search the course notes for any concepts you don’t understand. Think back: have you made any changes to the /boot files that seem related to these errors?\nIf you have a good idea of what happened and what needs to be fixed, proceed to the next section. Feel free to double check your understanding with the teacher at this point – for educational purposes, I may not give you the answer directly, but I can guide you in a direction that will help save you some time.\n\n\n\n2.2 Solving /boot/ firmare file problems\nIf indeed the error comes from an incorrect /boot configuration file setting, we can fix the error without reimaging the pi. Instead, we will mount the reTerminal storage on to a workstation and then fix the firmware files directly on that workstation.\nThe steps will look similar to Lab 1, except we won’t be reimaging the device at the end. You should open Reterminal Setup instructons for reference while you are working. The following steps will be necessary:\n\nMake sure your reTerminal is entirely unplugged and powered off.\nDisassemble your reTerminal, up to the step where you can toggle the Firmware Flash switch. Toggle the switch. This step will let you mount the storage device onto your workstation.\nOn your lab workstation, find the program rpibootloader and run it. You should not need elevated access.\nWhile the rpibootloader program is running, plug your reTerminal into the workstation using a USB-C to USB-3 connector.\nYou should see the rpibootloader acknowledge connection to your reterminal. A dozen or so logs will be printed to the rpibootloader program window. Once this is done: navigate to Windows Explorer (the file manager) and you should see the reTerminal (~28-32GB) as a connected drive on the workstation\nYou can now access all files on the reterminal filesystem.\nThe firmware files can now be edited directly to fix any mistakes made.\nJust in case, you can also recover any files you need that are stored on the reterminal in the /home/your-username directory.",
    "crumbs": [
      "Debugging Reterminal Issues"
    ]
  },
  {
    "objectID": "notes/azure-message-routing/index.html",
    "href": "notes/azure-message-routing/index.html",
    "title": "Message Routing",
    "section": "",
    "text": "It is possible to route messages to different end-points according to the type of message and the content of the message.\nMessage routing involves the following steps:",
    "crumbs": [
      "Message Routing"
    ]
  },
  {
    "objectID": "notes/azure-message-routing/index.html#free-account-limitation",
    "href": "notes/azure-message-routing/index.html#free-account-limitation",
    "title": "Message Routing",
    "section": "1 Free Account Limitation",
    "text": "1 Free Account Limitation\nUnfortunately the free tier of the IoT Hub is limited to only one additional endpoint (see Quotas and throttling).\nIn this lesson we will create and delete multiple endpoints to illustrate their capabilities.",
    "crumbs": [
      "Message Routing"
    ]
  },
  {
    "objectID": "notes/azure-message-routing/index.html#routing-queries",
    "href": "notes/azure-message-routing/index.html#routing-queries",
    "title": "Message Routing",
    "section": "2 Routing Queries",
    "text": "2 Routing Queries\nA message event can be routed according to its type. Messages can be of the following types:\n\ndevice life-cycle events,\ndevice telemetry messages,\ndevice twin change events.\n\nFor telemetry messages, it’s possible to query on: - System properties, - Application properties, - Message body.\n\nSee Create and read IoT Hub messages for a list of message properties.\n\nConsider the following message:\n{ \n  \"message\": { \n    \"systemProperties\": { \n      \"contentType\": \"application/json\", \n      \"contentEncoding\": \"UTF-8\",\n      \"iothub-connection-device-id\": \"myDevice-1\"\n      \"iothub-message-source\": \"deviceMessages\", \n      \"iothub-enqueuedtime\": \"2017-05-08T18:55:31.8514657Z\" \n    }, \n    \"appProperties\": { \n      \"processingPath\": \"{cold | warm | hot}\", \n      \"verbose\": \"{true, false}\", \n      \"severity\": 1-5, \n      \"testDevice\": \"{true | false}\" \n    }, \n    \"body\": \"{\\\"Weather\\\":{\\\"Temperature\\\":50}}\" \n  } \n} \nThis message includes system properties, application properties and a body (payload).\nTo query on the application property processingPath and the system property iothub-connection-device-id we would write the following query:\nprocessingPath = 'hot' AND $connectionModuleId = 'myDevice-1'\n\nSee IoT Hub message routing query syntax for the query syntax and examples.\n\nNote that in order for the message body to be stored in plain text, the following properties must be set on the message prior to sending the message:\n\"contentType\": \"application/json\", \n\"contentEncoding\": \"UTF-8\"\nWith the Python SDK, these properties would be set with:\nmsg = Message(payload)\nmsg.content_encoding = 'utf-8'\nmsg.content_type = 'application/json'",
    "crumbs": [
      "Message Routing"
    ]
  },
  {
    "objectID": "notes/async-python/index.html",
    "href": "notes/async-python/index.html",
    "title": "Asynchronous progamming in Python",
    "section": "",
    "text": "The library asyncio enables code to be run concurrently: Tasks share the execution thread while appearing to be executing in parallel.\n\nAsyncio API:\n\nCoroutines and Tasks (good intro to asyncio).\n\nIn-depth guide:\n\nAsync IO in Python: A Complete Walkthrough by realpython.com.\n\nVideo Summary:\n\nHow To Easily Do Asynchronous Programming With Asyncio by ArjanCodes\n\n\n\n\nIt’s important to note that concurrent code does not necessarily mean parallel.\n\nConcurrent does necessarily mean parallel execution. By Ten thousand meters\n\nIn Python, the only way to run parallel code is to use multiple CPU cores by running multiple Python interpreters that interact. See multiprocessing module.\n\nAsyncio is used to run multiple tasks or multiple threads concurrently but never at the same time (in parallel).\n\n\n\nIn typical programming, most tasks and processes are synchronous, meaning, they run one at the time, always waiting for the previous taks to finish\nSynchronous is disadvantageous when there are many tasks that need to wait for external input or output before it can proceed. For example:\n\nThe code was asked to sleep.\nWaiting for a HTTP response.\nWaiting for a Database call.\nWaiting to write to a file.\n\n\nSynchronous code execution by RealPython.com\nAsynchronous execution enables the execution of the next task while waiting for the external input/output.\n\nAsynchronous code execution by RealPython.com\n\nNote that the amount CPU processing (blue area in the diagrams above) is the same in both sync and async executions.\nHowever, the async execution spends less time waiting idle.\n\n\n\n\nasyncio uses the concept of tasks to manage and share the CPU processing time.\n\nEach task must explicitly yield (or pass) execution time back to the main loop so that the next task can start/continue.\n\n\nTasks yielding execution time with the await keyword. By Adafruit.com\nIn the image above, Task 1 (running function f()) yields execution time to Task 2 (running function g()) by using the await keyboard and vice-versa.\n\n\n\nIn Asyncio, there are 3 types of awaitable objects:\n\nCoroutines\nTasks\nFutures\n\n\n\nCoroutines are functions that are awaitable.\nThey must be declared with the async keyword\nasync def countdown(task_name, seconds):\n    for i in range(seconds, 0, -1):\n        print(task_name, i)\n        await asyncio.sleep(1)\n    print(\"done!\")\n\n\nasync def main():\n    await countdown(\"simple count\", 4)\n\n\nasyncio.run(main())\n\n\"\"\" Output\nsimple count 4\nsimple count 3\nsimple count 2\nsimple count 1\ndone!\n\"\"\"\nIn the example above, countdown(), asyncio.sleep() and main() are coroutines.\ncountdown() and asyncio.sleep() were called with await , however, main() was called with asyncio.run()\n\nIn asyncio, all awaitable objects must run inside an event loop.\n\nThe method asyncio.run() creates a “top-level” event loop\n\n\n\n\n\nTasks are used to schedule coroutines concurrently.\nOnce a task is scheduled, it will run as soon as the event loop is available.\n\nTasks are scheduled (wrapped) with asyncio.create_task()\nIn order to run a task to completion, it must be awaited.\n\nasync def main():\n    # Scheduling and starting tasks\n    taskA = asyncio.create_task(countdown(\"task A\", 3))\n    taskB = asyncio.create_task(countdown(\"task B\", 2))\n\n    # Doing something else while they run\n    await asyncio.sleep(1)\n    print(\"Everything, everywhere, all at once\")\n\n    # Awaiting for tasks to complete before ending main()\n    # Try running this code without awaiting\n    await taskA\n    await taskB\n\n\nasyncio.run(main())\n\n\"\"\" Output\ntask A 3\ntask B 2\nEverything, everywhere, all at once\ntask A 2\ntask B 1\ntask A 1\ndone!\ndone!\n\"\"\"\nInstead of creating Tasks and awaiting them individually, it’s possible to run multiple coroutines concurrently and wait for all of them:\nasync def main():\n    # Running and awaiting multiple coroutines\n\n    await asyncio.gather(countdown(\"task A\", 4), countdown(\"task B\", 3))\n\n    await asyncio.sleep(1)\n    print(\"Everything, everywhere, all at once\")\n\n\nasyncio.run(main())\n\"\"\"\ntask A 4\ntask B 3\ntask A 3\ntask B 2\ntask A 2\ntask B 1\ntask A 1\ndone!\ndone!\nEverything, everywhere, all at once\n\"\"\"\n\n\n\nA Future in Asyncio is the equivalent of a Promise in JavaScript.\n\nIt is rare that developers need to use a Future directly.\n\n\n\n\n\nThe following are excellent resources for learning more about Asyncio:\n\nAsync IO in Python: A Complete Walkthrough by RealPython.com",
    "crumbs": [
      "Asynchronous progamming in Python"
    ]
  },
  {
    "objectID": "notes/async-python/index.html#asynch-with-asyncio",
    "href": "notes/async-python/index.html#asynch-with-asyncio",
    "title": "Asynchronous progamming in Python",
    "section": "",
    "text": "The library asyncio enables code to be run concurrently: Tasks share the execution thread while appearing to be executing in parallel.\n\nAsyncio API:\n\nCoroutines and Tasks (good intro to asyncio).\n\nIn-depth guide:\n\nAsync IO in Python: A Complete Walkthrough by realpython.com.\n\nVideo Summary:\n\nHow To Easily Do Asynchronous Programming With Asyncio by ArjanCodes\n\n\n\n\nIt’s important to note that concurrent code does not necessarily mean parallel.\n\nConcurrent does necessarily mean parallel execution. By Ten thousand meters\n\nIn Python, the only way to run parallel code is to use multiple CPU cores by running multiple Python interpreters that interact. See multiprocessing module.\n\nAsyncio is used to run multiple tasks or multiple threads concurrently but never at the same time (in parallel).\n\n\n\nIn typical programming, most tasks and processes are synchronous, meaning, they run one at the time, always waiting for the previous taks to finish\nSynchronous is disadvantageous when there are many tasks that need to wait for external input or output before it can proceed. For example:\n\nThe code was asked to sleep.\nWaiting for a HTTP response.\nWaiting for a Database call.\nWaiting to write to a file.\n\n\nSynchronous code execution by RealPython.com\nAsynchronous execution enables the execution of the next task while waiting for the external input/output.\n\nAsynchronous code execution by RealPython.com\n\nNote that the amount CPU processing (blue area in the diagrams above) is the same in both sync and async executions.\nHowever, the async execution spends less time waiting idle.\n\n\n\n\nasyncio uses the concept of tasks to manage and share the CPU processing time.\n\nEach task must explicitly yield (or pass) execution time back to the main loop so that the next task can start/continue.\n\n\nTasks yielding execution time with the await keyword. By Adafruit.com\nIn the image above, Task 1 (running function f()) yields execution time to Task 2 (running function g()) by using the await keyboard and vice-versa.\n\n\n\nIn Asyncio, there are 3 types of awaitable objects:\n\nCoroutines\nTasks\nFutures\n\n\n\nCoroutines are functions that are awaitable.\nThey must be declared with the async keyword\nasync def countdown(task_name, seconds):\n    for i in range(seconds, 0, -1):\n        print(task_name, i)\n        await asyncio.sleep(1)\n    print(\"done!\")\n\n\nasync def main():\n    await countdown(\"simple count\", 4)\n\n\nasyncio.run(main())\n\n\"\"\" Output\nsimple count 4\nsimple count 3\nsimple count 2\nsimple count 1\ndone!\n\"\"\"\nIn the example above, countdown(), asyncio.sleep() and main() are coroutines.\ncountdown() and asyncio.sleep() were called with await , however, main() was called with asyncio.run()\n\nIn asyncio, all awaitable objects must run inside an event loop.\n\nThe method asyncio.run() creates a “top-level” event loop\n\n\n\n\n\nTasks are used to schedule coroutines concurrently.\nOnce a task is scheduled, it will run as soon as the event loop is available.\n\nTasks are scheduled (wrapped) with asyncio.create_task()\nIn order to run a task to completion, it must be awaited.\n\nasync def main():\n    # Scheduling and starting tasks\n    taskA = asyncio.create_task(countdown(\"task A\", 3))\n    taskB = asyncio.create_task(countdown(\"task B\", 2))\n\n    # Doing something else while they run\n    await asyncio.sleep(1)\n    print(\"Everything, everywhere, all at once\")\n\n    # Awaiting for tasks to complete before ending main()\n    # Try running this code without awaiting\n    await taskA\n    await taskB\n\n\nasyncio.run(main())\n\n\"\"\" Output\ntask A 3\ntask B 2\nEverything, everywhere, all at once\ntask A 2\ntask B 1\ntask A 1\ndone!\ndone!\n\"\"\"\nInstead of creating Tasks and awaiting them individually, it’s possible to run multiple coroutines concurrently and wait for all of them:\nasync def main():\n    # Running and awaiting multiple coroutines\n\n    await asyncio.gather(countdown(\"task A\", 4), countdown(\"task B\", 3))\n\n    await asyncio.sleep(1)\n    print(\"Everything, everywhere, all at once\")\n\n\nasyncio.run(main())\n\"\"\"\ntask A 4\ntask B 3\ntask A 3\ntask B 2\ntask A 2\ntask B 1\ntask A 1\ndone!\ndone!\nEverything, everywhere, all at once\n\"\"\"\n\n\n\nA Future in Asyncio is the equivalent of a Promise in JavaScript.\n\nIt is rare that developers need to use a Future directly.\n\n\n\n\n\nThe following are excellent resources for learning more about Asyncio:\n\nAsync IO in Python: A Complete Walkthrough by RealPython.com",
    "crumbs": [
      "Asynchronous progamming in Python"
    ]
  },
  {
    "objectID": "notes/azure-sdks/index.html",
    "href": "notes/azure-sdks/index.html",
    "title": "Azure SDKs in Python and C#",
    "section": "",
    "text": "Azure offers Software Development Kits (SDKs) for most of its services. Below you will find references to the SDKs used in the course.",
    "crumbs": [
      "Azure SDKs in Python and C#"
    ]
  },
  {
    "objectID": "notes/azure-sdks/index.html#azure-iot-sdks",
    "href": "notes/azure-sdks/index.html#azure-iot-sdks",
    "title": "Azure SDKs in Python and C#",
    "section": "1 Azure IoT SDKs",
    "text": "1 Azure IoT SDKs\nAzure IoT provides SDKs for several languages such as Python, Java, .NET, C and Node.js.\n\n\n\n\n\n\n\nFigure 1: Azure IoT SDKs provide tooling and examples to develop devices, service applications and to manage the IoT Hub itself.\n\n\nThere are a few categories of SDKs to know, which are managed and packaged separately:\n\nDevice SDKs\n\nFunctionality to build device clients that communicate with and are controlled by an IoT Hub.\n\nService SDKs\n\nFunctionality for applications and services that communicate with an IoT Hub and registered devices. Examples are a back-end service running on a VM or a mobile application.\n\nIoT Hub Management SDKs\n\nHelp build back-end applications that manage an IoT Hub.\n\n\n\n\n\n\n\n\nNote\n\n\n\nRemember that the IoT Hub is only managing the device registration, authentication and routing of messages and data.\nThe application logic and device control is typically done by a back-end service and user applications using data routed via the IoT Hub’s end-points.",
    "crumbs": [
      "Azure SDKs in Python and C#"
    ]
  },
  {
    "objectID": "notes/azure-sdks/index.html#sdks-by-language",
    "href": "notes/azure-sdks/index.html#sdks-by-language",
    "title": "Azure SDKs in Python and C#",
    "section": "2 SDKs by language",
    "text": "2 SDKs by language\n\nPythonC#\n\n\n\nPackages (pip)\n\nDevice: azure-iot-device\n\n\nService: azure-iot-hub\n\n\nIoTHub Management: azure-mgmt-iothub\n\nSource (GitHub)\n\nDevice: azure/azure-iot-sdk-python\n\n\nService: azure/azure-iot-hub-python\n\n\nIoTHub Management: azure/azure-sdk-for-python\n\nTutorials\n\nAll: Connect to IoT Hub (Python)\n\nExamples\n\nDevice: azure-iot-sdk-python/samples\n\n\nService: azure-iot-hub-python/samples\n\nReference\n\nDevice: learn.microsoft.com\n\n\nService: learn.microsoft.com\n\n\nIoTHub Management: learn.microsoft.com\n\n\n\n\n\nPackage (NuGet)\n\nDevice: Microsoft.Azure.Devices.Client\n\n\nService: Microsoft.Azure.Devices\n\n\nIoTHub Management: Azure.ResourceManager.IotHub\n\nSource (GitHub)\n\nDevice/Service: azure/azure-iot-sdk-csharp\n\n\nIoTHub Management: azure/azure-iot-for-net\n\nTutorials\n\nAll: Connect to IoT Hub (C#)\n\nExamples\n\nDevice: azure-iot-sdk-csharp/iothub/device/samples\n\n\nService: azure-iot-sdk-csharp/iothub/service/samples\n\nReference\n\nDevice/Service: learn.microsoft.com\n\n\nIoTHub management: learn.microsoft.com",
    "crumbs": [
      "Azure SDKs in Python and C#"
    ]
  },
  {
    "objectID": "notes/azure-sdks/index.html#installing-dotnet-on-developer-environment",
    "href": "notes/azure-sdks/index.html#installing-dotnet-on-developer-environment",
    "title": "Azure SDKs in Python and C#",
    "section": "3 Installing dotnet on developer environment",
    "text": "3 Installing dotnet on developer environment\n\n3.1 WSL\nThese instructions were adatped from learn.microsoft.com – see the linked document for more details if you run into issues.\n# Update system\n$ sudo apt update && sudo apt upgrade -y\n\n# Install required dependencies\n$ sudo apt install libc6 libgcc-s1 libgssapi-krb5-2 libicu72 libssl3 libstdc++6 zlib1g\n\n# Add dotnet repository to debian package manager\nwget https://packages.microsoft.com/config/debian/12/packages-microsoft-prod.deb -O packages-microsoft-prod.deb\nsudo dpkg -i packages-microsoft-prod.deb\nrm packages-microsoft-prod.deb\n\n# Install .NET 8\nsudo apt update && sudo apt install -y dotnet-sdk-8.0\n\n# Verify .NET8 installed\ndotnet --version\n\n3.1.1 Setup tab completions using .bashrc\nTo get tab completions set up for dotnet, add the following snippet to your .bashrc:\n\nfunction _dotnet_bash_complete()\n{\n  local cur=\"${COMP_WORDS[COMP_CWORD]}\" IFS=$'\\n' # On Windows you may need to use use IFS=$'\\r\\n'\n  local candidates\n  read -d '' -ra candidates &lt; &lt;(dotnet complete --position \"${COMP_POINT}\" \"${COMP_LINE}\" 2&gt;/dev/null)\n  read -d '' -ra COMPREPLY &lt; &lt;(compgen -W \"${candidates[*]:-}\" -- \"$cur\")\n}\n\ncomplete -f -F _dotnet_bash_complete dotnet\nSee learn.miscrosoft.com for more details.\n\n\n\n3.2 OSX\nTry the instructions here https://learn.microsoft.com/en-us/dotnet/core/install/macos\nNOTE: I haven’t tested these instructions myself.",
    "crumbs": [
      "Azure SDKs in Python and C#"
    ]
  },
  {
    "objectID": "notes/gpios/index.html",
    "href": "notes/gpios/index.html",
    "title": "GPIOs: Inputs and Outputs",
    "section": "",
    "text": "Note: most of these notes were adapted directly from the Raspberry Pi docs: GPIO-Pinout 1\n1 Raspberry Pi documentation is copyright © 2012-2024 Raspberry Pi Ltd and is licensed under a Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA) licence. Some content originates from the eLinux wiki, and is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported licence.The Raspberry Pi can read and generate digital signals using General Purpose Input and Output (GPIO) pins.\nAny of the GPIO pins can be designated (in software) as an input or output pin and used for a wide range of purposes.\n\n\n\nDiagram of Pi’s 40 GPIO pins\n\n\n\n GPIO and the 40-pin headers of the Raspberry Pi   - Official docs, Raspberry Pi Foundation.\n\nThe reTerminal exposes the same 40-pin header as the Pi on it’s side:\n\n\n\nBreakout of 40-pin header for reTerminal\n\n\n\n GPIO and pin diagram of the reTerminal   - reTerminal Official Wiki, Seeed.\n\n\n\nTwo 5V pins and two 3V3 pins are present on the board, as well as a number of ground pins (0V), which are unconfigurable. The remaining pins are all general purpose 3V3 pins, meaning outputs are set to 3V3 and inputs are 3V3-tolerant.\n\n\n\nA GPIO pin designated as an output pin can be set to high (3V3) or low (0V).\n\n\n\nA GPIO pin designated as an input pin can be read as high (3V3) or low (0V).\nWe will learn how to use the GPIOs in a future lesson.\n\n\n\nIn addition to simple input and output devices, the GPIO pins can be used with a variety of alternative functions and digital communication protocols.\nThese digital communication protocols are:\n\nPWM (pulse-width modulation)\nSPI (serial peripheral interface)\nI2C (inter-integrated circuit)\nSerial\nPCM (pulse-code modulation)\n\nSome digital functions are available on all pins, others on specific pins.\n\n\n\n\nThe Raspberry Pi does not have an ADC.\nIn order to process analog electrical signals an external ADC must be used.\n\nHat to the Rescue\nIn this course we will use the integrate ADC of the Grove Base Hat for the Raspberry Pi.\n\n\n\nGrove Base Hat for the Raspberry Pi\n\n\n\n Seeed’s Grove Base Hat for the Raspberry Pi has an integrated ADC   Base Hat official wiki, Seeed.\n\nPi HATs is the term for expansion boards for the Raspberry Pi.\n\nSeeed’s Grove Base Hat for the Raspberry Pi has 4 connectors with integrated ADC.\nEach ADC connector has 12-bit resolution.\n\nIn addition to the 4 ADC connectors, the Base Hat also exposes the original 40-pin header and other digital connectors.\n\n\n\nConnectors of the Grove Base Hat\n\n\n\n Specialized connectors of the Raspberry Pi available via the Base Hat   Base Hat official wiki, Seeed.",
    "crumbs": [
      "GPIOs: Inputs and Outputs"
    ]
  },
  {
    "objectID": "notes/gpios/index.html#intro-to-pis-gpios",
    "href": "notes/gpios/index.html#intro-to-pis-gpios",
    "title": "GPIOs: Inputs and Outputs",
    "section": "",
    "text": "Note: most of these notes were adapted directly from the Raspberry Pi docs: GPIO-Pinout 1\n1 Raspberry Pi documentation is copyright © 2012-2024 Raspberry Pi Ltd and is licensed under a Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA) licence. Some content originates from the eLinux wiki, and is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported licence.The Raspberry Pi can read and generate digital signals using General Purpose Input and Output (GPIO) pins.\nAny of the GPIO pins can be designated (in software) as an input or output pin and used for a wide range of purposes.\n\n\n\nDiagram of Pi’s 40 GPIO pins\n\n\n\n GPIO and the 40-pin headers of the Raspberry Pi   - Official docs, Raspberry Pi Foundation.\n\nThe reTerminal exposes the same 40-pin header as the Pi on it’s side:\n\n\n\nBreakout of 40-pin header for reTerminal\n\n\n\n GPIO and pin diagram of the reTerminal   - reTerminal Official Wiki, Seeed.\n\n\n\nTwo 5V pins and two 3V3 pins are present on the board, as well as a number of ground pins (0V), which are unconfigurable. The remaining pins are all general purpose 3V3 pins, meaning outputs are set to 3V3 and inputs are 3V3-tolerant.\n\n\n\nA GPIO pin designated as an output pin can be set to high (3V3) or low (0V).\n\n\n\nA GPIO pin designated as an input pin can be read as high (3V3) or low (0V).\nWe will learn how to use the GPIOs in a future lesson.\n\n\n\nIn addition to simple input and output devices, the GPIO pins can be used with a variety of alternative functions and digital communication protocols.\nThese digital communication protocols are:\n\nPWM (pulse-width modulation)\nSPI (serial peripheral interface)\nI2C (inter-integrated circuit)\nSerial\nPCM (pulse-code modulation)\n\nSome digital functions are available on all pins, others on specific pins.\n\n\n\n\nThe Raspberry Pi does not have an ADC.\nIn order to process analog electrical signals an external ADC must be used.\n\nHat to the Rescue\nIn this course we will use the integrate ADC of the Grove Base Hat for the Raspberry Pi.\n\n\n\nGrove Base Hat for the Raspberry Pi\n\n\n\n Seeed’s Grove Base Hat for the Raspberry Pi has an integrated ADC   Base Hat official wiki, Seeed.\n\nPi HATs is the term for expansion boards for the Raspberry Pi.\n\nSeeed’s Grove Base Hat for the Raspberry Pi has 4 connectors with integrated ADC.\nEach ADC connector has 12-bit resolution.\n\nIn addition to the 4 ADC connectors, the Base Hat also exposes the original 40-pin header and other digital connectors.\n\n\n\nConnectors of the Grove Base Hat\n\n\n\n Specialized connectors of the Raspberry Pi available via the Base Hat   Base Hat official wiki, Seeed.",
    "crumbs": [
      "GPIOs: Inputs and Outputs"
    ]
  },
  {
    "objectID": "notes/gpios/index.html#gpio-pinout",
    "href": "notes/gpios/index.html#gpio-pinout",
    "title": "GPIOs: Inputs and Outputs",
    "section": "2 GPIO pinout",
    "text": "2 GPIO pinout\nA GPIO reference can be accessed on your Raspberry Pi by opening a terminal window and running the command pinout. This tool is provided by the GPIO Zero Python library, which is installed by default in Raspberry Pi OS.\n$ pinout\n\n\n\n\n\n\nImportant\n\n\n\nWhile connecting up simple components to the GPIO pins is perfectly safe, it’s important to be careful how you wire things up. LEDs should have resistors to limit the current passing through them. Do not use 5V for 3.3V components. Do not connect motors directly to the GPIO pins, instead use an H-bridge circuit or a motor controller board.\n\n\n\n2.1 Permissions\nIn order to use the GPIO ports, your user must be a member of the gpio group. The default user account is a member by default, other users need to be added manually.\n# Only do this to add a NEW user to the gpio group\nsudo usermod -a -G gpio &lt;username&gt;",
    "crumbs": [
      "GPIOs: Inputs and Outputs"
    ]
  },
  {
    "objectID": "notes/gpios/index.html#gpio-in-python",
    "href": "notes/gpios/index.html#gpio-in-python",
    "title": "GPIOs: Inputs and Outputs",
    "section": "3 GPIO in Python",
    "text": "3 GPIO in Python\nUsing the GPIO Zero library makes it easy to control GPIO devices with Python. The library is comprehensively documented at gpiozero.readthedocs.io.\n\n3.1 LED\nTo control an LED connected to GPIO17:\nfrom gpiozero import LED\nfrom time import sleep\n\nled = LED(17)\n\nwhile True:\n    led.on()\n    sleep(1)\n    led.off()\n    sleep(1)\nLED methods include on(), off(), toggle(), and blink().\n\n\n3.2 Button\nTo read the state of a button connected to GPIO2:\nfrom gpiozero import Button\nfrom time import sleep\n\nbutton = Button(2)\n\nwhile True:\n    if button.is_pressed:\n        print(\"Pressed\")\n    else:\n        print(\"Released\")\n    sleep(1)\nButton functionality includes the properties is_pressed and is_held; callbacks when_pressed, when_released, and when_held; and methods wait_for_press() and wait_for_release.\n\n\n3.3 Button and LED\nTo connect the LED and button together, you can use this code:\nfrom gpiozero import LED, Button\n\nled = LED(17)\nbutton = Button(2)\n\nwhile True:\n    if button.is_pressed:\n        led.on()\n    else:\n        led.off()\nAlternatively:\nfrom gpiozero import LED, Button\n\nled = LED(17)\nbutton = Button(2)\n\nwhile True:\n    button.wait_for_press()\n    led.on()\n    button.wait_for_release()\n    led.off()\nor:\nfrom gpiozero import LED, Button\n\nled = LED(17)\nbutton = Button(2)\n\nbutton.when_pressed = led.on\nbutton.when_released = led.off",
    "crumbs": [
      "GPIOs: Inputs and Outputs"
    ]
  },
  {
    "objectID": "notes/bash-essentials/index.html",
    "href": "notes/bash-essentials/index.html",
    "title": "Bash essentials",
    "section": "",
    "text": "Image source",
    "crumbs": [
      "Bash essentials"
    ]
  },
  {
    "objectID": "notes/bash-essentials/index.html#using-cli-effectively",
    "href": "notes/bash-essentials/index.html#using-cli-effectively",
    "title": "Bash essentials",
    "section": "1 Using CLI effectively",
    "text": "1 Using CLI effectively\nFirst things first: the terminal can feel awkward to use. What can we do about this?\nEach section below is some set of tips for using the interactive bash CLI effectively.\n\n1.1 Keyboard shortcuts\n\n\n\n\n\n\nKeyboard Shortcuts: details & examples\n\n\n\n\n\nThis section was adapted from (“Bash Keyboard Shortcuts - Linux - SS64.com” n.d.).\n\n1.1.1 Completions\nUse TAB completion for file/directory names. Type just enough characters to uniquely identify the item.\nFor example, to move to a directory sample1; Type cd sam. Then press TAB and ENTER.\n\n\n1.1.2 Moving the cursor\n\nCtrl+a: Go to the beginning of the line (Home).\nCtrl+e: Go to the End of the line (End).\nCtrl+p: Previous command (Up arrow).\nCtrl+n: Next command (Down arrow).\nAlt+b: Back (left) one word.\nAlt+f: Forward (right) one word.\nCtrl+f: Forward one character.\nCtrl+b: Backward one character.\n\n\n\n1.1.3 While using man or command --help | less\n\nk: Scroll up one line\nj: Scroll down one line\nCtrl+u: Page up\nCtrl+d: Page down\n/: Begin forward search\n?: Begin reverse search\nn/N: Find next/previous match\nq: close the less pager\n\n\n\n1.1.4 Editing\n\nCtrl+L: Clear the Screen, similar to the clear command.\nAlt+Del: Delete the Word before the cursor.\nAlt+d: Delete the Word after the cursor.\nCtrl+d: Delete character under the cursor.\nCtrl+h: Delete character before the cursor (Backspace).\nCtrl+w: Cut the Word before the cursor to the clipboard.\nCtrl+k: Cut the Line after the cursor to the clipboard.\nCtrl+u: Cut/delete the Line before the cursor to the clipboard.\nAlt+t: Swap current word with previous.\nCtrl+t: Swap the last two characters before the cursor (typo).\nctrl+y: Paste the last thing to be cut (yank).\nAlt+u: UPPER capitalize every character from the cursor to the end of the current word.\nAlt+l: Lower the case of every character from the cursor to the end of the current word.\nAlt+c: Capitalize the character under the cursor and move to the end of the word.\nAlt+r: Cancel the changes and put back the line as it was in the history (revert).\nctrl+_: Undo.\n\n\n\n1.1.5 Special keys\n\nCtrl+v tells the terminal to not interpret the following character\n\nso Ctrl+v TAB will display a tab character rather than attempting completion.\nsimilarly Ctrl+v ENTER will display the escape sequence for the Enter key: ^M\n\n\n\n\n1.1.6 History\n\nCtrl+r: Recall the last command including the specified character(s).\nCtrl+p: Previous command in history (walk back).\nCtrl+n: Next command in history (walk forward).\nCtrl+o: Execute the command found via Ctrl+r or Ctrl+s Ctrl+o\nCtrl+g: Escape from history searching mode.\n\n\n\n1.1.7 Process Control\n\nCtrl+c: Interrupt/Kill whatever you are running (SIGINT).\nCtrl+l: Clear the screen.\nCtrl+s: Stop output to the screen (for long running verbose commands). Then use PgUp/PgDn for navigation.\nCtrl+q: Allow output to the screen (if previously stopped using command above).\nCtrl+d: Send an EOF marker, unless disabled by an option, this will close the current shell (EXIT).\nCtrl+z: Send the signal SIGTSTP to the current task, which suspends it. To return to it later enter fg 'process name'\n\n\n\n\n\n\n\n1.2 Configuration\n\n\n\n\n\n\n.bashrc details & examples\n\n\n\n\n\nEvery time you open a new terminal window/tab in the bash shell, the ~/.bashrc file is read and executed.\nThe typical usecases for customising ~/.bashrc are:\n\nsetting a custom Command prompt\nsetting various useful shopts (shell options)\nsetting environment variables/aliases\nsourcing other bash files\n\nExamples of each of these are shown below.\n\n\n~/.bashrc\n\n# Set a prompt like: [username@hostname:~/CurrentWorkingDirectory]$\nexport PS1='[\\u@\\h:\\w]\\$ '\n#   Explanation:\n#   \\u: username\n#   \\h: hostname\n#   \\w: the current working directory\n#   \\$: the character $\n#   all other characters are interpreted literally\n#   See https://ss64.com/bash/syntax-prompt.html for more examples\n\n# Set useful shell options\nshopt -s autocd # auto-appends `cd` to directory, so you can cd to a path without writing `cd`\nshopt -s globstar # enables the pattern '**' for recursive file/directory wildcard matching\nshopt -s extglob # fancier pattern matching\n# See https://www.gnu.org/software/bash/manual/html_node/The-Shopt-Builtin.html for more options\n\n# Set environment variables/aliases\nexport EDITOR=\"nvim\" # neovim\nexport EDITOR=\"code\" # vscode (overwrites previous line)\n# See https://ss64.com/bash/export.html for more information\n\nalias ll=\"ls -l\" # create new alias ll for a long list\nalias cp=\"cp -iv\" # replace default cp command with interactive/verbose cp\n# See https://ss64.com/bash/alias.html for more alias info and examples\n# Note that aliases cannot handle complex logic or accept positional parameters\n# For that, we would need functions.\n\n# Source all bash files in ~/.bashrc.d/\n# This lets you define functions in various shell files in this folder and source them at startup.\nif [ -d ~/.bashrc.d ]; then\n    for rc in ~/.bashrc.d/*; do\n        if [ -f \"$rc\" ]; then\n            source \"$rc\"\n        fi\n    done\nfi\nunset rc\n# See https://ss64.com/bash/source.html for more info on sourcing\n\n\n1.2.1 Ever Wonder Why it’s Called .bashrc?\nThere are many files that end with the mysterious suffix rc like .bashrc, .vimrc, etc. Why is that? It’s a holdover from ancient Unix. Its original meaning was “run commands,” but it later became “run-control.” A run-control file is generally some kind of script or configuration file that prepares an environment for a program to use. In the case of .bashrc for example, it’s a script that prepares a user’s bash shell environment.\n\n\n\n\n\n\n\n\n\n\n.profile details & examples\n\n\n\n\n\nEvery time you log in to a linux user, the ~/.profile file is read and executed.\nThe typical usecases for customizing ~/.profile are:\n\nsetting environment variables INDEPENDENT of bash instances\n\ni.e., these variables will work in sh, zsh, and other shells\n\nsetting environment variables once per session\n\nparticularly useful for PATH, since setting it in ~/.bashrc will cause it to be updated more frequently than useful\n\n\nExamples of these are shown below:\n\n\n~/.profile\n\n# Add a directory to PATH, checking if that directory is not already in PATH first\nif ! [[ \"$PATH\" =~ \"$HOME/bin:\" ]]; then\n  export PATH=\"$PATH:$HOME/bin\"  # Adds ~/bin to your path\nfi\n\n# Source all profile files in ~/.profile.d/\n# This is useful for programs like npm, you can put its bashrc/path stuff in here instead.\nfor script in $HOME/.profile.d/*.sh ; do\n    if [ -r \"$script\" ] ; then\n        . \"$script\"\n    fi\ndone\nunset script\n# See https://ss64.com/bash/source.html for more info on sourcing\n\n\n\n\n\n\n\n\n\n\n.inputrc details & examples\n\n\n\n\n\nThis section was adapted from (“How-To: Bash Startup Files Inputrc - Linux - SS64.com” n.d.).\nThe library that is used to implement a command line interface for bash is called the Readline library.\nWhile it comes with a set of default keybindings (see the #keyboard-shortcuts section), it is possible to modify these and other behaviors of the CLI interface by putting commands into a .inputrc file, typically in the home directory.\nThe configuration options in .inputrc are particularly useful for customising the way Tab-completion works, e.g. with the ls command.\nThe inputrc variable syntax is simple:\nset variable value\nBelow are a list of variables I find particularly useful, as well as a sample .inputrc file showing how each of these are set.\n\nbell-style\nControls what happens when Readline wants to ring the terminal bell. If set to ‘none’, Readline never rings the bell. If set to ‘visible’, Readline uses a visible bell if one is available. If set to ‘audible’ (the default), Readline attempts to ring the terminal’s bell.\ncompletion-ignore-case\nIf set to ‘on’, Readline performs filename matching and completion in a case-insensitive fashion. The default value is ‘off’.\nediting-mode\nThe editing-mode variable controls which default set of key bindings is used. By default, Readline starts up in Emacs editing mode, where the keystrokes are most similar to Emacs. This variable can be set to either ‘emacs’ or ‘vi’.\nmark-symlinked-directories\nIf set to ‘on’, completed names which are symbolic links to directories have a slash appended (subject to the value of mark-directories). The default is ‘off’.\nshow-all-if-ambiguous\nThis alters the default behavior of the completion functions. If set to ‘on’, words which have more than one possible completion cause the matches to be listed immediately instead of ringing the bell. The default value is ‘off’.\n\nA sample ~/.inputrc file with these variables in use:\n\n\n~/.inputrc\n\nset bell-style none\nset completion-ignore-case On\nset editing-mode vi\nset mark-symlinked-directories On\nset show-all-if-ambiguous On\n\nYou can find many more configuration options in (“How-To: Bash Startup Files Inputrc - Linux - SS64.com” n.d.).",
    "crumbs": [
      "Bash essentials"
    ]
  },
  {
    "objectID": "notes/bash-essentials/index.html#bare-necessities",
    "href": "notes/bash-essentials/index.html#bare-necessities",
    "title": "Bash essentials",
    "section": "2 Bare necessities",
    "text": "2 Bare necessities\nThe following sections explain the purpose of each command and show a few use cases and useful options.\nThese are commands you probably already know – if you don’t, you’ll know by the end of lab-0, as you’ll need them all!\n\n2.1 Getting around: cd and ls\nNAME\n  cd - change the current directory\n  ls - list directory contents\n\nSYNOPSIS\n  cd [DIR]\n  ls [OPTION]... [FILE]...\n\n\n\n\n\n\ncd & ls details & examples\n\n\n\n\n\n\n2.1.1 cd\nUseful shorthands for cd to know:\n# Change to user home directory \n# (usually: /home/username)\n$ cd ~\n\n# WSL: Change to Windows mounted directory\n$ cd /mnt/c/\n\n# Return to previous directory\n$ cd -    # in this case, /home/username\n\n\n2.1.2 ls\nUseful ls options:\n-l                     use a long listing format\n-a, --all              do not ignore entries starting with .\n-d, --directory        list directories themselves, not their contents\n-s, --size             print the allocated size of each file, in blocks\n-t                     sort by time, newest first; see --time\n-h, --human-readable   with -l and -s, print sizes like 1K 234M 2G etc.\n    --si               likewise, but use powers of 1000 not 1024\n-R, --recursive        list subdirectories recursively\n\n\n\n\n\n\n2.2 Viewing files: cat and tac\nNAME\n  cat - concatenate files and print on the standard output\n  tac - concatenate and print files in reverse\n\nSYNOPSIS\n  cat [OPTION]... [FILE]...\n  tac [OPTION]... [FILE]...\n\n\n2.3 Creating files: touch and mkdir\nNAME\n  touch - Update the modification times of each `FILE` to the current time.\n          Creates the files if they do not exist.\n  mkdir - Create the given DIRECTORY(ies) if they do not exist\n\nSYNOPSIS\n  touch [FILE]...\n  mkdir [-p/--parents] [DIRECTORY]...\n\n\n2.4 Moving files: mv and cp\n\nNAME\n  mv - Move `SOURCE` to `DEST`, or multiple `SOURCE`(s) to `DIRECTORY`.\n  cp - Copy SOURCE to DEST, or multiple SOURCE(s) to DIRECTORY.\n\nSYNOPSIS\n  mv  [-f/--force] [-i/--interactive] [-g/--progress] [SOURCE]... [DEST]\n  cp  [-f/--force] [-i/--interactive] [-g/--progress] [-R/--recursive] [SOURCE]... [DEST]\n\n\n\n2.5 Managing permissions: chmod and chown\nNAME\n  chmod - Change the permissions mode of each FILE to MODE.\n  chown - Change file owner and group of each FILE to USER:GROUP\n\nSYNOPSIS\n  chmod [-R/--recursive] [MODE] [FILE]\n  chown [-R/--recursive] [USER:GROUP] [FILE]\n\n\n2.6 Deleting files: rm\nNAME\n  rm - Remove the FILE(s)\n\nSYNOPSIS\n  rm [-f/--force] [-i/--interactive] [-r/--recursive] [FILE]...",
    "crumbs": [
      "Bash essentials"
    ]
  },
  {
    "objectID": "notes/bash-essentials/index.html#the-five-fingers-of-death",
    "href": "notes/bash-essentials/index.html#the-five-fingers-of-death",
    "title": "Bash essentials",
    "section": "3 The five fingers of death",
    "text": "3 The five fingers of death\n\n\n\n\n\nFive Fingers of Death, or King Boxer as it is known on Wikipedia, is a martial-arts movie I have not seen, but I have heard referenced in many songs. It speaks to me that the mastery of a seemlingly small set tools (five fingers) can lead to drastic increases in capability (the ability to inflict death) and I believe this spirit applies directly to working with unix tools. Image source\n\n\nThe following 5 sets of commands are indispensable GNU Coreutils that are included on all linux systems.\nThere are many more coreutils that I have not included – I have chosen these 5 sets as I believe that mastering them, above all, will bring you in harmony with your linux system, and therefore closer to truth, happiness, and the meaning of life – or, if not, at least they will help you solve the labs that I give you in this course.\nAlmost all of these notes are adapted from a resource I found that’s pretty much exactly what I wanted to write myself: (“CLI Text Processing with GNU Coreutils” n.d.). It comes with great explanations and exercises and solutions. I may base some quizzes and tests on it!\n\n3.1 find files and grep content\nNAME\n  find - search for files that match a given expression\n  grep - print lines in file(s) that match a given pattern\n\nSYNOPSIS\n  find [STARTING-POINT...] [OPTION...] [EXPRESSION]\n  grep [OPTION...] PATTERNS [FILE...]\n\n\n\n\n\n\nfind details & examples\n\n\n\n\n\n3.1.1 find\nThis section was adapted from (“A Practical Guide to GNU Find With Examples” 2023)\nLet’s begin by looking first at find’s general syntax:\nfind [STARTING-POINT...] [OPTION...] [EXPRESSION]\nWhat are these different elements?\n\n\n\n\n\n\n\n\nElement\nDescription\nDefault\n\n\n\n\n[OPTION...]\nOptions are arguments about symlinks and search optimization.\nNone\n\n\n[STARTING-POINT...]\nList of directories to search through. The subdirectories are recursively included.\nCurrent directory\n\n\n[EXPRESSION]\nList of expressions with their (often required) values.\nNone\n\n\n\nNothing is mandatory here: running find alone will give you some output.\nHere are the different categories of [EXPRESSION]. Each of these are queries describing how to match files, or what action to perform on these files. They’re always prefixed with a single dash - (like -name for example).\n\n\n\n\n\n\n\nCategory\nDescription\n\n\n\n\nTest expressions\nMost common expressions. They’re used to filtering your files.\n\n\nAction expressions\nExpressions used to perform an action on each file found.\n\n\nOperators\nBoolean operators to manage the relationships between the different expressions.\n\n\n\nLet’s see an example that demonstrates these categories:\n\n\nbash\n\nfind . -name '*.png' -or -perm '664' -delete\n\nThis will recursively search the current directory for all files that EITHER have a filename ending with .png OR that has the permissions 664, then will delete those files. (see the course notes on permissions for more details on the meaning of 664 here.)\nLet’s see what category each of these expressions is:\n\n\n\nExpression\nCategory\n\n\n\n\n-name and -perm\nTest expressions\n\n\n-delete\nAction expression\n\n\n-or\nOperator expression\n\n\n\nThere are, of course, many different Test/Action/Operator expressions, and the beauty of the find command is combining each of these types of expressions to create stunningly efficient file search commands.\nI recommend reading/bookmarking the following resources for great explanations and examples of the various uses for the find command:\n\n“Why is using a shell loop to process text considered bad practice?” on unix.stackexchange\n“Why looping over find’s output bad practice?” on unix.stackexchange\n\n\n\n\n\n\n\n\n\n\ngrep details & examples\n\n\n\n\n\n3.1.2 grep\nThis section was adapted from (“Mastering Linux ‘Grep’ Command Guide with Practical Examples” 2024)\nThe Linux grep command is one of the most powerful and frequently used tools for text search and data filtering. Whether you’re managing system logs, searching through files, or debugging code, grep helps you find specific patterns within large sets of data quickly and efficiently.\nThe basic syntax of grep is as follows:\ngrep [OPTION...] PATTERNS [FILE...]\n\n[OPTION...]: various options you can provide grep to modify the default behavior.\nPATTERNS: the string or regular expression you want to search for.\n[FILE...]: the file(s) where you want to search.\n\n\n3.1.2.1 Practical examples\nEssentially, grep searches for a pattern and displays the matching lines. Here are a few common use-cases:\n\n3.1.2.1.1 Example 1: Searching for a Word in a File\nIf you want to search for a specific word in a file, the most basic command would be:\ngrep \"word\" filename.txt\nThis will return all lines in filename.txt that contain the word “word.”\n\n\n3.1.2.1.2 Example 2: Case-Insensitive Search\nBy default, grep is case-sensitive. If you want to ignore case distinctions, you can use the -i option:\ngrep -i \"word\" filename.txt\nThis command will return matches for both “Word” and “word” in filename.txt.\n\n\n3.1.2.1.3 Example 3: Searching Across Multiple Files\nTo search for a pattern in multiple files at once, you can use wildcards *:\ngrep \"word\" *.txt\nThis will search for “word” in all .txt files in the current directory.\n\n\n3.1.2.1.4 Example 4: Displaying Line Numbers\nTo see the line numbers where the matches occur, use the -n option:\ngrep -n \"word\" filename.txt\nThis command will display the line numbers along with the matching lines.\n\n\n3.1.2.1.5 Example 5: Recursive Search in Directories\nIf you want to search for a pattern across all files in a directory and its subdirectories, use the -r (recursive) option:\ngrep -r \"word\" /path/to/directory/\nThis will search for “word” in all files within /path/to/directory/, including subdirectories.\n\n\n3.1.2.1.6 Example 6: Inverting Search (Exclude a Pattern)\nIf you want to exclude lines that contain a specific pattern, you can use the -v option:\ngrep -v \"word\" filename.txt\nThis command will return all lines that do not contain “word”.\n\n\n3.1.2.1.7 Example 7: Counting Matches\nTo count how many times a pattern appears in a file, use the -c option:\ngrep -c \"word\" filename.txt\nThis will output the number of lines that contain “word” in filename.txt.\n\n\n\n\n\n\n\n3.2 tr characters and cut fields\nNAME\n  tr - Translate characters matching STRING1 in stdin/FILE to STRING2,\n       writing to stdout\n  cut - Prints specified columns from each line of stdin, writes to stdout\n\nSYNOPSIS\n  tr [OPTION]... STRING1 STRING2\n  cut [-d/--delimiter] [-f/--fields] [FILE]\n\n\n\n\n\n\ntr details & examples\n\n\n\n\n\n3.2.1 tr\nThe following section was adapted from (“CLI Text Processing with GNU Coreutils” n.d.)\ntr helps you to map one set of characters to another set of characters. Features like range, repeats, character sets, squeeze, complement, etc makes it a must know text processing tool.\nHere are some examples that map one set of characters to another. As a good practice, always enclose the sets in single quotes to avoid issues due to shell metacharacters.\n\n\nbash\n\n# 'l' maps to '1', 'e' to '3', 't' to '7' and 's' to '5'\n$ echo 'leet speak' | tr 'lets' '1375'\n1337 5p3ak\n\n# example with shell metacharacters\n$ echo 'apple;banana;cherry' | tr\n:\ntr: missing operand\nTry 'tr --help' for more information.\n$ echo 'apple;banana;cherry' | tr ';' ':'\napple:banana:cherry\n\n\n3.2.1.1 Character ranges\nYou can use - between two characters to construct a range (ascending order only).\n\n\nbash\n\n# uppercase to lowercase\n$ echo 'HELLO WORLD' | tr 'A-Z' 'a-z'\nhello world\n\n# swap case\n$ echo 'Hello World' | tr 'a-zA-Z' 'A-Za-z'\nhELLO wORLD\n\n# rot13\n$ echo 'Hello World' | tr 'a-zA-Z' 'n-za-mN-ZA-M'\nUryyb Jbeyq\n$ echo 'Uryyb Jbeyq' | tr 'a-zA-Z' 'n-za-mN-ZA-M'\nHello World\n\n\n\n3.2.1.2 Deleting characters\nUse the -d option to specify a set of characters to be deleted.\n$ echo ‘2024-08-12’ | tr -d ‘-’\n\n\nbash\n\n20240812\n\n# delete all punctuation characters\n$ s='\"Hi\", there! How *are* you? All fine here.'\n$ echo \"$s\" | tr -d '[:punct:]'\nHi there How are you All fine here\n\n\n\n3.2.1.3 Squeezing characters\nThe -s option changes consecutive repeated characters to a single copy of that character.\n\n\nbash\n\n$ echo 'HELLO... hhoowwww aaaaaareeeeee yyouuuu!!' | tr -s 'a-z'\nHELLO... how are you!!\n\n# translate and squeeze\n$ echo 'hhoowwww aaaaaareeeeee yyouuuu!!' | tr -s 'a-z' 'A-Z'\nHOW ARE YOU!!\n\n# delete and squeeze\n$ echo 'hhoowwww aaaaaareeeeee yyouuuu!!' | tr -sd '!' 'a-z'\nhow are you\n\n# squeeze other than lowercase alphabets\n$ echo 'apple    noon     banana!!!!!' | tr -cs 'a-z'\napple noon banana!\n\nYou can see more examples and explanations here: (“CLI Text Processing with GNU Coreutils” n.d.).\n\n\n\n\n\n\n\n\n\n\ncut details & examples\n\n\n\n\n\n3.2.2 cut\nThe following section was adapted from (“CLI Text Processing with GNU Coreutils” n.d.)\nBy default, cut splits the input content into fields based on the tab (\\t) character. You can use the -f option to select a desired field from each input line. To extract multiple fields, specify the selections separated by the comma character.\n\n\nbash\n\n# only the second field\n$ printf 'apple\\tbanana\\tcherry\\n' | cut -f2\nbanana\n\n# first and third fields\n$ printf 'apple\\tbanana\\tcherry\\n' | cut -f1,3\napple cherry\n\n\n3.2.2.1 Field ranges\nYou can use the - character to specify field ranges. You can skip the starting or ending range, but not both.\n\n\nbash\n\n# 2nd, 3rd and 4th fields\n$ printf 'apple\\tbanana\\tcherry\\tfig\\tmango\\n' | cut -f2-4\nbanana cherry fig\n\n# all fields from the start till the 3rd field\n$ printf 'apple\\tbanana\\tcherry\\tfig\\tmango\\n' | cut -f-3\napple banana cherry\n\n# all fields from the 3rd one till the end\n$ printf 'apple\\tbanana\\tcherry\\tfig\\tmango\\n' | cut -f3-\ncherry fig mango\n\n\n\n3.2.2.2 Input Delimiter\nUse the -d option to change the input delimiter. Only a single byte character is allowed. By default, the output delimiter will be same as the input delimiter.\n\n\nbash\n\n$ cat scores.csv\nName,Maths,Physics,Chemistry\nIth,100,100,100\nCy,97,98,95\nLin,78,83,80\n\n$ cut -d, -f2,4 scores.csv\nMaths,Chemistry\n100,100\n97,95\n78,80\n\n# use quotes if the delimiter is a shell metacharacter\n$ echo 'one;two;three;four' | cut -d -f3\ncut: option requires an argument -- 'd'\nTry 'cut --help' for more information.\n-f3: command not found\n$ echo 'one;two;three;four' | cut -d';' -f3\nthree\n\n\n\n3.2.2.3 Output Delimiter\nUse the --output-delimiter option to customize the output separator to any string of your choice. The string is treated literally. Depending on your shell you can use ANSI-C quoting to allow escape sequences.\n\n\nbash\n\n$ printf 'apple\\tbanana\\tcherry\\n' | cut --output-delimiter=, -f1-\napple,banana,cherry\n\n# example for multicharacter output separator\n$ echo 'one;two;three;four' | cut -d';' --output-delimiter=' : ' -f1,3-\none : three : four\n\n# ANSI-C quoting example\n# depending on your environment, you can also press Ctrl+v and then the Tab key\n$ echo 'one;two;three;four' | cut -d';' --output-delimiter=$'\\t' -f1,3-\none three four\n\n# newline as the output field separator\n$ echo 'one;two;three;four' | cut -d';' --output-delimiter=$'\\n' -f2,4\ntwo\nfour\n\nYou can see more examples and explanations here: (“CLI Text Processing with GNU Coreutils” n.d.).\n\n\n\n\n\n\n3.3 sort data and uniq duplicates\nNAME\n  sort - Display sorted concatenation of all FILE(s).\n         With no FILE, or when FILE is -, read stdin\n  uniq - Report or omit repeated lines.\n\nSYNOPSIS\n  sort [FILE]...\n  uniq [-d/--repeated] [FILE]...\n\n\n\n\n\n\nsort details & examples\n\n\n\n\n\n3.3.1 sort\nThe following section was adapted from (“CLI Text Processing with GNU Coreutils” n.d.)\nThe sort command provides a wide variety of features. In addition to lexicographic ordering, it supports various numerical formats. You can also sort based on particular columns. And there are nifty features like merging already sorted input, debugging, determining whether the input is already sorted and so on.\nBy default, sort orders the input in ascending order:\n\n\nbash\n\n$ cat greeting.txt\nHi there\nHave a nice day\n\n# extract and sort space separated words\n$ &lt;greeting.txt tr ' ' '\\n' | sort\na\nday\nHave\nHi\nnice\nthere\n\n\n3.3.1.1 Dictionary sort\nThe -d option will consider only alphabets, numbers and blanks for sorting. Space and tab characters are considered as blanks, but this would also depend on the locale.\n\n\nbash\n\n$ printf '(banana)\\n{cherry}\\n[apple]' | LC_ALL=C sort -d\n[apple]\n(banana)\n{cherry}\n\n\n\n3.3.1.2 Reversed order\nThe -r option will reverse the output order. Note that this doesn’t change how sort performs comparisons, only the output is reversed. You’ll see an example later where this distinction becomes clearer.\n\n\nbash\n\n$ printf 'peace\\nrest\\nquiet' | sort -r\nrest\nquiet\npeace\n\n\n\n3.3.1.3 Numeric sort\nThe sort command provides various options to work with numeric formats. For most cases, the -n option is enough. Here’s an example:\n\n\nbash\n\n# lexicographic ordering isn't suited for numbers\n$ printf '20\\n2\\n3\\n111\\n314' | sort\n111\n2\n20\n3\n314\n\n# -n helps in this case\n$ printf '20\\n2\\n3\\n111\\n314' | sort -n\n2\n3\n20\n111\n314\n\n\n\n\n\n\n\n\n\n\n\nuniq details & examples\n\n\n\n\n\n3.3.2 uniq\nThe following section was adapted from (“CLI Text Processing with GNU Coreutils” n.d.)\nThe uniq command identifies similar lines that are adjacent to each other. There are various options to help you filter unique or duplicate lines, count them, group them, etc.\n\n3.3.2.1 Retain single copy of duplicates\nThis is the default behavior of the uniq command. If adjacent lines are the same, only the first copy will be displayed in the output.\n\n\nbash\n\n# only the adjacent lines are compared to determine duplicates\n# which is why you get 'red' twice in the output for this input\n$ printf 'red\\nred\\nred\\ngreen\\nred\\nblue\\nblue' | uniq\nred\ngreen\nred\nblue\n\nYou’ll need sorted input to make sure all the input lines are considered to determine duplicates. For some cases, sort -u is enough, like the example shown below:\n\n\nbash\n\n# same as sort -u for this case\n$ printf 'red\\nred\\nred\\ngreen\\nred\\nblue\\nblue' | sort | uniq\nblue\ngreen\nred\n\nSometimes though, you may need to sort based on some specific criteria and then identify duplicates based on the entire line contents. Here’s an example:\n\n\nbash\n\n# can't use sort -n -u here\n$ printf '2 balls\\n13 pens\\n2 pins\\n13 pens\\n' | sort -n | uniq\n2 balls\n2 pins\n13 pens\n\n\n\n3.3.2.2 Duplicates only\nThe -d option will display only the duplicate entries. That is, only if a line is seen more than once.\n\n\nbash\n\n$ cat purchases.txt\ncoffee\ntea\nwashing powder\ncoffee\ntoothpaste\ntea\nsoap\ntea\n\n$ sort purchases.txt | uniq -d\ncoffee\ntea\n\nTo display all the copies of duplicates, use the -D option.\n\n\nbash\n\n$ sort purchases.txt | uniq -D\ncoffee\ncoffee\ntea\ntea\ntea\n\n\n\n3.3.2.3 Unique only\nThe -u option will display only the unique entries. That is, only if a line doesn’t occur more than once.\n\n\nbash\n\n$ sort purchases.txt | uniq -u\nsoap\ntoothpaste\nwashing powder\n\n# reminder that uniq works based on adjacent lines only\n$ printf 'red\\nred\\nred\\ngreen\\nred\\nblue\\nblue' | uniq -u\ngreen\nred\n\nYou can see more examples and explanations here: (“CLI Text Processing with GNU Coreutils” n.d.).\n\n\n\n\n\n\n3.4 know head from tail\nNAME\n  head - Print the first 10 lines of each `FILE` to standard output.\n         With no `FILE`, or when `FILE` is `-`, read stdin\n  tail - Print the last 10 lines of each `FILE` to standard output.\n         With no `FILE`, or when `FILE` is `-`, read stdin\n\nSYNOPSIS\n  head [-n/--lines] [FILE]...\n  tail [-n/--lines] [-f/--follow] [FILE]...\n\n\n\n\n\n\nhead & tail details & examples\n\n\n\n\n\n3.4.1 head and tail\nThe following section was adapted from (“CLI Text Processing with GNU Coreutils” n.d.)\nhead and tail, or a combination of both, are used to extract text content that you know is at the beginning, end, or specific line number of a file.\n\n3.4.1.1 Leading and trailing lines\nConsider this sample file, with line numbers prefixed for convenience.\n\n\nbash\n\n$ cat sample.txt\n 1) Hello World\n 2) \n 3) Hi there\n 4) How are you\n 5) \n 6) Just do-it\n 7) Believe it\n 8) \n 9) banana\n10) papaya\n11) mango\n12) \n13) Much ado about nothing\n14) He he he\n15) Adios amigo\n\nBy default, head and tail will display the first and last 10 lines respectively.\n\n\nbash\n\n$ head sample.txt\n 1) Hello World\n 2) \n 3) Hi there\n 4) How are you\n 5) \n 6) Just do-it\n 7) Believe it\n 8) \n 9) banana\n10) papaya\n\n$ tail sample.txt\n 6) Just do-it\n 7) Believe it\n 8) \n 9) banana\n10) papaya\n11) mango\n12) \n13) Much ado about nothing\n14) He he he\n15) Adios amigo\n\nNote: If there are less than 10 lines in the input, only those lines will be displayed.\nYou can use the -nN option to customize the number of lines:\n\n\nbash\n\n# first three lines\n# space between -n and N is optional\n$ head -n3 sample.txt\n 1) Hello World\n 2) \n 3) Hi there\n\n# last two lines\n$ tail -n2 sample.txt\n14) He he he\n15) Adios amigo\n\n\n\n3.4.1.2 Excluding N lines\nBy using a “subtraction” style syntax, like head -n -N, you can invert the selection – that is, get all the input lines EXCEPT the last -N lines in the case of head, or the first -N lines in the case of tail.\n\n\nbash\n\n# except the last 11 lines\n$ head -n -11 sample.txt\n 1) Hello World\n 2) \n 3) Hi there\n 4) How are you\n\n# except the first 11 lines\n$ tail -n -11 sample.txt\n12) \n13) Much ado about nothing\n14) He he he\n15) Adios amigo\n\nYou can see more examples and explanation at (“CLI Text Processing with GNU Coreutils” n.d.).\n\n\n\n\n\n\n3.5 tree and tee\nNAME\n  tree - list contents of DIRECTORIES in a tree-like format.\n  tee - Copy standard input to each FILE, and also to standard output.\n\nSYNOPSIS\n  tree [-L level] [DIRECTORY]...\n  tee [FILE]...\n\n\n\n\n\n\ntree & tee details & examples\n\n\n\n\n\nThere’s nothing here yet… stay tuned!",
    "crumbs": [
      "Bash essentials"
    ]
  },
  {
    "objectID": "notes/bash-essentials/index.html#redirection-and-pipes",
    "href": "notes/bash-essentials/index.html#redirection-and-pipes",
    "title": "Bash essentials",
    "section": "4 Redirection and Pipes",
    "text": "4 Redirection and Pipes\nThis section was adapted from (“How-To: Redirection and Process Substitution - Linux - SS64.com” n.d.)\nWhen Bash starts, normally, 3 file descriptors are opened, 0, 1 and 2 also known as standard input (stdin), standard output (stdout) and standard error (stderr).\nYou can use the &gt; operator to “redirect” the output of commands (which normally goes to stdout) to different files or other file descriptors. Some common examples are shown below:\ncommand  &gt;  filename     Redirect command output (stdout) into a file\ncommand  &gt;  /dev/null    Discard stdout of command\ncommand  2&gt; /dev/null    Discard stderr of command\n\ncommand  &gt;&2             Redirect command output (stdout) to stderr\n\ncommand  &gt;&gt; filename     Redirect command output and APPEND into a file\ncommand  &lt;  filename     Redirect a file into a command\n\ncommandA | commandB       Pipe stdout of commandA to commandB\ncommandA | tee filename   Pipe stdout of commandA into filename AND stdout\n\n\n\n\n\n\nRedirection explained further\n\n\n\n\n\nThis section was adapted from (“Illustrated Redirection Tutorial [Bash Hackers Wiki]” 2023)\n\n4.1 Output Redirection n&gt; file\n&gt; is probably the simplest redirection.\necho foo &gt; file\nthe &gt; file after the command alters the file descriptors belonging to the command echo. It changes the file descriptor 1 (&gt; file is the same as 1&gt;file) so that it points to the file file. They will look like:\n                  ---       +-----------------------+\nstandard input   ( 0 ) ----&gt;| /dev/pts/5            |\n                  ---       +-----------------------+\n\n                  ---       +-----------------------+\nstandard output  ( 1 ) ----&gt;| file                  |\n                  ---       +-----------------------+\n\n                  ---       +-----------------------+\nstandard error   ( 2 ) ----&gt;| /dev/pts/5            |\n                  ---       +-----------------------+\nNow characters written by our command, echo, that are sent to the standard output, i.e., the file descriptor 1, end up in the file named file.\nIn the same way, command 2&gt; file will change the standard error and will make it point to file. For example, command 2&gt; /dev/null will delete all errors outputted by command:\n                  ---       +-----------------------+\nstandard input   ( 0 ) ----&gt;| /dev/pts/5            |\n                  ---       +-----------------------+\n\n                  ---       +-----------------------+\nstandard output  ( 1 ) ----&gt;| /dev/pts/5            |\n                  ---       +-----------------------+\n\n                  ---       +-----------------------+\nstandard error   ( 2 ) ----&gt;| /dev/null             |\n                  ---       +-----------------------+\n\n\n4.2 Input Redirection n&lt; file\nWhen you run a command using command &lt; file, it changes the file descriptor 0 so that it looks like:\n                  ---       +-----------------------+\nstandard input   ( 0 ) &lt;----| file                  |\n                  ---       +-----------------------+\n\n                  ---       +-----------------------+\nstandard output  ( 1 ) ----&gt;| /dev/pts/5            |\n                  ---       +-----------------------+\n\n                  ---       +-----------------------+\nstandard error   ( 2 ) ----&gt;| /dev/pts/5            |\n                  ---       +-----------------------+\nIf the command reads from stdin, it now will read from file and not from the console.\n\n\n4.3 Pipes |\nWhat does this | do? Among other things, it connects the standard output of the command on the left to the standard input of the command on the right. That is, it creates a special file, a pipe, which is opened as a write destination for the left command, and as a read source for the right command.\ncommand:   echo foo               |                cat\n\n ---       +--------------+               ---       +--------------+\n( 0 ) ----&gt;| /dev/pts/5   |     ------&gt;  ( 0 ) ----&gt;|pipe (read)   |\n ---       +--------------+    /          ---       +--------------+\n                              /\n ---       +--------------+  /            ---       +--------------+\n( 1 ) ----&gt;| pipe (write) | /            ( 1 ) ----&gt;| /dev/pts     |\n ---       +--------------+               ---       +--------------+\n\n ---       +--------------+               ---       +--------------+\n( 2 ) ----&gt;| /dev/pts/5   |              ( 2 ) ----&gt;| /dev/pts/    |\n ---       +--------------+               ---       +--------------+\nThis is possible because the redirections are set up by the shell before the commands are executed, and the commands inherit the file descriptors.",
    "crumbs": [
      "Bash essentials"
    ]
  },
  {
    "objectID": "notes/bash-essentials/index.html#core-utilities",
    "href": "notes/bash-essentials/index.html#core-utilities",
    "title": "Bash essentials",
    "section": "5 Core utilities",
    "text": "5 Core utilities\n\n5.1 ssh\nNAME\n  ssh - OpenSSH remote login client\n\nSYNOPSIS\n  ssh [-l login_name] [-p port] DESTINATION [command [argument...]\nssh is a program for logging into a remote machine and for executing commands on a remote machine. It is intended to provide secure encrypted communications between two untrusted hosts over an insecure network.\nssh connects and logs into the specified destination, which may be specified as either [user@]hostname or a URI of the form ssh://[user@]hostname[:port].\nIf a command is specified, it will be executed on the remote host instead of a login shell.\n\n\n\n\n\n\nssh details & examples\n\n\n\n\n\n\nThere’s nothing here yet… stay tuned!\n\n\n\n\n\n5.2 rsync\nNAME\n  rsync - a fast, versatile, remote (and local) file-copying tool\n\nSYNOPSIS\n  Local:\n    rsync [OPTION...] SRC... [DEST]\n  Access via remote shell:\n    Pull:\n        rsync [OPTION...] [USER@]HOST:SRC... [DEST]\n    Push:\n        rsync [OPTION...] SRC... [USER@]HOST:DEST\nRsync is a fast and extraordinarily versatile file copying tool. It can copy locally, to/from another host over any remote shell, or to/from a remote rsync daemon.\nIt is famous for sending only the differences between the source files and the existing files in the destination, increasing efficiency for repetitive synchronization between source and destination.\nRsync is widely used for backups and mirroring, and as an improved cp command for everyday use.\n\n\n\n\n\n\nrsync details & examples\n\n\n\n\n\n\nThere’s nothing here yet… stay tuned!\n\n\n\n\n\n5.3 tar, zip, and unzip\nNAME\n  tar - a general archiving utility for creation/extraction/compression and more\n  zip - package and compress files into a ZIP archive\n  unzip - list, test and extract compressed files from a ZIP archive\n\nSYNOPSIS\n  tar --create/--extract [--file ARCHIVE] [OPTIONS] [FILE...]\n  zip [OPTIONS] [ARCHIVE] [FILE...]\n  unzip [ARCHIVE] [-d OUTPUTDIR]\nThe tar, zip, and unzip programs provide the ability to create, extract, and otherwise manipulate archives of files, where an archive of files is simply a file that stores a collection of other files.\n\n\n\n\n\n\ntar, zip, and unzip details & examples\n\n\n\n\n\nThis section was adapted from (“GNU Tar 1.35: 2 Tutorial Introduction to Tar” n.d.).\nThe specific usecases for tar/zip/unzip are similar but vary slightly.\nAll three tools are used for efficient storage, transfer, and backup of collections of files, particularly large files via compression.\n\ntar:\n\ndefault: create/extract an uncompressed archive (.tar) of a collection of files\nwith --gzip/-z: create/extract a compressed archive (.tar.gz) of a collection of files\nwith --bzip2/-j: create/extract a compressed archive (.tar.bz2) of a collection of files\n\nzip:\n\ncreate a compressed collection of files (.zip)\n\nunzip:\n\nextract a compressed collection of files (.zip)\n\n\n\n5.3.1 Operations\nThere are two main operations of interest for archiving programs:\n\ncreate: create a new archive (.zip, .tar, .tar.gz, tar.bz2)\nextract: extract the files of an archive to a directory\n\nExamples of each follow below:\n\nCreateExtract\n\n\n# Assume you have a directory called music/ and three folders inside it:\n$ tree music\nmusic/\n├── blues\n│   └── nina-simone\n├── folk\n│   └── phil-ochs\n└── jazz\n    └── charles-mingus\n\n# Create an uncompressed archive (.tar) of all three files\n$ tar --create --file=collection.tar music\n\n# Creates a compressed archive (.zip, .tar.gz, .tar.bz2)\n$ zip -r collection.zip music\n$ tar --create --gzip --file=collection.tar.gz music\n$ tar --create --bzip2 --file=collection.tar.bz2 music\n\n# tar has shorthand versions of the above parameters\n$ tar -c -f collection.tar music\n$ tar -c -z -f collection.tar.gz music\n$ tar -cjf collection.tar.bz2 music\n\n\n\n# Assume you have the archives from the Create example:\n$ tar --list collection.tar\nmusic/\n├── blues\n│   └── nina-simone\n├── folk\n│   └── phil-ochs\n└── jazz\n    └── charles-mingus\n\n# Extract all files from an uncompressed archive (.tar) to the current directory\n$ tar --extract --file=collection.tar\n\n# Extract all files from a compressed archive (.zip, .tar.gz, .tar.bz2) to the current directory\n$ unzip collection.zip\n$ tar --extract --gzip --file=collection.tar.gz\n$ tar --extract --bzip2 --file=collection.tar.bz2\n\n# Extract all files from a compressed archive, specifying a different output directory\n$ unzip collection.zip -d ~/some-folder\n$ tar --extract --gzip --file=collection.tar.gz --directory ~/music\n$ tar --extract --bzip2 --file=collection.tar.bz2 --directory /tmp/music\n\n# tar has shorthand versions of the above parameters\n$ tar -x -f collection.tar\n$ tar -x -z -f collection.tar.gz -C ~/music\n$ tar -xjf collection.tar.bz2 -C /tmp/music\n\n\n\nEach of these operations is mutually exclusive, which makes some sense. You cannot create and extract an archive at the same time, that doesn’t make sense!\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen using --create or -c, tar will overwrite current contents of the file named by -f. To add files to an existing archive, you need to use --append or -r.\n\nYou can read more:\n\nthe usecases and history of tar at (gnu.org)\nA helpful comparison between tar and zip (stackoverflow)\nA reallyy thorough breakdown of compression in tar and zip (stackoverflow)\n\n\n\n5.4 git\nNAME\n  git - the stupid content tracker\n\nSYNOPSIS\n  git &lt;command&gt; [&lt;args&gt;]\nGit is a fast, scalable, distributed revision control system with an unusually rich command set that provides both high-level operations and full access to internals.\nSee man 7 gittutorial to get started, then see man 7 giteveryday for a useful minimum set of commands.\n\n\n\n\n\n\ngit details & examples\n\n\n\n\n\n\nThere’s nothing here yet… stay tuned!",
    "crumbs": [
      "Bash essentials"
    ]
  },
  {
    "objectID": "notes/bash-essentials/index.html#more-resources",
    "href": "notes/bash-essentials/index.html#more-resources",
    "title": "Bash essentials",
    "section": "6 More resources",
    "text": "6 More resources\n\nLinuxCommand.org\n\nShort guides on learning bash shell and bash scripting.\nLinks to interactive learning games under “Adventures”. Basic Shell Features\nComplete reference with examples.",
    "crumbs": [
      "Bash essentials"
    ]
  },
  {
    "objectID": "notes/azure-eventhub/index.html",
    "href": "notes/azure-eventhub/index.html",
    "title": "Azure EventHubs",
    "section": "",
    "text": "The notes below were taken from the official Microsoft documentation page Features and terminology in Azure Event Hubs.",
    "crumbs": [
      "Azure EventHubs"
    ]
  },
  {
    "objectID": "notes/azure-eventhub/index.html#azure-event-hub",
    "href": "notes/azure-eventhub/index.html#azure-event-hub",
    "title": "Azure EventHubs",
    "section": "1 Azure Event Hub",
    "text": "1 Azure Event Hub\nAzure Event Hub is a big data streaming platform and event ingestion service. It can receive and process millions of events per second.\nThe Event Hub routes live data from a data producer (publisher) to one or many data consumers (subscribers). The focus of Event Hub is to route streaming data\nIn addition to messages being streamed to all consumers, messages can also be stored. Typically messages are stored within an Azure service such as blob storage or data lake (more on this later).\n\nFor overview of what is an EventHub and its terminology, refer to:\n\nFeatures and terminology in Azure Event Hubs\n\n\n\n\n\nData flow through Event Hub being forwarded to a consumer and stored\n\n\n\n Data flow through Event Hub being forwarded to a consumer and stored   Azure Event Hubs, Blaize Stewart.\n\n\n1.1 EventHub Partitions\nEvent Hubs organizes sequences of events sent to an event hub into one or more partitions. As newer events arrive, they’re added to the end of this sequence.\n\nThe Free Tier of the Azure IoT Hub uses 2 partitions by default.\n\n\n\n\nEventHub Partition diagram\n\n\nA partition can be thought of as a “commit log”. Partitions hold event data that contains:\n\nOffset,\nBody of the event,\nSequence number (number in the stream sequence),\nUser-defined custom properties,\nSystem properties,\n\nSuch as service-side timestamp at which the event was accepted.\n\n\n\n\n1.2 Stream offsets\nAn offset is the position of an event within a partition. This offset enables an event consumer (reader) to specify a point in the event stream from which they want to begin reading events.\nYou can specify the offset as a timestamp or as an offset value.\n\n\n\npartition offser\n\n\nConsumers are responsible for storing their own offset values outside of the Event Hubs service.\n\n\n1.3 Message Checkpointing\nIt’s possible to save a “checkpoint” offset within a partition.\nThis checkpoint information needs to be stored somewhere. A typical location is an Azure Blob Storage or a fast cloud-based database.\nFor convenience, the Azure EventHub .Net SDK includes methods for storing and retrieving checkpoints from an Azure Blob Storage (more on this later).",
    "crumbs": [
      "Azure EventHubs"
    ]
  },
  {
    "objectID": "notes/azure-eventhub/index.html#eventhub-and-iot-hub",
    "href": "notes/azure-eventhub/index.html#eventhub-and-iot-hub",
    "title": "Azure EventHubs",
    "section": "2 EventHub and IoT Hub",
    "text": "2 EventHub and IoT Hub\nAn Azure IoT Hub is also an instance of an Azure EventHub.\nDefault, Device to Cloud (D2C) messages to an Azure IoT Hub are exposed to external applications via the build-in endpoints. The IoT Hub’s endpoints behave the same way as a EventHub endpoint.\n\nTherefore, the Azure EventHub SDK is used by an application or service to receive D2C messages sent to the IoT Hub.\n\nFor details on the difference between an IoT Hub and an Event Hub, see Connecting IoT Devices to Azure: IoT Hub and Event Hubs.",
    "crumbs": [
      "Azure EventHubs"
    ]
  },
  {
    "objectID": "notes/azure-eventhub/index.html#eventhub-.net-sdk",
    "href": "notes/azure-eventhub/index.html#eventhub-.net-sdk",
    "title": "Azure EventHubs",
    "section": "3 EventHub .NET SDK",
    "text": "3 EventHub .NET SDK\nAzure provides two distinct packages, each with their respective classes for reading EventHub Messages:\n\nEventProcessorClient included in the Azure.Messaging.EventHubs.Processor package.\n\nSupports reading events from all partitions with the call-back methods ProcessEventAsync and ProcessErrorAsync (recommended for production).\n\nEventHubConsumerClient included in the Azure.Messaging.EventHubs package.\n\nSupports reading events from a single partition with the method ReadEventsFromPartitionAsync (suitable for production).\nSupports reading from all partitions with the method ReadEventsAsync (not recommended for production).\n\n\nBelow are the references for both packages mentioned above:\n\n\n\nNuget Package\nOverview\nReference\nSamples\n\n\n\n\n\nMessaging.EventHubs.Processor\nGetting started: Event Processor\nAPI Reference for Messaging.EventHubs.Processor\nSamples for Messaging.EventHubs.Processor\n\n\n\nMessaging.EventHubs\nGetting Started: Event Hubs Client\nAPI Reference for Messaging.EventHubs.Consumer\nSamples for Messaging.EventHubs\n\n\n\n\nWhen going through the sample code, the following order is recommended:\n\nEventProcessorClient\n\nSample01_HelloWorld.md\nSample04_ProcessingEvents.md\n\nEventHubConsumerClient\n\nSample01_HelloWorld.md\nSample05_ReadingEvents.md\n\nRun the “Quickstart” code Read device-to-cloud messages\n\n\n3.1 Cancellation Tokens\nMany of examples listed above make use of Cancellation Tokens to manage Tasks.\n\nTo learn more about Cancellation Tokens, see: How to use CancellationTokens to cancel tasks in the Azure SDK for .NET",
    "crumbs": [
      "Azure EventHubs"
    ]
  },
  {
    "objectID": "notes/azure-eventhub/index.html#connection-string-event-hub-endpoint",
    "href": "notes/azure-eventhub/index.html#connection-string-event-hub-endpoint",
    "title": "Azure EventHubs",
    "section": "4 Connection String: Event Hub Endpoint",
    "text": "4 Connection String: Event Hub Endpoint\nWhen you use Event Hubs SDKs or product integrations that are unaware of IoT Hub, you need an Event Hub-compatible endpoint and Event Hub-compatible name. You can retrieve these values from the portal as follows:\n\nSign in to the Azure portal and navigate to your IoT hub.\nSelect Built-in endpoints from the resource menu, under Hub settings.\nThe Built-in endpoints working pane contains three sections:\n\nThe Event Hub Details section contains the following values:\n\nEvent Hub-compatible name\nConsumer Groups.\n\nThe Event Hub compatible endpoint section contains the following values:\n\nShared access policy (the default is \"$Default\")\nEvent Hub-compatible endpoint.\n\n\n\nIn the working pane, the Event Hub-compatible endpoint field contains a complete Event Hubs connection string that looks like the following example:\nNote that the default consumer group name is \"$Default\" (might required including double quotes).\n\n\n\nScreen capture showing device-to-cloud settings.",
    "crumbs": [
      "Azure EventHubs"
    ]
  },
  {
    "objectID": "notes/azure-cli-cheatsheet/index.html",
    "href": "notes/azure-cli-cheatsheet/index.html",
    "title": "Azure CLI Cheatsheet",
    "section": "",
    "text": "Here is a list of commands we’ll commonly use with Azure CLI.\n\n\n\nAzure CLI is installed: see Azure CLI Installation Instructions\nAzure IoT extension enabled: az extension add --name azure-iot\n\nYou can check that both requirements are met with the command az --version.",
    "crumbs": [
      "Azure CLI Cheatsheet"
    ]
  },
  {
    "objectID": "notes/azure-cli-cheatsheet/index.html#azure-cli-cheatsheet",
    "href": "notes/azure-cli-cheatsheet/index.html#azure-cli-cheatsheet",
    "title": "Azure CLI Cheatsheet",
    "section": "",
    "text": "Here is a list of commands we’ll commonly use with Azure CLI.\n\n\n\nAzure CLI is installed: see Azure CLI Installation Instructions\nAzure IoT extension enabled: az extension add --name azure-iot\n\nYou can check that both requirements are met with the command az --version.",
    "crumbs": [
      "Azure CLI Cheatsheet"
    ]
  },
  {
    "objectID": "notes/azure-cli-cheatsheet/index.html#create-a-device",
    "href": "notes/azure-cli-cheatsheet/index.html#create-a-device",
    "title": "Azure CLI Cheatsheet",
    "section": "2 Create a Device",
    "text": "2 Create a Device\nTo register a device on IoTHub in Azure CLI, use the az iot hub device-identity create command:\n\n\n\n\n\n\n\nNote\n\n\n\nRecall that Bash environment variables allow you to store strings in a shell session. You can save environment variables in a project .env file that you can source later.\n\n# It's convenient to store these names in environment variables for later re-use\n$ export IOT_DEVICE_NAME=simDevice\n$ export IOTHUB_NAME=YourIoTHubName\n\n# Create a device with the name \"simDevice\"\n$ az iot hub device-identity create -d ${IOT_DEVICE_NAME} -n ${IOTHUB_NAME}",
    "crumbs": [
      "Azure CLI Cheatsheet"
    ]
  },
  {
    "objectID": "notes/azure-cli-cheatsheet/index.html#simulate-a-device",
    "href": "notes/azure-cli-cheatsheet/index.html#simulate-a-device",
    "title": "Azure CLI Cheatsheet",
    "section": "3 Simulate a Device",
    "text": "3 Simulate a Device\nYou can “simulate” an IoT Device (that is, create a temporary stream that sends IoT messages similar to real IoT Devices) using the commands below:\n# Begin device simulation for an existing device\n$ az iot device simulate -d ${IOT_DEVICE_NAME} -n ${IOTHUB_NAME}\n\n\n\n\n\n\n\nNote\n\n\n\nThe az iot device simulate command runs for a few minutes unless interrupted by the user. Use Bash Process Control keyboard shortcuts to suspend, resume, scroll through the output, etc.!\n\nBy default, this command will:\n\nSend D2C messages with a payload of Ping from Az CLI IoT Extension\nAutomatically receive and acknowledge C2D messages.\n\nSee az iot device simulate documentation for details.",
    "crumbs": [
      "Azure CLI Cheatsheet"
    ]
  },
  {
    "objectID": "notes/azure-cli-cheatsheet/index.html#monitor-messages",
    "href": "notes/azure-cli-cheatsheet/index.html#monitor-messages",
    "title": "Azure CLI Cheatsheet",
    "section": "4 Monitor Messages",
    "text": "4 Monitor Messages\nYou can monitor all actions in an Azure IOT Hub using the az iot hub monitor-events command:\n\n\n\n\n\n\n\nNote\n\n\n\nBash Process Control keyboard shortcuts and Bash redirection and pipes are useful for pausing/continuing/scrolling/parsing the az iot hub monitor-events command.\n\n# List all message details for all devices.\n$ az iot hub monitor-events --output table -p all -n ${IOTHUB_NAME}\n\n# List message details for a specific device:\n$ az iot hub monitor-events --output table --device-id ${IOT_DEVICE_NAME} --hub-name ${IOTHUB_NAME}\n\n\n\n\n\n\nWarning\n\n\n\nIf you are having uamqp errors when using this command, try installing it in pip first.\n(.venv) $ pip install uamqp\nIf the above is giving you issues, see the course notes on installing azure cli for more troubleshooting info.\n\n\nSee az iot hub monitor-events documentation for details.",
    "crumbs": [
      "Azure CLI Cheatsheet"
    ]
  },
  {
    "objectID": "notes/azure-cli-cheatsheet/index.html#device-commands",
    "href": "notes/azure-cli-cheatsheet/index.html#device-commands",
    "title": "Azure CLI Cheatsheet",
    "section": "5 Device Commands",
    "text": "5 Device Commands\n\n5.1 Send C2D Message to Simulated Device\nThe az iot device c2d-message send command sends a cloud-to-device message from your IoT hub to an IoT device. The message can include a data string, representing a payload for the command, and a set of key-value pairs, representing command properties.\naz iot device c2d-message send -d ${IOT_DEVICE_NAME} --data \"Hello World\" --props \"key0=value0;key1=value1\" -n ${IOTHUB_NAME}\nIf simDevice is the default simulated device as seen in the Simulate a Device section, then you should see the following log printed when a c2d-message is sent to it:\nC2D Message Handler [Received C2D message]:\n{ 'Message Properties': { 'content_encoding': 'utf-8',\n                          'key0': 'value0',\n                          'key1': 'value1',\n                          'message_id': '1bbb2dd3-7b24-4e62-ab3a-79a8b13cb1fe'},\n  'Payload': 'Hello World',\n  'Topic': '/devices/simDevice/messages/devicebound'}\nNote that the keys, values, and payload are set by the --props and --data arguments of the c2d-message command.\nSee az iot device c2d-message send documentation for more details.\n\n\n5.2 Invoke Direct Method on Device\nThe az iot hub invoke-device-method command calls a method (specified by name) on a chosen device, and returns a payload.\naz iot hub invoke-device-method --mn ${METHOD_NAME} -d ${IOT_DEVICE_NAME} -n ${IOTHUB_NAME}\nIf simDevice is the default simulated device as seen in the Simulate a Device section, then you should see the following log printed when the SetTelemetryInterval direct method is invoked:\n\nDevice consoleService console\n\n\nMethod Request Handler [Received direct method invocation request]:\n{ 'Device Id': 'simDevice',\n  'Method Request Id': '1',\n  'Method Request Name': 'SetTelemetryInterval',\n  'Method Request Payload': {}}\n\n\n{\n  \"payload\": {\n    \"methodName\": \"SetTelemetryInterval\",\n    \"methodRequestId\": \"1\",\n    \"methodRequestPayload\": {}\n  },\n  \"status\": 200\n}\n\n\n\n\n\n5.3 Update Device Twin Properties\naz iot hub device-twin update -d ${IOT_DEVICE_NAME} --desired '{\"conditions\":{\"temperature\":{\"warning\":98, \"critical\":107}}}' -n ${IOTHUB_NAME}\n\n\n5.4 Get Device Twin Properties\naz iot hub device-twin show -d ${IOT_DEVICE_NAME} --query properties.reported -n ${IOTHUB_NAME}",
    "crumbs": [
      "Azure CLI Cheatsheet"
    ]
  },
  {
    "objectID": "notes/azure-cli-cheatsheet/index.html#connection-strings",
    "href": "notes/azure-cli-cheatsheet/index.html#connection-strings",
    "title": "Azure CLI Cheatsheet",
    "section": "6 Connection Strings",
    "text": "6 Connection Strings\nConnection Strings provide authentication for most Azure SDK commands, both in Python and C#.\nThe sections below give example commands for Azure CLI connection strings retrieval.\n\n\n\n\n\n\n\nNote\n\n\n\nBash redirection and pipes are useful tools for saving the output of these connection string commands as environment variables. You can save environment variables in a project .env file that you can source later.\n\n\n6.1 Device connection string\nRecommended string for a single device\naz iot hub device-identity connection-string show \\\n  --device-id ${IOT_DEVICE_NAME} \\\n  --hub-name ${IOTHUB_NAME}\n\n\n6.2 Service connection string\nRecommended string for external applications.\naz iot hub connection-string show \\\n  --policy-name service \\\n  --hub-name ${IOTHUB_NAME}\n\n\n6.3 IoT Hub EventHub-compatible connection string\nNeeded for certain applications.\naz iot hub connection-string show \\\n  -n ${IOTHUB_NAME} \\\n  --default-eventhub\n\n\n6.4 EventHub Connection string\nPrerequisites:\n\nyou have an eventhub namespace\nyou have an eventhub\nyou have a shared access policy set for the eventhub\n\naz eventhubs eventhub authorization-rule keys list \\\n  --resource-group ${AZURE_RESOURCE_GROUP} \\\n  --namespace-name ${AZURE_EVENTHUB_NAMESPACE} \\\n  --eventhub-name ${AZURE_EVENTHUB_NAME} \\\n  --name ${AZURE_SAS_POLICY}\nSee https://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-get-connection-string#azure-cli for more details.\n\n\n6.5 Owner connection string\nNOT recommended, provides access to entire resource group.\naz iot hub connection-string show --hub-name ${IOTHUB_NAME}\nSee detailed documentation at learn.microsoft.com:\n\naz iot hub connection-string\naz iot hub device-identity connection-string show\nEventHub connection string",
    "crumbs": [
      "Azure CLI Cheatsheet"
    ]
  },
  {
    "objectID": "notes/python-oop/index.html",
    "href": "notes/python-oop/index.html",
    "title": "Python OOP",
    "section": "",
    "text": "Explanation of how OOP works in Python\nType annotations for Python variables, functions, and classes\nInheritance, interfaces\nEnums",
    "crumbs": [
      "Python OOP"
    ]
  },
  {
    "objectID": "notes/python-oop/index.html#overview",
    "href": "notes/python-oop/index.html#overview",
    "title": "Python OOP",
    "section": "",
    "text": "Explanation of how OOP works in Python\nType annotations for Python variables, functions, and classes\nInheritance, interfaces\nEnums",
    "crumbs": [
      "Python OOP"
    ]
  },
  {
    "objectID": "notes/python-oop/index.html#oop-in-python",
    "href": "notes/python-oop/index.html#oop-in-python",
    "title": "Python OOP",
    "section": "2 OOP in Python",
    "text": "2 OOP in Python\nThese notes have been adapted from https://realpython.com/python3-object-oriented-programming/ with few modifications.\nObject-oriented programming is a programming paradigm that provides a means of structuring programs so that properties and behaviors are bundled into individual objects.\n\n2.1 How to define a class in python\nIn python, you define a class by using the class keyword followed by a name and a colon. Then you declare a constructor function that is always called __init__() to declare which attributes each instance of the class should have:\n\n\npython\n\nclass Person:\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n\nOnce the class is defined, you can create instances of it:\n\n\nPyInterpreter\n\n&gt;&gt;&gt; bob = Person(\"Bob\", 102)\n&gt;&gt;&gt; print(bob.name)\nBob\n&gt;&gt;&gt; print(bob.age)\n102\n\n\n\n2.2 Classes vs Instances\nA class is a blueprint. It doesn’t actually contain any data. The Person class specifies that a name and an age are necessary for defining a person, but it doesn’t contain the name or age of any specific person.\nWhile the class is the blueprint, an instance is an object that’s built from a class and contains real data. An instance of the Person class is not a blueprint anymore. It’s an actual person with a name, like Bob, who’s 102 years old.\n\n\n2.3 Instance Methods\nInstance methods are functions that you define inside a class and can only call on an instance of that class. Just like __init__(), an instance method always takes self as its first parameter.\nLet’s add a few methods to the Person class from the previous example:\n\n\nPython\n\nclass Person:\n\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n\n    # Instance method\n    def description(self):\n        return f\"{self.name} is {self.age} years old\"\n\n    # Another instance method\n    def speak(self, sound):\n        return f\"{self.name} says {sound}\"\n\nThis Person class has two instance methods:\n\ndescription returns a string displaying the name and age of the person.\nspeak has one parameter called sound and returns a string containing the person’s name and the sound that the person makes.\n\n\n\nPyInterpreter\n\n&gt;&gt;&gt; miles = Person(\"Miles\", 4)\n\n&gt;&gt;&gt; miles.description()\n'Miles is 4 years old'\n\n&gt;&gt;&gt; miles.speak(\"Woof Woof\")\n'Miles says Woof Woof'\n\n&gt;&gt;&gt; miles.speak(\"Bow Wow\")\n'Miles says Bow Wow'\n\n\n\n2.4 Dunder methods\nIn the editor window, change the name of the Dog class’s .description() method to .__str__():\n\n\nPython\n\nclass Dog:\n    # ...\n\n    def __str__(self):\n        return f\"{self.name} is {self.age} years old\"\n\nMethods like .__init__() and .__str__() are called dunder methods because they begin and end with double underscores. There are many dunder methods that you can use to customize classes in Python. Understanding dunder methods is an important part of mastering object-oriented programming in Python.\n\nNote: Check out When Should You Use .__repr__() vs .__str__() in Python? to learn more about .__str__() and its cousin .__repr__().\n\n\n\n2.5 How Do You Inherit From Another Class in Python?\nInheritance is the process by which one class takes on the attributes and methods of another. Newly formed classes are called child classes, and the classes that you derive child classes from are called parent classes.\nYou inherit from a parent class by creating a new class and putting the name of the parent class into parentheses:\n\n\nPython\n\nclass Parent:\n    hair_color = \"brown\"\n\n\nclass Child(Parent):\n    pass\n\nIn this minimal example, the child class Child inherits from the parent class Parent. Because child classes take on the attributes and methods of parent classes, Child.hair_color is also \"brown\" without your explicitly defining that.\nChild classes can override or extend the attributes and methods of parent classes. In other words, child classes inherit all of the parent’s attributes and methods but can also specify attributes and methods that are unique to themselves:\n\n\nPython\n\nclass Parent:\n    speaks = [\"English\"]\n\n\nclass Child(Parent):\n    def __init__(self):\n        super().__init__()\n        self.speaks.append(\"German\")",
    "crumbs": [
      "Python OOP"
    ]
  },
  {
    "objectID": "notes/python-oop/index.html#typings",
    "href": "notes/python-oop/index.html#typings",
    "title": "Python OOP",
    "section": "3 Typings",
    "text": "3 Typings\nThis section adapted from https://docs.python.org/3/library/typing.html and https://mypy.readthedocs.io/en/stable/cheat_sheet_py3.html\nThis section is a quick cheat sheet showing how to use type annotations for various common types in Python.\n\n\n\n\n\n\nNote\n\n\n\nThe Python runtime does not enforce function and variable type annotations. They can be used by third party tools such as type checkers, IDEs, linters, etc.\n\n\n\n3.1 Variables\nBasics:\n# This is how you declare the type of a variable\nage: int = 1\n\n# You don't need to initialize a variable to annotate it\na: int  # Ok (no value at runtime until assigned)\n\n# Doing so can be useful in conditional branches\nchild: bool\nif age &lt; 18:\n    child = True\nelse:\n    child = False\nUseful built-in types:\n# the common basic \"primitive\" types in Python\nx: int = 1\nx: float = 1.0\nx: bool = True\nx: str = \"test\"\nx: bytes = b\"test\"\n\n# Collections (Python 3.9+)\nx: list[int] = [1]\nx: set[int] = {6, 7}\n\n# For mappings, we need the types of both keys and values\nx: dict[str, float] = {\"field\": 2.0}  # Python 3.9+\n\n# For tuples of fixed size, we specify the types of all the elements\nx: tuple[int, str, float] = (3, \"yes\", 7.5)  # Python 3.9+\n\n# For tuples of variable size, we use one type and ellipsis\nx: tuple[int, ...] = (1, 2, 3)  # Python 3.9+\n\n\n3.2 Functions\nfrom typing import Callable, Iterator, Union, Optional\n\n\n# This is how you annotate a function definition\ndef stringify(num: int) -&gt; str:\n    return str(num)\n\n\n# And here's how you specify multiple arguments\ndef plus(num1: int, num2: int) -&gt; int:\n    return num1 + num2\n\n\n# If a function does not return a value, use None as the return type\n# Default value for an argument goes after the type annotation\ndef show(value: str, excitement: int = 10) -&gt; None:\n    print(value + \"!\" * excitement)\n\n\n# Note that arguments without a type are dynamically typed (treated as Any)\n# and that functions without any annotations are not checked\ndef untyped(x):\n    x.anything() + 1 + \"string\"  # no errors\n\n\n# You can of course split a function annotation over multiple lines\ndef send_email(\n    address: Union[str, list[str]],\n    sender: str,\n    cc: Optional[list[str]],\n    bcc: Optional[list[str]],\n    subject: str = \"\",\n    body: Optional[list[str]] = None,\n) -&gt; bool: ...\n\n\n3.3 Classes\nHere’s an example of typings with a custom defined class:\nclass BankAccount:\n    # The \"__init__\" constructor method doesn't return anything, so it gets return\n    # type \"None\" just like any other method that doesn't return anything\n    def __init__(self, account_name: str, initial_balance: int = 0) -&gt; None:\n        # mypy will infer the correct types for these instance variables\n        # based on the types of the parameters.\n        self.account_name = account_name\n        self.balance = initial_balance\n\n    # For instance methods, omit type for \"self\"\n    def deposit(self, amount: int) -&gt; None:\n        self.balance += amount\n\n    def withdraw(self, amount: int) -&gt; None:\n        self.balance -= amount\n\n\n# User-defined classes are valid as types in annotations\naccount: BankAccount = BankAccount(\"Alice\", 400)\n\n\ndef transfer(src: BankAccount, dst: BankAccount, amount: int) -&gt; None:\n    src.withdraw(amount)\n    dst.deposit(amount)\n\n\n# Functions that accept BankAccount also accept any subclass of BankAccount!\nclass AuditedBankAccount(BankAccount):\n    # You can optionally declare instance variables in the class body\n    audit_log: list[str]\n\n    def __init__(self, account_name: str, initial_balance: int = 0) -&gt; None:\n        super().__init__(account_name, initial_balance)\n        self.audit_log: list[str] = []\n\n    def deposit(self, amount: int) -&gt; None:\n        self.audit_log.append(f\"Deposited {amount}\")\n        self.balance += amount\n\n    def withdraw(self, amount: int) -&gt; None:\n        self.audit_log.append(f\"Withdrew {amount}\")\n        self.balance -= amount\n\n\naudited = AuditedBankAccount(\"Bob\", 300)\ntransfer(audited, account, 100)  # type checks!",
    "crumbs": [
      "Python OOP"
    ]
  },
  {
    "objectID": "notes/github-basics/index.html#overview",
    "href": "notes/github-basics/index.html#overview",
    "title": "GitHub basics",
    "section": "1 Overview",
    "text": "1 Overview\nThese notes cover the basics for using GitHub in this class.",
    "crumbs": [
      "GitHub basics"
    ]
  },
  {
    "objectID": "notes/github-basics/index.html#branch-management",
    "href": "notes/github-basics/index.html#branch-management",
    "title": "GitHub basics",
    "section": "2 Branch management",
    "text": "2 Branch management\nAlmost all of our lab and assignment work will take place on branches.\n\n2.1 Creating a branch using the GitHub website\nYou can add branches to your repository directly by clicking the “Branches” icon, and then “New Branch” on the subsequent window (screenshots below).\nYou’ll need the following information:\n\nNew branch name: the lab name (e.g. lab-0).\nSource: the instructions branch from your own repository\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: The user interface for creating branches on GitHub.\n\n\n\n\n\n2.2 Creating a branch using VSCode\nYou can create branches within your project using VSCode:\n\n\n\n\n\n\nFigure 2: Clicking on the branch button (bottom left) launches a dialog which allows for a few branch operations: you can create a new branch, switch to a local branch, switch/pull a remote branch, etc.\n\n\n\n\n\n2.3 Creating a branch using the command line\n# make sure you are on the `instructions` branch before proceeding\ngit status\n# the switch command switches branches, -c flag stands for \"create\"\ngit switch -c lab-0\n# upload your branch to the remote repository\ngit push -u origin\n# ensure your new lab-0 branch is up to date with new remote lab-0 branch\ngit status",
    "crumbs": [
      "GitHub basics"
    ]
  },
  {
    "objectID": "notes/github-basics/index.html#authentication",
    "href": "notes/github-basics/index.html#authentication",
    "title": "GitHub basics",
    "section": "3 Authentication",
    "text": "3 Authentication\nMany git operations require authentication to get permission. Some examples:\n\nPushing to a repository\nPulling from a private repository\nUsing GitHub CLI\n\nSince July 2021, GitHub no longer accepts account passwords to authenticate git operations. You have probably run into this error many times when trying to push changes or clone your private repositories on a new machine.\nThe only reason VSCode works out of the box is because VSCode and GitHub are integrated by default, both being owned by Microsoft.\nThe following sections gives us more flexible and useful ways to authenticate git commands with GitHub.\n\n3.1 Creating a personal access token\nRead “Managing your personal access tokens” on Github, and create a classic (not fine-grained) personal access token.\nAt the very least, select the repo scope – this will give your token the ability to authenticate using git on the CLI. You can select all other scopes as well if you like.\nOnce you’re finished, you’ll see your token is a string of the following form:\nghp_&lt;long string of letters and numbers&gt;\nKeep this window open – the string of characters will disappear as soon as you refresh the page.\nWe need to configure a secure storage location for this string. For this, we will use the tool pass.\n\n\n3.2 Using a password manager to store your token\nSecrets like personal access tokens need to be readily accessible to be useful – but they also should be secret, so that others cannot easily impersonate you using the token.\nA common method for managing secrets is to use a password manager. In our course we use pass to securely manage our personal access tokens on our developer environment.\n\n3.2.1 Install pass dependencies\nFirst, ensure pass and some useful related dependencies are installed:\n# On WSL / Linux\nsudo apt install pass pass-extension-otp zbar-tools\n\n# On macOS\nbrew install pass pass-otp zbar\n\n\n3.2.2 Set up gpg\npass works by using asymmetric key encryption to store secrets. That means: you posess the private key that can decrypt secrets, and you make the public key available which can encrypt secrets.\nThis scheme is not only useful for private communication (something similar is used by apps like Signal and Telegram), but also for storing any secrets – for us, we will store our github token as a secret.\nTo get started, you’ll need to generated a gpg key-pair in order to use pass.\n\n\n\n\n\n\nNote\n\n\n\nThe GitHub instructions mention using git bash – ignore them, you have a developer environment to use instead.\nIn general, when I link to external instructions, you will need to pay attention to what parts of them may be different in our class. This is a good skill in general for making effective use of resources posted online when learning a new skill.\n\n\nFollow the instructions below:\n\nCreate the gpg key-pair following the instructions on GitHub: Generating a new GPG key\nrun gpg --full-generate-key to get started.\nRecommended: You can accept the default key type (RSA)\nRecommended: Choose 4096 bits for the keysize.\nRecommended: You can accept the default “does not expire” option.\nEnter user ID information. This information should match what you have provided to GitHub already (username/email address)\nYou have to choose a password for GPG keys. Choose something strong that you can remember.\nAdd the public key to your GitHub account following the instructions: Adding a GPG Key to your GitHub account.\n\nThe name of the key on GitHub does not matter (Personal GPG Key is fine)\nThe command: gpg --armor --export prints your key to the console, you can copy/paste this output for GitHub\nEven better: use a pipe to clip.exe to put the key in your clipboard automatically with gpg --armor --export | clip.exe\n\non macOS: use pbcopy instead of clip.exe\non Linux: use xclip or wl-copy instead of clip.exe\n\n\n\n3.2.3 Store personal access token in pass\nOnce you’ve created the gpg key-pair, we can now set up pass:\npass init &lt;the-email-you-used-for-gpg-key&gt;\nFinally, copy the token string from GitHub to your clipboard. Then, open your developer terminal:\n$ pass insert github/token\nEnter password for github/token: # paste your token here, then press enter\nOnce you’ve done this, you should be able to access your token using pass github/token, or pass github/token | clip.exe to place it on your clipboard directly.\n\n\n\n3.3 Troubleshooting\nSome common errors that arise with using gpg:\n\n3.3.1 No secret key\nThis error looks like:\n\n\nbash\n\n$ pass github/token\ngpg: decryption failed: No secret key\n\nTry the following:\n\n3.3.1.1 Double check the password store setup\nConfirm that your password store is encrypted with the gpg key you expect:\n\n\nbash\n\n# This command prints out your gpg key information\n$ gpg -k \n\n# This command shows the gpg key used to encrypt your password-store\n$ cat ~/.password-store/.gpg-id\n\n# The id and/or email address should match for both!\n\n\n\n3.3.1.2 Restart gpg daemon\n\n\nbash\n\n$ gpgconf --kill gpg-agent\n\nIf that doesn’t work, try restarting your WSL instance. In powershell:\n\n\nPowershell\n\nPS &gt; wsl --shutdown &lt;distro-name&gt;\n\nAfter the shutdown attempt, retry using pass in the WSL again.",
    "crumbs": [
      "GitHub basics"
    ]
  },
  {
    "objectID": "notes/github-basics/index.html#repository-management",
    "href": "notes/github-basics/index.html#repository-management",
    "title": "GitHub basics",
    "section": "4 Repository management",
    "text": "4 Repository management\n\n4.1 Troubleshooting\n\n4.1.1 Method 1\n# Adapted from: https://stackoverflow.com/a/40098509\n\n# Delete the corrupted .git/objects\nfind .git/objects/ -size 0 -exec rm -rf {} \\;\n\n# Might need to do this?\n# git symbolic-ref HEAD refs/heads/master\n\n# Update the git repository\ngit fetch\n\n\n4.1.2 Method 2\n# Adapted from: https://stackoverflow.com/a/18238322\n\n# Delete the corrupted .git folder\n$ rm -fr .git\n\n# Create a new .git folder\n$ git init\n\n# Add your GitHub repository as the \"origin\" remote\n$ git remote add origin [your-git-remote-url]\n\n# Update the .git folder to have the latest changes from remote\n$ git fetch\n\n# Keep your current changes, but set your git reference to your working branch\n$ git reset --mixed origin/&lt;branch-name&gt;\n\n# Ensure you're on the correct branch\n$ git switch &lt;branch-name&gt;",
    "crumbs": [
      "GitHub basics"
    ]
  },
  {
    "objectID": "notes/reterminal-setup/index.html",
    "href": "notes/reterminal-setup/index.html",
    "title": "Reterminal setup",
    "section": "",
    "text": "The reTerminal device. Image source",
    "crumbs": [
      "Reterminal setup"
    ]
  },
  {
    "objectID": "notes/reterminal-setup/index.html#overview",
    "href": "notes/reterminal-setup/index.html#overview",
    "title": "Reterminal setup",
    "section": "1 Overview",
    "text": "1 Overview\nThis page documents the general steps needed to perform an initial set up, or reset, of the reTerminal device that we will be using throughout the class.\nThe general steps are:\n\nInstall necessary dependencies for reimaging a Pi on a host computer\nReimage and configure the reTerminal’s operating system.\nConnect to the reTerminal remotely and ensure the remote connection services are working:\n\nGraphical desktop session using a VNC client.\nCLI session using ssh\n\n\nThese instructions are mostly adapted from the instructions available at (“Getting Started with reTerminal | Seeed Studio Wiki” 2023).",
    "crumbs": [
      "Reterminal setup"
    ]
  },
  {
    "objectID": "notes/reterminal-setup/index.html#prerequisites",
    "href": "notes/reterminal-setup/index.html#prerequisites",
    "title": "Reterminal setup",
    "section": "2 Prerequisites",
    "text": "2 Prerequisites\nYou can perform this lab on any computer that you are able to run applications with “elevated permissions”.\n\non the Lab computers, you will need to use the “run with elevated permissions” mode at a few steps. This will be explained in class.\non a personal Windows machine, you can follow all of these instructions normally, using “admin” mode when prompted for elevated permissions.\nif you are using a personal macOS or Linux machine, I have not written the instructions with you in mind – you can do this lab, but make sure to adjust the instructions to your OS accordingly\n\n\n2.1 Hardware required\nYou need to prepare the following hardware before getting started with reTerminal:\n\nEthernet cable or Wi-Fi connection\nUSB Type-C cable\nreTerminal kit case, containing\n\nsmall screw driver (black handle)\nreTerminal power adapter\n\n\n\n\n2.2 Software required\nThere are two tools to install for this lab:\n\nRaspberry Pi (RPi) USB Device Boot daemon\nRaspberry Pi Imager\n\nInstructions for installing each follow below.\n\n\n\n\n\n\nNote 1: Elevated Permissions\n\n\n\nWe will sometimes need elevated permissions in order to install or operate software.\n\nOn the Lab Computers, you can achieve elevated permissions by right-clicking on the executable and selecting the option: “Run with elevated access”.\n\nYou will be asked for your college username and password.\n\nClick on the password field to get focus (the program doesn’t focus on the password by default, which is very annoying)\n\nYou will then be asked for a reason for elevating permissions. Copy-paste: “6P3-W25 Raspberry Pi Setup”\n\nOn your personal computers, the options will depend on your OS:\n\nWindows: The same as the Lab Computers, but use “Run as administrator” instead.\nmacOS/Linux: use your terminal environment to run the executable and sudo to elevate your permissions\n\n\n\n\n\n2.2.1 RPi USB Device Boot installation\nThe instructions for installing and using this software vary greatly depending on your host operating system.\nYou can find all instructions in the README of the repository for the software: https://github.com/raspberrypi/usbboot/.\nI’ve adapted those instructions for each possible operating system in the section below.\n\nWindows (recommended)macOS/LinuxWSL\n\n\n\nDownload the repository source code using git clone.\n\nUse your developer environment, i.e. your WSL instance, to do this (not git bash)\n\n(important) Move the cloned usbboot folder to your home directory in the C:\\ drive\n\nPro tip: do this in the terminal using the mv command, e.g. mv /path/to/usbboot /mnt/c/Users/Michael.Haaf/Downloads\n\nin Windows Explorer, locate rpiboot_setup.exe within the usbboot/win32/ directory.\nRun the executable with elevated permissions (see Note 1)\n\nIf the software has already been installed, press Yes to overwrite the existing installation\nBy doing so, we will ensure that the software is the latest version (which will be important).\n\nRaspberry pi drivers will begin to be installed on your computer.\n\nThis process takes a few minutes. Keep the window open and move on to the next steps in the lab.\n\nWhen this process is finished, you should now have the folder C:\\Program Files (x86)\\Raspberry Pi\\ on your computer.\n\n\n\n\n(On macOS / Linux): read the README of the repository and follow those instructions instead.\n\n\n\nNot recommended at this time.\n\n\n\n\n\n2.2.2 Raspberry Pi Imager installation\n\nFollow software from the official Raspberry Pi website.. This software is straightforward to install.\nNOTE: this should already be installed on the Lab computers. Check to see if Raspberry Pi Imager is an application you can open before installing.",
    "crumbs": [
      "Reterminal setup"
    ]
  },
  {
    "objectID": "notes/reterminal-setup/index.html#reimage-the-reterminal",
    "href": "notes/reterminal-setup/index.html#reimage-the-reterminal",
    "title": "Reterminal setup",
    "section": "3 Reimage the reTerminal",
    "text": "3 Reimage the reTerminal\nBelow is a brief overview of the three step process for reimaging the reTerminal:\n\nDisassemble the back cover and flip a switch to have direct access to the eMMc storage\n\nDo not disassemble the entire device! It is only necessary to remove the heatsink.\n\nReimage and configure the OS using the Raspberry Pi Imager software.\nReturn the memory selector switch to the original position and reassemble.\n\n\n3.1 reTerminal Disassembly\n\n\n\n\n\n\nFigure 1: Only remove the heatsink in order to access the memory switch. It is not necessary fully disassemble the reTerminal like they do in the video.\n\n\n\n\nWatch the video in Figure 1 to understand the disassembly process (2 mins).\nFollow Steps 1, 2, & 3 in the reTerminal documentation to remove the heatsink. Flash Raspberry Pi OS/ 64-bit Ubuntu OS or Other OS to eMMC. Use the following hardware from your reTerminal kit:\n\nsmall screw driver (black handle)\nkit case (store the plastic nubs and removed screws in your case. Don’t lose the screws!)\n\n\nAfter the following the above steps, you will have:\n\nremoved the heatsink\ntoggled the eMMc memory switch (see Figure 2).\n\nYour reTerminal is now ready for a firmware flash.\n\n\n\n\n\n\n\nFigure 2: Memory select switch behind the reTerminal’s heatsink in the “down” position.\n\n\n\n\n3.2 New OS image & Configuration\nTo re-image the reTerminal, follow the steps below.\n\n3.2.1 Launch rpiboot\n\nDouble check you have finished the installation\nLaunch the rpiboot executable file with elevated permissions (see Note 1)\n\nOn Windows, this should be C:\\Program Files (x86)\\Raspberry Pi\\rpiboot.exe\nOn a personal macOS/Linux: I think it’s rpiboot.sh in the installation directory, but check the project README to be sure\n\nKeep the rpiboot window open throughout the next steps of this lab.\n\nAfter launching, you should see a terminal window with something like the following dialog appear:\nRPIBOOT: build-date Jan 22 2023 version 20221215-105525 864863bc\nWaiting for BCM2835/6/7/2711...\nThe rpiboot program creates a daemon (a dedicated background process) that will detect when a reTerminal device is connected in flash mode (i.e. the eMMc switch toggled “down” as in Figure 2).\n\n\n3.2.2 Connect the reTerminal to the USB port of your machine.\n\nFind a USB-C to USB 3.0 Cable and plug it into a 3.0 port on your computer.\n\nUSB-C to USB-C is also fine.\n\nPlug the other end of the cable into your reTerminal\n\nNOTE: if the screen of the reTerminal turns on when you plug in the USB cable, you have missed a step in the disassembly process. In this case, stop, unplug the device, and read the previous instructions more carefully.\n\nrpiboot will detect and attach the reTerminal’s internal memory as a storage device.\nAt this stage, you should see some dialog appear in the RPIBOOT program:\n\nSending bootcode4.bin…\nReceived 4 bytes\nsomethingsomethingsomething\nEtc. etc.\nLoading startup.elf file\nWarning: file not interpreted as such-and-such\nFinished\nYour output will not exactly match the example I have provided. Here are some guidelines:\n\nThe program may close on its own, or may not. Either way is fine.\nYou can ignore the “warnings” that appear at the end of the logs.\nYou can ignore the “There is a problem with this drive” Windows notification.\nThe program should NOT repeatedly loop at Sending bootcode4.bin...\n\nif this is the case, and no other messages appear, try the previous steps again, make sure you pay attention to details.\n\n\nBasically, unless the program is stuck in a loop, you should continue to the next step.\n\n\n3.2.3 Run Rasperry Pi (RPi) Imager\n\nRun the program on your desktop with elevated permissions (see Note 1)\nBefore making any selections, press Control+Shift+X to open the “OS Customizations” Advanced Options menu.\nMake the following customizations (you will need to click through all 3 tabs at the top).\n\nSet a unique hostname (suggestion: your github username)\nEnable SSH with password authentication.\nSet a unique username and password.\n\nDo not use the defaults or forget these. You will need to reimage your reTerminal if you do.\n\nConfigure the wireless LAN for the lab network:\n\nSSID: P326-hotspot\n\nNOTE: there is no whitespace. Take care your SSID matches exactly.\n\nPassword: 6P3-W25-pallet-overcast\n\nNOTE: take care your password matches exactly\n\nWireless LAN country: CA\n\nSet locale settings: America/Montreal\nDisable telemetry.\nEnable “eject media” and “play sound when finished”.\nTake note of your hostname, username, and password (see Moodle for place to enter this information)\n\nYou will be responsible for maintaining your system.\nIf you get locked out, you may have to re-image the system.\n\nPress “SAVE” when finished.\n\nOnce you’ve finished making the above customizations, there are three main configuration choices to make:\n\nRaspberry Pi Device: Raspberry Pi 4\nOperating System: Raspberry Pi OS 64-bit (Recommended)\nStorage: RPi-MSD-0001 (31.3GB). DO NOT SELECT ANY OTHER STORAGE DEVICE.\n\nIf this storage device does not appear, you need to re-do the rpiboot steps\n\n\nOnce Raspberry Pi Images starts, writing the image to the reTerminal’s memory can take 10-15 minutes.\n\nDo not disconnect the reTerminal during flashing!\n\n\n\n\n\n3.3 Reassembly\n\nOnce the writing and verification process is completed, disconnect the USB-C cable from the reTerminal.\nReturn the memory select switch to the original position. (Do you know why? If not, Reread the part about why we toggled it down in the first place!)\nDon’t re-assemble the heatsink+terminal cover yet – we have a few more steps to take first.\nPlug the raspberry pi into the wall using the Pi Power Supply cable in your reTerminal kit. You may need an extension cord/power bar– you can find one at the front of the class.",
    "crumbs": [
      "Reterminal setup"
    ]
  },
  {
    "objectID": "notes/reterminal-setup/index.html#first-boot",
    "href": "notes/reterminal-setup/index.html#first-boot",
    "title": "Reterminal setup",
    "section": "4 First Boot",
    "text": "4 First Boot\nYour reTerminal has been re-imaged! Make sure you have completed the reassembly steps, particularly that you have toggled the memory select switch back up to normal boot mode.\nAt this point, we are now going to start running commands directly on the reTerminal itself. Follow the steps below:\n\nPlug your reTerminal into the wall using the power supply. There are power bars in the lab that you can use if you need more outlets.\n\nIn general, the reTerminal is powered using the provided power supply in your lab kit.\n\nPlug your reTerminal into the lab monitor using the provided microHDMI to HDMI converter\nPlug your reTerminal into the ethernet using the lab computer ethernet.\nPlug the lab keyboard and mouse into your reterminal USB.\n\nOn first boot, your reTerminal screen will not turn on – this is why we need the microHDMI connection to the external monitor.\nThe first task we need to take care of is fixing the reTerminal screen display drivers.\n\n4.1 Display driver fix\nOnce your are logged into the reTerminal and you can see the display on the lab monitor, follow the steps below:\n\nRead and follow the steps outlined here: “Install reTerminal drivers after flashing new Raspberry Pi OS/ Ubuntu OS or Other OS”, up to and including sudo reboot.\n\nNOTE: recall that you have installed a 64-bit OS on your reTerminal. Do not follow any 32-bit OS steps in the above instructions.\n\nIf the above steps have been completed successfully, your device should reboot and BOTH the raspberry pi screen AND the HDMI connection should work (this takes several seconds, give it a minute before you panic).\n\nMake sure your Pi is connected to the wall power supply, NOT to your computer (the pi screen needs more power than your lab computer can provide).\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf your keyboard is in French mode on the raspberry pi, you can follow the instructions here to set it back into English (US) mode.\n\n\n\n4.2 Update & Upgrade\nA good first step for any OS installation is to ensure all system packages are at the latest version.\nFollow the three steos in the official guide for the reTerminal FAQ Wiki: How to upgrade Raspberry Pi OS and the installed packages - For any steps that ask you to make a choice, just pick the defaults.",
    "crumbs": [
      "Reterminal setup"
    ]
  },
  {
    "objectID": "notes/reterminal-setup/index.html#set-up-remote-connections",
    "href": "notes/reterminal-setup/index.html#set-up-remote-connections",
    "title": "Reterminal setup",
    "section": "5 Set up Remote connections",
    "text": "5 Set up Remote connections\nGoing forward, we want to be able to use the reTerminal without having to plug it into an external monitor.\nFixing the device screen was one step – however, we would also like to be able to use the reTerminal without relying on the small touchscreen either.\nWe are going to rely on remote connections to the reTerminal in general in this class – that is, connecting to the reTerminal using an IP Address.\nBecause the lab network has firewalls, however, we cannot do so directly using the lab ethernet or the campus WIFI.\nTo fix this problem, we are going to use a remote networking tool called Tailscale. Follow the steps below:\n\n5.1 Set up Tailscale\nFirst, create an account on Tailscale. You have the following choices for authentication:\n\n(Recommended) Your GitHub account\nYour school email address\n\nIf you’re curious to know more about Tailscale before you sign up, please ask me! You can also read about it here:\n\nhttps://tailscale.com/why-tailscale\nhttps://tailscale.com/blog/how-tailscale-works\n\nIn the big picture: Tailscale will allow us to establish direct remote connections between our raspberry Pi and our developer environments by creating a Wireguard VPN mesh network for us.\nTo be clear, Tailscale is free, you will not need any of the paid features.\nAfter you have made an account, you will need to set it up on your pi and your developer environments, see instructions below.\n\n5.1.1 On the raspberry pi\nFollow these instructions\nYou can do the “Install with one command” step, OR follow the manual steps if you prefer.\nNOTE: you will probably need to also run the following command on your raspberry pi:\nsudo apt install curl\n\n\n5.1.2 On your lab computer\n\nInstall on your WSL by Following these instructions\n\nYou can do the “Install with one command” step, OR follow the manual steps if you prefer.\n\nALSO install on the main Windows machine by following these instructions\n\nYou may need to use elevated permissions for this, see Note 1\n\n\nNOTE: you will probably need to also run the following command on your WSL:\nsudo apt install curl\n\n\n5.1.3 On your personal computer\nDepending on your operating system:\n\nWindows:\n\nInstall on your WSL by Following these instructions\nYou can do the “Install with one command” step, OR follow the manual steps if you prefer.\n\n\nNOTE: you will probably need to also run the following command on your WSL:\nsudo apt install curl\n\nmacOS: Follow these instructions\nLinux: Follow these instructions\n\nYou can do the “Install with one command” step, OR follow the manual steps if you prefer.\n\n\n5.1.4 Verify your tailscale setup\nOn either your lab/personal developer environment, OR your raspberry pi, run the command:\ntailscale status\nYou should see the IP address for both your reTerminal AND your lab/personal developer environment. Take note of these IP addresses before moving on to the next steps.\n\n\n\n5.2 Set up VNC\nIn this section you will connect to the graphical desktop environment remotely using a VNC session. This will allow you to control the raspberry pi from your lab computer over the graphical shell of the lab computer, in addition to SSH.\n\nOnce connected to the provided power cable, the reTerminal will boot and automatically login into the graphical desktop environment as the default user.\nThe reTerminal has a touch screen which you are welcome to use for the next steps. However, I recommend plugging in your lab keyboard and/or mouse for these next few steps. Let me know if you need a keyboard/mouse.\nEnable the VNC client in the Raspberry Pi Configuration menu.\n\nClick on the Raspberry Pi icon (top right).\nSelect Preferences &gt; Raspberry Pi Configuration.\nOpen the Interfaces tab.\nEnable the VNC server (disabled by default).\n\nOpen a terminal on your Raspberry Pi. Double check your IP address using tailscale status\nAt your lab computer, start the VNC Viewer client (RealVNC). You can run this program without elevated access.\n\nRealVNC is already installed on the lab computers. If working on a personal device, you can install it here\nNOTE: you DO NOT need to make an account or sign up for RealVNC. There is an option to\n\nConnect to your Raspberry Pi using VNC Viewer in your lab computer.\nEnter the hostname you assigned to your Raspberry Pi in Part 1, step 5 or the IP address you noted in step 5.\n\nUse the username and password you configured in Part 1, step 5.\n\n\n\n\n5.3 Set up SSH\nThe SSH server inside your Raspberry Pi should already be enabled by default (from Part 1, step 5).\n\nTo double check that the ssh server is enabled on your Pi: follow the official instructions on Setting up the SSH Server on the Raspberry Pi.\n\n\n5.3.1 Connecting over CLI\nYou can establish an SSH connection to the reTerminal from your developer environment. If your connection is successful you should see the a similar prompt:\nuser-name@hostname:~ $\nwhere hostname and user-name correspond to the choices you made during your imaging of the pi.\n\nFollow the official Raspberry Pi instructions (NOTE: Linux instructions apply to WSL!) Secure Shell from Linux or Mac OS\n\nFor Lab 3, I ask you to obtain the reTerminal’s MAC address.\nYou can do so running ifconfig command in an ssh session, and checking the properties of the wireless network card (wlan0):\n\nRun the command ifconfig or the command ip address\nLook for the wireless network adapter wlan0:\nThe MAC address will be listed there.I\n\nStyle points: use grep and pipe to grab the MAC address directly to your clipboard\n\n\n\n\n5.3.2 Connecting over VSCode\nYou can use VS Code in your lab workstation to create a development environment inside the Raspberry Pi which will be controlled from the lab workstation.\nIf you would like to know more about how this extension works, visit Remote Development using SSH.\nBelow is a 5min video that illustrates how the Remote - SSH extension works:\nVS Code Remote Development using SSH to a Raspberry Pi\n\nIn your lab workstation, ensure you have installed the following VS Code extensions:\n\nRemote - SSH, by Microsoft.\nPython extension, by Microsoft.\n\nConnect your lab workstation to your Raspberry Pi by following the Remote - SSH extension’s official instructions: Getting started. Once VS Code is connected to the reTerminal, you are now in a new development environment inside the reTerminal. Complete the following tasks:\nInstall the VS Code Python extension (this time inside the reTerminal, not in your lab workstation like in step 1).\n\nIf necessary, follow the guide: Getting Started with Python in VS Code\n\nOn the Raspberry Pi, open the folder lab1 in the home directory of the reTerminal (created in Part 3, step 4).\nCreate a new file named lab-script.py inside the folder lab1 and include the code:\n\nprint(‘Hello from inside the pi!’)\n\nExecute your code from within VS Code using the play button.",
    "crumbs": [
      "Reterminal setup"
    ]
  },
  {
    "objectID": "notes/azure-portal/index.html",
    "href": "notes/azure-portal/index.html",
    "title": "Azure Portal Setup",
    "section": "",
    "text": "Image source",
    "crumbs": [
      "Azure Portal Setup"
    ]
  },
  {
    "objectID": "notes/azure-portal/index.html#getting-started-with-azure",
    "href": "notes/azure-portal/index.html#getting-started-with-azure",
    "title": "Azure Portal Setup",
    "section": "1 Getting started with Azure",
    "text": "1 Getting started with Azure\nThese notes cover the basics for setting up your Azure account, and the main Azure resource we will use in class, the IoT Hub.\n\n1.1 Creating an account\nIf you have not already, create an Azure account using your student email.\nOtherwise, log in to your azure account at https://portal.azure.com\n\n\n1.2 Account cost overview\nThe Azure services we need for this course (creating an IoTHub Resource group and sending messages) are free to use, within a daily message limit (~8000).\nThis means that we need to take care to either use student credits, or ensure we have a budget-warning system in place, to prevent incurring unnecessary costs.\nNote that John Abbott students get $100 of credit for 12 months. After 12 months, or when your student credit runs out, you have two options:\n\nupgrade your account to “pay-as-you-go”\n\nPay-as-you-go is will still be free for our course, as long as you set up a budget alert in case you exceed the free-usage-limits\n\ncreate a new non-student account using a non-student email address, comes with $200 credit\n\nIn the big picture, no matter what your situation is, your Azure plan should have the following properties:\n\nNo upfront costs – your account should be free to create, and every service/resource you create should be free to create.\nNo monthly costs – your costs should be covered entirely by one of the following:\n\nyour student account credit\n$200 free credit for\npay-as-you-go that is free per month, but will charge your credit card per usage beyond the threshold of free tools.\n\nA budget alert system that warns you when your usage exceeds &gt;$5 . Even if you are in situation 2.3 (pay-as-you-go free services), you should not expect to pay more than a couple dollars. See the Setting budget alerts section.\n\nSee the FAQ section if you have further questions.",
    "crumbs": [
      "Azure Portal Setup"
    ]
  },
  {
    "objectID": "notes/azure-portal/index.html#managing-your-account",
    "href": "notes/azure-portal/index.html#managing-your-account",
    "title": "Azure Portal Setup",
    "section": "2 Managing your account",
    "text": "2 Managing your account\nBelow are some useful account management how-tos.\n\n2.1 Checking your balance\nAs you experiment with Azure you might want to know your current balance.\n\nSign in to the Azure portal\nSearch for Cost Management\nSelect Payment methods\nCheck your Azure credits panel.\n\n\n\n\n\n\nCredit check panel in Azure\n\n\nFor more details, see Track Microsoft Customer Agreement Azure credit balance\n\n\n2.2 Setting budget alerts\nIt’s possible to setup email alerts when your Azure spending exceeds a certain threshold. This avoids unexpected spending of Azure credits.\n\nIn the Azure Portal Home, select the Resource groups service.\nSelect the resource group you want to monitor with a budget.\nIn the left panel, select Budgets under the Cost Management group and select Add.\nGive your budget a name and an amount.\nSelect the percent spending for the alert and the email to receive the notification.\nClick Create.\n\n\n\n\n\n\nMenu for creating a budget for a resource group.",
    "crumbs": [
      "Azure Portal Setup"
    ]
  },
  {
    "objectID": "notes/azure-portal/index.html#using-the-tool",
    "href": "notes/azure-portal/index.html#using-the-tool",
    "title": "Azure Portal Setup",
    "section": "3 Using the tool",
    "text": "3 Using the tool\nBelow are some how-tos for services we will be using in this class.\n\n3.1 Documentation\nThe Azure resource we will be using the most, IoT Hub, has complete documentation/how-to-guides/tutorials at https://learn.microsoft.com/en-us/azure/iot-hub/.\n\nFamiliarize yourself with the left-hand menu, including the “Quickstarts” and “How-to guides”.\n\n\n3.2 Creating an IoT Hub Resource\nThere are two ways to create resources in Azure:\n\nusing the Azure portal website: https://portal.azure.com\nusing Azure CLI\n\nThe Azure documentation typically has instructions for both methods in all their tutorials – you can select which method you want to choose by clicking the tabset button for it:\n\nAzure PortalAzure CLI\n\n\nCreate and manage Azure IoT Hubs using Azure Portal.\nNote that each section has the “Azure Portal” option selected.\n\n\nCreate and manage Azure IoT Hubs using Azure CLI.\nNote that each section has an “Azure CLI” option selected.\n\n\n\nNo matter which method you choose, you should make sure the resource you’re making is free and in a nearby region.\nSuggested choices for the following options:\n\nSubscription: Your active subscription (probably “Azure for Students”)\nResource group: Create ONE resource group for this class (any name is fine)\nIoT hub name: Recommended: yourname-iot-hub\nRegion: Central US or East US or Canada East\nTier: Free\nDaily message limit: 8,000 ($0/month)\n\nYou can keep the defaults for all subsequent menus (Networking, Management, Add-ons, Tags).\nOnce you’re done creating your IoT Hub, it should appear in your Azure portal. See the figure below:\n\n\n\n\n\n\nFigure 1: In this figure, the Resource Group is named 6P3, and a IoT Hub Resource named 6P3-IoT-Hub can be seen in the list of its resources. You can view your resource groups on Azure Portal by clicking on the “Resource groups” menu item on the left.",
    "crumbs": [
      "Azure Portal Setup"
    ]
  },
  {
    "objectID": "notes/azure-portal/index.html#faq",
    "href": "notes/azure-portal/index.html#faq",
    "title": "Azure Portal Setup",
    "section": "4 FAQ",
    "text": "4 FAQ\n\nQ: I’m not eligible for a student account?\n\nStudent accounts last for 12 months – if you have taken courses in the past that require an Azure account, and it was more than 12 months since you created that account, your student credits have unfortunately expired.\nSee What happens after I use my $100 credit or when I’m at the end of 12 months?\n\nQ: I’m not eligible for the $200 free account credit for my non-student account?\n\nThe $200 credit is only available if you have not used that email address for an Azure account in the past (and it only lasts for one month).\nIf you are not eligible for the $200 credit on the email address you have provided, you have two choices, both of which are fine:\n\nuse a new email address\nregister for a free-tier pay-as-you-go.\n\n\nQ: I tried to register for the pay-as-you-go and I’m told it costs $35 per month?\n\nThe pay-as-you-go account should not cost any upfront or monthly charges. Double check that you do not select options like “tech support plans” or “standard-tier service” – ALWAYS opt out of any paid options, ALWAYS select free-tier for any choices provided to you by Azure.\n\nQ: if I’m on free-tier and there’s no monthly costs, why do I need to monitor the budget?\n\nThe pay-as-you-go account may charge your credit card, only when you use the service beyond the defined free-tier limits.\nIn our class, we will have IoT Hub services that handle communication between your Mobile App from App Dev III, and your reTerminal devices from Connected Objects. The free-tier limit for the IoTHub is 8000 messages per day – I don’t think there will be a reason to exceed this, so by default, you may never have to pay any money for this service.\nHowever, there may be small charges to your credit card, ONLY IF you exceed the free-tier limits of the services we use. These charges will be small, but it is possible for them to get out-of-hand by accident. To make sure they do not get out of hand by accident, you must set up a Budget for your Resource Group to alert you when charges exceed a certain number. Please review the course notes about budgets and set up an alert if you have not done so.\n\nQ: I have some other question?\n\nPlease let me know if you have any questions, or if there’s something else that doesn’t seem to work. I will update these instructions if there are more troubles you run into that I can share answers for.",
    "crumbs": [
      "Azure Portal Setup"
    ]
  },
  {
    "objectID": "notes/python-package-management/index.html",
    "href": "notes/python-package-management/index.html",
    "title": "Python package management",
    "section": "",
    "text": "This lecture looks at the ways to acquire use and install external python packages.\n\nInstalling system python packages with apt on debian\nCreating virtual environments with venv\nInstalling packages locally with pip",
    "crumbs": [
      "Python package management"
    ]
  },
  {
    "objectID": "notes/python-package-management/index.html#overview",
    "href": "notes/python-package-management/index.html#overview",
    "title": "Python package management",
    "section": "",
    "text": "This lecture looks at the ways to acquire use and install external python packages.\n\nInstalling system python packages with apt on debian\nCreating virtual environments with venv\nInstalling packages locally with pip",
    "crumbs": [
      "Python package management"
    ]
  },
  {
    "objectID": "notes/python-package-management/index.html#python-on-raspberry-pi",
    "href": "notes/python-package-management/index.html#python-on-raspberry-pi",
    "title": "Python package management",
    "section": "2 Python on Raspberry Pi",
    "text": "2 Python on Raspberry Pi\nNote: most of these notes were adapted directly from the Raspberry Pi docs: Python on Raspberry Pi\nPython 3 is installed by default on Raspberry Pi OS, and is used for many important functions. Interfering with the system Python installation can cause problems for your operating system, so it’s important that if you install third-party Python libraries, you use the correct package-management tools.\nThere are two routes to installing libraries into the default python distribution:\n\napt to install pre-configured system python packages distributed by Debian\npip to install cross-platform packages from https://pypi.org/\n\n\n\n\n\n\n\nNote\n\n\n\nInstalling packages using apt is the preferred method for using python applications in Raspberry Pi OS / Debian.\nInstalling packages using pip in a virtual environment is the preferred method for installing Python libraries for developing a python project.",
    "crumbs": [
      "Python package management"
    ]
  },
  {
    "objectID": "notes/python-package-management/index.html#installing-python-packages-using-apt",
    "href": "notes/python-package-management/index.html#installing-python-packages-using-apt",
    "title": "Python package management",
    "section": "3 Installing Python packages using apt",
    "text": "3 Installing Python packages using apt\nPackages installed via apt are tested, are usually pre-compiled so they install faster, and are designed for Raspberry Pi OS. They won’t break your system. Installing via this route also means that all required dependencies are also installed, and a log of installation is maintained by the OS so installation can be easily rolled back (libraries can be uninstalled) if needed.\nYou can see a complete list of Python apps maintained by Debian developers on the stable repositories here. Many, but not all, popular open-source Python packages are maintained for Debian by professionals and hobbyists alike.\nTo see an example: you may find yourself wanting to install the Python 3 library to support the Raspberry Pi BuildHAT component. To install this using apt, you would:\n$ sudo apt install python3-build-hat\nIf you want to install a Python library called “foobar” you can use apt search foobar to find the exact package name – partial search matches are supported, so you don’t need to know the whole name in advance. Very useful!\nUsing apt makes installing larger packages, like numpy (which has many native dependencies including a Fortran compiler), much simpler and more predictable than installing individual packages using Python’s own package-management system.\n\n\n\n\n\n\nNote\n\n\n\nIn apt, Python packages have a consistent naming scheme: you’ll find that the a given package “foobar” is going to be called python-foobar or python3-foobar in the apt repositories – this helps distinguish python packages from other packages on a debian system.",
    "crumbs": [
      "Python package management"
    ]
  },
  {
    "objectID": "notes/python-package-management/index.html#installing-python-packages-using-pip",
    "href": "notes/python-package-management/index.html#installing-python-packages-using-pip",
    "title": "Python package management",
    "section": "4 Installing python packages using pip",
    "text": "4 Installing python packages using pip\nThe goal of operating systems like Debian is to be stable, that is, unlikely to crash due to a poorly tested/implemented program. That does come at a cost of not always offering the latest versions of packages, nor having a complelely comprehensive set of the bleeding edge packages that are available.\nMany developer depend on packages that are under active development. And, there are many packages that there are no Debian maintainers for. For cases like these (and many other cases, as we’ll see) it is useful for programming languages like Python to have an independent packaging manager.\nFor python, the most commonly used package manager is pip.\n\nPip installs packages from the Python Package Index repository (pypi.org) rather than the Debian or any other package repositories.\n\nEven though Raspberry Pi OS comes with pip installed, we can’t just use it right away, as we we’ll see in the next section.\n\n4.1 pip cannot be used to install system-wide python packages\nIn previous versions of Debian/RaspberryPi OS operating system, it was possible to install libraries directly, system-wide, using the package installer for Python, pip.\nYou’ll find the following sort of command in many tutorials online:\n$ pip install buildhat\nIn newer versions of Raspberry Pi OS, and most other operating systems, this is disallowed. If you try and install a Python package system-wide you’ll receive an error similar to this:\n$ pip install buildhat\nerror: externally-managed-environment\n\n× This environment is externally managed\n╰─&gt; To install Python packages system-wide, try apt install\n  python3-xyz, where xyz is the package you are trying to\n  install.\n\n  If you wish to install a non-Debian-packaged Python package,\n  create a virtual environment using python3 -m venv path/to/venv.\n  Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make\n  sure you have python3-full installed.\n\n  For more information visit http://rptl.io/venv\n\nnote: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\nhint: See PEP 668 for the detailed specification.\nThis error is generated because you’re trying to install a third-party package into the system Python. A long-standing practical problem for Python users has been conflicts between OS package managers like apt and Python-specific package management tools like pip. These conflicts include both Python-level API incompatibilities and conflicts over file ownership.\nTherefore from Debian Bookworm onwards, packages installed via pip must be installed into a Python virtual environment using venv. A virtual environment is a container where you can safely install third-party modules so they won’t interfere with, or break, your system Python.",
    "crumbs": [
      "Python package management"
    ]
  },
  {
    "objectID": "notes/python-package-management/index.html#virtual-environments",
    "href": "notes/python-package-management/index.html#virtual-environments",
    "title": "Python package management",
    "section": "5 Virtual environments",
    "text": "5 Virtual environments\n\n5.1 How virtual environments solve the pip problem\nPython virtual environments are folders that we can create in a python project using the venv module.\nThey act as a container that allows you to install python libraries and applications in a folder separated from your system python packages.\nThis has the following benefits:\n\nAvoid system pollution\n\nInstalling packages to the OS’s global Python will mix them with OS relevant packages. This could have unexpected side effects on OS tasks.\nBecause of the reason above, updating OS packages might overwrite or delete global Python packages.\n\nAvoid project dependency conflicts\n\nPython projects might require different versions the same external library.\n\nMake projects reproducible in other environments.\n\nSince all dependencies are isolated to a specific project, it is easier to identify and document them.\nOnce the dependencies are “locked”, the project dependency can be easily reproduced in other environments.\n\n\n\n\n5.2 Creating a virtual environment\nInside of the project directory where you want to create virtual environment:\n$ python -m venv .venv\nNote that the .venv argument is the name of your virtual environment. .venv is a popular choice that will be automatically recognized by VSCode.\nThe python -m venv .venv command creates a directory called .venv:\n$ ls -la\ntotal 12\ndrwxr-xr-x  3 user user 4096 Oct  3 14:34 .\ndrwx------ 20 user user 4096 Oct  3 14:34 ..\ndrwxr-xr-x  5 user user 4096 Oct  3 14:34 .venv\n$\n\n\n5.3 Activating and deactivating a virtual environment\nInside the virtual environment directory is a full Python distribution (use ls -l to see the files). How do we use it?\nA nice feature of venvs is that they must be activated to be used. This allows you to maintain many different venvs all over your machine, allowing you to control the python dependencies for each project independently.\nTo activate your virtual environment and make that version of Python the one you’re currently using, python -m venv creates a bash environment script called activate that you can run with the bash command source:\n$ source .venv/bin/activate\n(.venv) $\nYou’ll see that your prompt is now prepended with (.venv) to indicate that you’re no longer using the system Python. Instead, you’re using the version of Python contained inside your virtual environment. Any changes you make here won’t cause problems for your system Python; nor will any new modules you install into your environment.\n(.venv) $ which python\n/home/username/my_project/.venv/bin/python\nYou can leave your virtual environment and return to using the system Python by typing:\n(.venv) $ deactivate\n…and check for yourself that the shell’s python has updated by using which python.\n\n\n5.4 Installing venv packages with pip\nOnce a virtual environment is active, packages installed with pip will be local to that virtual environment.\nFor a virtual environment named .venv, for example:\n.venv/\n│  └── lib/\n│    └── python3.X/\n│      └── site-packages/\nFor example, installing gpiozero:\n# Notice the shell prompt indicates a virtual env is active\n$ (.venv) pip install gpiozero\n...\n$ (.venv) ls -l .venv/lib/python3.11/site-packages/ | grep gpiozero\ndrwxr-xr-x@    - username 23 Jan 20:48 gpiozero                   # source code\ndrwxr-xr-x@    - username 23 Jan 20:48 gpiozero-2.0.1.dist-info   # distribution archive\ndrwxr-xr-x@    - username 23 Jan 20:48 gpiozerocli                # gpiozero comes with an extra python package for cli use\n\n\n5.5 Checking what packages are installed in a .venv\nYou can use pip freeze to get the packages installed in the python environment:\n$ pip freeze\nThis command is useful for a few reasons:\n\nverifying that the package you attempted to install is the version you expect it to be\ncreating a requirements.txt file from the output of the pip freeze command.\n\n\n\n5.6 “Re-using” Virtual Environments\nIf you move or copy a project that uses virtual environments to a different folder, you must re-initialize the virtual environment.\nFrom the official docs:\n\nScripts installed in environments contain the absolute paths to their environment’s interpreters.\n\nBecause of this, virtual environments are non-portable – instead of moving them around, the better thing is to make sure you have a method to recreate a virtual environment.\nFor example, create a requirements file requirements.txt, and invoke pip install -r requirements.txt, to recreate your virtual environment dependencies.\nYou can see more about managing your projects dependencies in the course notes on project configuration",
    "crumbs": [
      "Python package management"
    ]
  },
  {
    "objectID": "notes/python-package-management/index.html#troubleshooting",
    "href": "notes/python-package-management/index.html#troubleshooting",
    "title": "Python package management",
    "section": "6 Troubleshooting",
    "text": "6 Troubleshooting\nHere are a few things to know in case you’re having issues using packages you’ve installed in a virtual environment.\n\n6.1 Check library location\nCheck the location of where the module was installed with pip show &lt;module-name&gt;:\n(.venv) user@host:~ $ pip show gpiozero\n\nName: gpiozero\nVersion: 2.0.1\nSummary: A simple interface to GPIO devices with Raspberry Pi\nHome-page: https://gpiozero.readthedocs.io/\nAuthor: Ben Nuttall\nAuthor-email: ben@bennuttall.com\nLicense: BSD-3-Clause\nLocation: /absolute/path/to/.venv/lib/python3.12/site-packages\nRequires: colorzero\nRequired-by:\nThe library location is specified by the Location field.\n\n\n6.2 Check python’s library paths\nSimilarly to Linux, there are environmental variables that determine where python will look for installed modules/libraries.\nSee the paths where python is looking for libraries with sys.path. They should look something like:\n\nvenvsystem\n\n\n(.venv) pi@raspberrypi:~ $ python\n\n&gt;&gt;&gt; import sys      # Exposes configuration used by the python interpreter.\n&gt;&gt;&gt; sys.path        # Lists all paths where interpreter looks for modules.\n['', '/usr/lib/python311/site-packages']\n\n\npi@raspberrypi:~ $ python\n\n&gt;&gt;&gt; import sys      # Exposes configuration used by the python interpreter.\n&gt;&gt;&gt; sys.path        # Lists all paths where interpreter looks for modules.\n['', '/usr/lib/python311.zip', '/usr/lib/python3.11', '/usr/lib/python3.11/lib-dynload', '/usr/local/lib/python3.11/dist-packages', '/usr/lib/python3/dist-packages', '/usr/lib/python3.11/dist-packages']\n\n\n\nIf the output of pip show seeed-python-reterminal is not in this list, python will not find it when you import it.\n\n\n6.3 Using packages that require root permissions\nThe venv pattern is very useful for maintaining packages that do not require root permissions. This is a feature, not a bug – that way, you can have many developers independently install packages on one machine without needing to give them root permissions.\nSometimes (and particularly, in this couse) the packages we install require root permissions to work. Consider the packages that edit LED /sys/ files, like seeed-python-reterminal we install in Lab 2. We will run into an issue if we try to use these packages as normal users:\n(.venv) user@hostname:~/lab2/python $ python ./leds.py\n# ... output redacted\n&lt;Permissions error&gt;\nIt is unfortunately not fixable by simply using sudo:\n(.venv) user@hostname:~/lab2/python $ sudo python ./leds.py\n# ... output redacted\n&lt;Module seeed-python-terminal not included/available&gt;\nWhy isn’t the package available? Well, sudo runs python as the root user. The root user does not have your venv of python in its path. Try the commands below on your system to see if you understand what I mean.\n\nnormal usersudo user\n\n\n(.venv)user@host:~ $ which python\n/home/user/path/to/.venv/bin/python\n\n\n(.venv)user@host:~ $ sudo su\n(.venv)root@host:~ # which python\n/usr/bin/python\n\n\n\nHow to resolve this problem? It is similar to the sudo echo &gt; file problem we saw with Bash. We need to make sure sudo is applying to the correct python executable. There are a few ways we can do this, all of which will look something like this:\n# use absolute path to virtual environment python\n(.venv)user@host:~ $ sudo /absolute/path/to/venv/python ./leds.py\n\n# use relative path to virtual environment python\n(.venv)user@host:~ $ sudo ./.venv/relative/path/to/venv/python ./leds.py\n\n# use command substitution\n(.venv)user@host:~ $ sudo $(shell command for finding where python is installed) ./leds.py\nTry a few of these options out (will not work if you copy paste! Make sure you understand the commands)",
    "crumbs": [
      "Python package management"
    ]
  },
  {
    "objectID": "notes/python-package-management/index.html#further-reading",
    "href": "notes/python-package-management/index.html#further-reading",
    "title": "Python package management",
    "section": "7 Further reading",
    "text": "7 Further reading\n\nShort guide by python.org: Installing packages using pip and virtual environments\nDetailed guide by RealPython.com: Python Virtual Environments: A Primer",
    "crumbs": [
      "Python package management"
    ]
  },
  {
    "objectID": "notes/analog-vs-digital/index.html",
    "href": "notes/analog-vs-digital/index.html",
    "title": "Signals: Analog and digital",
    "section": "",
    "text": "We will use analog and digital signals to communicate with sensors.\nThis section briefly compares these two types of signals and illustrates how we might encounter while using IoT hardware.\nTo compare analog and digital signals, let’s look at how we can track 3 different sources of information:",
    "crumbs": [
      "Signals: Analog and digital"
    ]
  },
  {
    "objectID": "notes/analog-vs-digital/index.html#analog-the-natural-world",
    "href": "notes/analog-vs-digital/index.html#analog-the-natural-world",
    "title": "Signals: Analog and digital",
    "section": "1 Analog: the natural world",
    "text": "1 Analog: the natural world\nWe live in an analog world. Most of the naturally occurring events that we experience happen in an analog form.\n\nAnalog signals varies continuously in time and amplitude.\nThis means that changes happen over a range of values with infinite “in between” steps.\n\nFor example, the following events have an infinite number of transitions. It only depends on how accurately you can track them:\n\nThe colors in a rainbow.\nThe position of a swinging pendulum.\nThe exact time of the day.\n\n\n1.1 Temperature\nThe A/C and heating systems in a house are typically activated once the temperature reaches some minimum and maximum values.\nThe chart below is tracking the temperature in a room as the A/C and heating systems are activated over a few days.\n\n\n Temperature changes over time   Adapted from poster: Analog and Digital Signals by Digikey.\n\nLet’s say the maximum temperature reached in the room is 30C and the minimum temperature is 15C. A person in the room would have observed every possible temperature in between that range (15.01, 15.0101, 15.0102, 15.01021, etc).\nThe change in temperature produces analog information since it is changing over an infinite range of values.\n\n\n1.2 Sound\nSound is also an example of an analog signal.\nWhen a volume of air is displaced quickly, air molecules are compressed against each other creating a high-pressure region. The high pressure region expands and compresses the air in the neighbouring area. Sound can travel thanks to this “chain effect”.\n\nBy observing how the air pressure changes over time, we can draw a sound wave.\nThis is how speakers and microphones interact with air to generate sound.\n\n\n\n\nAnimation of acoustic wave travelling in air\n\n\n\n Sound waves travelling through air   - Waves and Acoustics Animations by isvr.\n\nSound waves are also an example of an analog signal because the amplitude of the wave transitions through the entire range of possible values between the Min and Max.\n\n\n1.3 Analog Signals Graphs\nA signal varies over time. It’s helpful to plot it on a graph where time is plotted on the horizontal, x-axis, and the value being tracked on the vertical, y-axis.\nLooking at a graph of a signal is usually the easiest way to identify if it’s analog or digital.\n\nWhen dealing with electricity we track voltage levels (amplitude).\nA time-versus-voltage graph of an analog signal should be smooth and continuous.\n\n\n\n\nAnalog Sine Wave\n\n\nWhile these signals may be limited to a range of maximum and minimum values, there are still an infinite number of possible values within that range.\nFor example, the analog voltage coming out of your wall socket oscillates between -120V and +120V. As you increase the resolution more and more, you discover an infinite number of values that the signal can actually be (64.4V, 64.42V, 64.424V, and other increasingly precise values).",
    "crumbs": [
      "Signals: Analog and digital"
    ]
  },
  {
    "objectID": "notes/analog-vs-digital/index.html#digital-discrete-values",
    "href": "notes/analog-vs-digital/index.html#digital-discrete-values",
    "title": "Signals: Analog and digital",
    "section": "2 Digital: discrete values",
    "text": "2 Digital: discrete values\nDigital signals are discrete, which means that at any given moment, the signal strength must be represented by a integer number. There are no half numbers.\nTypically, digital signals will be one of two values, a high voltage and a low voltage.\n\nThe specific values of the high and low voltages depend on the hardware being used.\n\nBelow is a the timing graph of a signal whose low voltage is 0 volts and high voltage is 5 volts.\nThis type of signal is also known as a square waves.\n\n\n\n\n\n\nSquare wave signal. Two values, either 0V or 5V.\n\n\n\n\nFigure 1: 0 to 5 volt “pulses” forming a square wave. Analog vs. Digital by Digikey\n\n\n\n\n2.1 Digital Simulating Analogue\nA digital signal might be a discrete representation of an analog waveform.\nViewed from afar, the wave function below may seem smooth and analog, but when you look closely there are tiny discrete steps as the signal tries to approximate analog values.\n\n\n\n\n\n\nDigital Sine Wave\n\n\n\n\nFigure 2: Oscillating voltage from -120V to 120V approximated by overlapping digital signals. Analog vs. Digital by Digikey\n\n\n\nThe number of discrete steps that a digital signal can use depends on how many bits are available for each recorded value. This is also called bit resolution (see Sampling below).\nFor example, if a data point can only use one bit of definition, then it can only track on/off states, resulting in the red square wave seen above.\nHowever, if a data point can use 8 bits to store a value, then it can represent a value ranging from 0 to 255 (see the green sinusoidal voltage wave above).",
    "crumbs": [
      "Signals: Analog and digital"
    ]
  },
  {
    "objectID": "notes/analog-vs-digital/index.html#analog-to-digital-conversion-adc",
    "href": "notes/analog-vs-digital/index.html#analog-to-digital-conversion-adc",
    "title": "Signals: Analog and digital",
    "section": "3 Analog to Digital Conversion (ADC)",
    "text": "3 Analog to Digital Conversion (ADC)\nIf most natural events are analog, how do we represent them in digital format? For example, if audio is analog, how can we get a computer to play music?\n\nAn analog to digital converter (ADC) is a device that can read a analog signal and generate a digital representation of this signal.\n\nADC’s are specific to the type of analog signal they are trying to read (sound, voltage, temperature, light, etc).\nReferring to the room temperature example used earlier, if we used a digital thermometer to record the temperature, our graph might look like the following:\n\n\n Temperature represented as a digital reading   Poster: Analog and Digital Signals by Digikey.\n\nIn the example above, a digital temperature reading was taken every 3hr.\n\nThe digital signal representation of the temperature is not smooth or accurate.\nHowever, depending on the application it might be sufficient.\n\nTo make the signal smoother or more representative of the real analog signal we need to take samples more regularly (eg. every 1min).\nThe process of reading an analog signal over time to generate its digital representation is called sampling.",
    "crumbs": [
      "Signals: Analog and digital"
    ]
  },
  {
    "objectID": "notes/analog-vs-digital/index.html#sampling",
    "href": "notes/analog-vs-digital/index.html#sampling",
    "title": "Signals: Analog and digital",
    "section": "4 Sampling",
    "text": "4 Sampling\nSampling is the process of inspecting the value of an analog signal at regular time intervals.\nWhen an ADC is sampling an analog signal, there are two variables that will characterize the digital output:\n\nSampling rate.\nBit resolution.\n\n\nMusic sampling Some music genres like rap and hip-hop commonly sample song segments from other artists to play in the background.\n\nThe term sampling is used because traditionally song segments were extracted from vinyl records which produce analog sound and saved in digital formats.\n\n\n\n\n\n4.1 Sampling rate\nThe time between samples is the sample period (T, in seconds), and the number of samples taken per second is the sample frequency or sample rate (fs, in samples/second or Hz).\nBasically, sampling is taking snap-shot values of the analog signal at regular time intervals.\n\n\n\nAnalog signal showing sample period and sample frequency\n\n\n\n Sample period and fequency for an electric signal   - US Naval Academy.\n\nSampling period examples:\n\nIf a signal is sampled every half second (0.5s sampling period, T), the sample frequency (*f**) is 1/0.5s = 2 Hz (times per second).\nIf the sample frequency (f) is 2000 Hz, the sampling period (T) is = 1/2000Hz = 0.0005 secs (0.5 milliseconds).\n\nThe higher the sample rate, the more accurate the digital signal is.\n\n\n\nSignals of increasing sample rate\n\n\n\n Same signal sampled with different sample rates   - US Naval Academy.\n\n\n\n4.2 Encoding & Bit resolution\nEncoding is the process of mapping the sampled analog signal value to discrete binary numbers (digital information).\n\nADC devices are characterized by the number of bits available to represent a signal.\n\nFor example, consider an electrical signal that ranges from -1 volts to to +1 volts and must be converted to a 3 bit number.\n\n\n4.3 Encoding Procedure\nTo approximate the numeric conversion from a voltage signal to a digital output, follow the steps below:\n\nIdentify the minimum and maximum voltage values that can be observed in the input.\n\nExample: -1 to +1 volts (2 volt range)\n\nIdentify the minimum and maximum binary values that can generated as the output. Note the amount of unique numbers represented by this range\n\nExample: For a 3-bit number: 000 (decimal 0) is the minimum and 111 (decimal 7) is the maximum, making a total of 8 unique numbers.\n\nDivide the voltage range that will be observed (max voltage - min voltage) by the amount of unique binary numbers available. This will determine the voltage increment that each binary number represents.\n\nExample: (+1V - (-1V)) / 8 = 2V/8 = 0.25V\n\nLayout each binary number and their increments and read where the voltage values lie.\n\nAssuming the ADC has a resolution of 3 bits, all voltage values (-1V to 1V) must be represented with at most 3 bits, or 2^3 = 8 unique values (0 is a unique value).\n\n\n\nvoltage signal converted to a bit resolution of 3 bits\n\n\n\n Voltage signal between -1V and 1V converted to digital with a 3 bit resolution   - US Naval Academy.\n\nIn this case:\n\n000 is assigned to the voltages from -0.75 V to -1.0 V,\n001 is assigned to the voltages from -0.5 V to -0.749 V,\n010 is assigned to the voltages from -0.25 V to -0.49 V,\nand so on.\n\nThe binary representation of the above signal is:\n110 101 100 011 011 100 110 110 100 010 000 000 001\nIf the sampling is happening 2000 times per second (Hz) and for every sample we generate 3 bits, we are generating 2000 x 3 bits = 6000 bits/sec of information. This is also know as the bitrate.\nIf a song of 3 minutes is sampled with the same specs, how big will this file be in kB?\nWhat is the bitrate of music on Spotify?\n\n\n4.4 Sensor voltages & bit resolution\nIoT systems typically work with electrical signals encoded in voltage levels.\nEvery hardware that converts from analog to digital (and vice-versa) needs to have an appropriate ACD device.\nMicrocontrollers are often used to process these signals because they often have an integrated ACD and they are very inexpensive.\n\n\n4.5 Video Reference",
    "crumbs": [
      "Signals: Analog and digital"
    ]
  },
  {
    "objectID": "about/calendar/index.html",
    "href": "about/calendar/index.html",
    "title": "Calendar",
    "section": "",
    "text": "This is the authoritative calendar for content covered in class, and for content you should expect to see covered as we continue through the course.\nYou can compare this calendar with the calendar from the Course Outline to see where this course has deviated from the original outline."
  },
  {
    "objectID": "about/calendar/index.html#deliverables",
    "href": "about/calendar/index.html#deliverables",
    "title": "Calendar",
    "section": "1 Deliverables",
    "text": "1 Deliverables\n\nLab 0: dev env setup and bash review: Due January 31 (Demo: in-class. Code: end of day 11:59pm)\nLab 1: git good, bash better: Due February 10 end of day 11:59pm\nLab 2: Reusing functions in bash: Due February 24 end of day 11:59pm"
  },
  {
    "objectID": "about/calendar/index.html#lectures",
    "href": "about/calendar/index.html#lectures",
    "title": "Calendar",
    "section": "2 Lectures",
    "text": "2 Lectures\n\nJan 20: Introduction to course\nJan 24: Setting up developer environment\nJan 27: Review bash, begin Lab 0\nJan 31: Work on Lab 0\nFeb 3: Begin Lab 1\nFeb 7: (class cancelled, continue lab 1)\nFeb 10: Finish Lab 1\nFeb 14: Start Lab 2\nFeb 17: Class cancelled due to storm\nFeb 21: Continue work on Lab 2\nFeb 24: Begin Lab 3"
  }
]