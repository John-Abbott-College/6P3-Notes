[
  {
    "objectID": "notes/python-oop/index.html",
    "href": "notes/python-oop/index.html",
    "title": "Python OOP",
    "section": "",
    "text": "Explanation of how OOP works in Python\nType annotations for Python variables, functions, and classes\nInheritance, interfaces\nEnums",
    "crumbs": [
      "Python OOP"
    ]
  },
  {
    "objectID": "notes/python-oop/index.html#overview",
    "href": "notes/python-oop/index.html#overview",
    "title": "Python OOP",
    "section": "",
    "text": "Explanation of how OOP works in Python\nType annotations for Python variables, functions, and classes\nInheritance, interfaces\nEnums",
    "crumbs": [
      "Python OOP"
    ]
  },
  {
    "objectID": "notes/python-oop/index.html#oop-in-python",
    "href": "notes/python-oop/index.html#oop-in-python",
    "title": "Python OOP",
    "section": "2 OOP in Python",
    "text": "2 OOP in Python\nThese notes have been adapted from https://realpython.com/python3-object-oriented-programming/ with few modifications.\nObject-oriented programming is a programming paradigm that provides a means of structuring programs so that properties and behaviors are bundled into individual objects.\n\n2.1 How to define a class in python\nIn python, you define a class by using the class keyword followed by a name and a colon. Then you declare a constructor function that is always called __init__() to declare which attributes each instance of the class should have:\n\n\npython\n\nclass Person:\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n\nOnce the class is defined, you can create instances of it:\n\n\nPyInterpreter\n\n&gt;&gt;&gt; bob = Person(\"Bob\", 102)\n&gt;&gt;&gt; print(bob.name)\nBob\n&gt;&gt;&gt; print(bob.age)\n102\n\n\n\n2.2 Classes vs Instances\nA class is a blueprint. It doesn’t actually contain any data. The Person class specifies that a name and an age are necessary for defining a person, but it doesn’t contain the name or age of any specific person.\nWhile the class is the blueprint, an instance is an object that’s built from a class and contains real data. An instance of the Person class is not a blueprint anymore. It’s an actual person with a name, like Bob, who’s 102 years old.\n\n\n2.3 Instance Methods\nInstance methods are functions that you define inside a class and can only call on an instance of that class. Just like __init__(), an instance method always takes self as its first parameter.\nLet’s add a few methods to the Person class from the previous example:\n\n\nPython\n\nclass Person:\n\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n\n    # Instance method\n    def description(self):\n        return f\"{self.name} is {self.age} years old\"\n\n    # Another instance method\n    def speak(self, sound):\n        return f\"{self.name} says {sound}\"\n\nThis Person class has two instance methods:\n\ndescription returns a string displaying the name and age of the person.\nspeak has one parameter called sound and returns a string containing the person’s name and the sound that the person makes.\n\n\n\nPyInterpreter\n\n&gt;&gt;&gt; miles = Person(\"Miles\", 4)\n\n&gt;&gt;&gt; miles.description()\n'Miles is 4 years old'\n\n&gt;&gt;&gt; miles.speak(\"Woof Woof\")\n'Miles says Woof Woof'\n\n&gt;&gt;&gt; miles.speak(\"Bow Wow\")\n'Miles says Bow Wow'\n\n\n\n2.4 Dunder methods\nIn the editor window, change the name of the Dog class’s .description() method to .__str__():\n\n\nPython\n\nclass Dog:\n    # ...\n\n    def __str__(self):\n        return f\"{self.name} is {self.age} years old\"\n\nMethods like .__init__() and .__str__() are called dunder methods because they begin and end with double underscores. There are many dunder methods that you can use to customize classes in Python. Understanding dunder methods is an important part of mastering object-oriented programming in Python.\n\nNote: Check out When Should You Use .__repr__() vs .__str__() in Python? to learn more about .__str__() and its cousin .__repr__().\n\n\n\n2.5 How Do You Inherit From Another Class in Python?\nInheritance is the process by which one class takes on the attributes and methods of another. Newly formed classes are called child classes, and the classes that you derive child classes from are called parent classes.\nYou inherit from a parent class by creating a new class and putting the name of the parent class into parentheses:\n\n\nPython\n\nclass Parent:\n    hair_color = \"brown\"\n\n\nclass Child(Parent):\n    pass\n\nIn this minimal example, the child class Child inherits from the parent class Parent. Because child classes take on the attributes and methods of parent classes, Child.hair_color is also \"brown\" without your explicitly defining that.\nChild classes can override or extend the attributes and methods of parent classes. In other words, child classes inherit all of the parent’s attributes and methods but can also specify attributes and methods that are unique to themselves:\n\n\nPython\n\nclass Parent:\n    speaks = [\"English\"]\n\n\nclass Child(Parent):\n    def __init__(self):\n        super().__init__()\n        self.speaks.append(\"German\")",
    "crumbs": [
      "Python OOP"
    ]
  },
  {
    "objectID": "notes/python-oop/index.html#typings",
    "href": "notes/python-oop/index.html#typings",
    "title": "Python OOP",
    "section": "3 Typings",
    "text": "3 Typings\nThis section adapted from https://docs.python.org/3/library/typing.html and https://mypy.readthedocs.io/en/stable/cheat_sheet_py3.html\nThis section is a quick cheat sheet showing how to use type annotations for various common types in Python.\n\n\n\n\n\n\nNote\n\n\n\nThe Python runtime does not enforce function and variable type annotations. They can be used by third party tools such as type checkers, IDEs, linters, etc.\n\n\n\n3.1 Variables\nBasics:\n# This is how you declare the type of a variable\nage: int = 1\n\n# You don't need to initialize a variable to annotate it\na: int  # Ok (no value at runtime until assigned)\n\n# Doing so can be useful in conditional branches\nchild: bool\nif age &lt; 18:\n    child = True\nelse:\n    child = False\nUseful built-in types:\n# the common basic \"primitive\" types in Python\nx: int = 1\nx: float = 1.0\nx: bool = True\nx: str = \"test\"\nx: bytes = b\"test\"\n\n# Collections (Python 3.9+)\nx: list[int] = [1]\nx: set[int] = {6, 7}\n\n# For mappings, we need the types of both keys and values\nx: dict[str, float] = {\"field\": 2.0}  # Python 3.9+\n\n# For tuples of fixed size, we specify the types of all the elements\nx: tuple[int, str, float] = (3, \"yes\", 7.5)  # Python 3.9+\n\n# For tuples of variable size, we use one type and ellipsis\nx: tuple[int, ...] = (1, 2, 3)  # Python 3.9+\n\n\n3.2 Functions\nfrom typing import Callable, Iterator, Union, Optional\n\n\n# This is how you annotate a function definition\ndef stringify(num: int) -&gt; str:\n    return str(num)\n\n\n# And here's how you specify multiple arguments\ndef plus(num1: int, num2: int) -&gt; int:\n    return num1 + num2\n\n\n# If a function does not return a value, use None as the return type\n# Default value for an argument goes after the type annotation\ndef show(value: str, excitement: int = 10) -&gt; None:\n    print(value + \"!\" * excitement)\n\n\n# Note that arguments without a type are dynamically typed (treated as Any)\n# and that functions without any annotations are not checked\ndef untyped(x):\n    x.anything() + 1 + \"string\"  # no errors\n\n\n# You can of course split a function annotation over multiple lines\ndef send_email(\n    address: Union[str, list[str]],\n    sender: str,\n    cc: Optional[list[str]],\n    bcc: Optional[list[str]],\n    subject: str = \"\",\n    body: Optional[list[str]] = None,\n) -&gt; bool: ...\n\n\n3.3 Classes\nHere’s an example of typings with a custom defined class:\nclass BankAccount:\n    # The \"__init__\" constructor method doesn't return anything, so it gets return\n    # type \"None\" just like any other method that doesn't return anything\n    def __init__(self, account_name: str, initial_balance: int = 0) -&gt; None:\n        # mypy will infer the correct types for these instance variables\n        # based on the types of the parameters.\n        self.account_name = account_name\n        self.balance = initial_balance\n\n    # For instance methods, omit type for \"self\"\n    def deposit(self, amount: int) -&gt; None:\n        self.balance += amount\n\n    def withdraw(self, amount: int) -&gt; None:\n        self.balance -= amount\n\n\n# User-defined classes are valid as types in annotations\naccount: BankAccount = BankAccount(\"Alice\", 400)\n\n\ndef transfer(src: BankAccount, dst: BankAccount, amount: int) -&gt; None:\n    src.withdraw(amount)\n    dst.deposit(amount)\n\n\n# Functions that accept BankAccount also accept any subclass of BankAccount!\nclass AuditedBankAccount(BankAccount):\n    # You can optionally declare instance variables in the class body\n    audit_log: list[str]\n\n    def __init__(self, account_name: str, initial_balance: int = 0) -&gt; None:\n        super().__init__(account_name, initial_balance)\n        self.audit_log: list[str] = []\n\n    def deposit(self, amount: int) -&gt; None:\n        self.audit_log.append(f\"Deposited {amount}\")\n        self.balance += amount\n\n    def withdraw(self, amount: int) -&gt; None:\n        self.audit_log.append(f\"Withdrew {amount}\")\n        self.balance -= amount\n\n\naudited = AuditedBankAccount(\"Bob\", 300)\ntransfer(audited, account, 100)  # type checks!",
    "crumbs": [
      "Python OOP"
    ]
  },
  {
    "objectID": "notes/async-python/index.html",
    "href": "notes/async-python/index.html",
    "title": "Asynchronous progamming in Python",
    "section": "",
    "text": "The library asyncio enables code to be run concurrently: Tasks share the execution thread while appearing to be executing in parallel.\n\nAsyncio API:\n\nCoroutines and Tasks (good intro to asyncio).\n\nIn-depth guide:\n\nAsync IO in Python: A Complete Walkthrough by realpython.com.\n\nVideo Summary:\n\nHow To Easily Do Asynchronous Programming With Asyncio by ArjanCodes\n\n\n\n\nIt’s important to note that concurrent code does not necessarily mean parallel.\n\nConcurrent does necessarily mean parallel execution. By Ten thousand meters\n\nIn Python, the only way to run parallel code is to use multiple CPU cores by running multiple Python interpreters that interact. See multiprocessing module.\n\nAsyncio is used to run multiple tasks or multiple threads concurrently but never at the same time (in parallel).\n\n\n\nIn typical programming, most tasks and processes are synchronous, meaning, they run one at the time, always waiting for the previous taks to finish\nSynchronous is disadvantageous when there are many tasks that need to wait for external input or output before it can proceed. For example:\n\nThe code was asked to sleep.\nWaiting for a HTTP response.\nWaiting for a Database call.\nWaiting to write to a file.\n\n\nSynchronous code execution by RealPython.com\nAsynchronous execution enables the execution of the next task while waiting for the external input/output.\n\nAsynchronous code execution by RealPython.com\n\nNote that the amount CPU processing (blue area in the diagrams above) is the same in both sync and async executions.\nHowever, the async execution spends less time waiting idle.\n\n\n\n\nasyncio uses the concept of tasks to manage and share the CPU processing time.\n\nEach task must explicitly yield (or pass) execution time back to the main loop so that the next task can start/continue.\n\n\nTasks yielding execution time with the await keyword. By Adafruit.com\nIn the image above, Task 1 (running function f()) yields execution time to Task 2 (running function g()) by using the await keyboard and vice-versa.\n\n\n\nIn Asyncio, there are 3 types of awaitable objects:\n\nCoroutines\nTasks\nFutures\n\n\n\nCoroutines are functions that are awaitable.\nThey must be declared with the async keyword\nasync def countdown(task_name, seconds):\n    for i in range(seconds, 0, -1):\n        print(task_name, i)\n        await asyncio.sleep(1)\n    print(\"done!\")\n\n\nasync def main():\n    await countdown(\"simple count\", 4)\n\n\nasyncio.run(main())\n\n\"\"\" Output\nsimple count 4\nsimple count 3\nsimple count 2\nsimple count 1\ndone!\n\"\"\"\nIn the example above, countdown(), asyncio.sleep() and main() are coroutines.\ncountdown() and asyncio.sleep() were called with await , however, main() was called with asyncio.run()\n\nIn asyncio, all awaitable objects must run inside an event loop.\n\nThe method asyncio.run() creates a “top-level” event loop\n\n\n\n\n\nTasks are used to schedule coroutines concurrently.\nOnce a task is scheduled, it will run as soon as the event loop is available.\n\nTasks are scheduled (wrapped) with asyncio.create_task()\nIn order to run a task to completion, it must be awaited.\n\nasync def main():\n    # Scheduling and starting tasks\n    taskA = asyncio.create_task(countdown(\"task A\", 3))\n    taskB = asyncio.create_task(countdown(\"task B\", 2))\n\n    # Doing something else while they run\n    await asyncio.sleep(1)\n    print(\"Everything, everywhere, all at once\")\n\n    # Awaiting for tasks to complete before ending main()\n    # Try running this code without awaiting\n    await taskA\n    await taskB\n\n\nasyncio.run(main())\n\n\"\"\" Output\ntask A 3\ntask B 2\nEverything, everywhere, all at once\ntask A 2\ntask B 1\ntask A 1\ndone!\ndone!\n\"\"\"\nInstead of creating Tasks and awaiting them individually, it’s possible to run multiple coroutines concurrently and wait for all of them:\nasync def main():\n    # Running and awaiting multiple coroutines\n\n    await asyncio.gather(countdown(\"task A\", 4), countdown(\"task B\", 3))\n\n    await asyncio.sleep(1)\n    print(\"Everything, everywhere, all at once\")\n\n\nasyncio.run(main())\n\"\"\"\ntask A 4\ntask B 3\ntask A 3\ntask B 2\ntask A 2\ntask B 1\ntask A 1\ndone!\ndone!\nEverything, everywhere, all at once\n\"\"\"\n\n\n\nA Future in Asyncio is the equivalent of a Promise in JavaScript.\n\nIt is rare that developers need to use a Future directly.\n\n\n\n\n\nThe following are excellent resources for learning more about Asyncio:\n\nAsync IO in Python: A Complete Walkthrough by RealPython.com",
    "crumbs": [
      "Asynchronous progamming in Python"
    ]
  },
  {
    "objectID": "notes/async-python/index.html#asynch-with-asyncio",
    "href": "notes/async-python/index.html#asynch-with-asyncio",
    "title": "Asynchronous progamming in Python",
    "section": "",
    "text": "The library asyncio enables code to be run concurrently: Tasks share the execution thread while appearing to be executing in parallel.\n\nAsyncio API:\n\nCoroutines and Tasks (good intro to asyncio).\n\nIn-depth guide:\n\nAsync IO in Python: A Complete Walkthrough by realpython.com.\n\nVideo Summary:\n\nHow To Easily Do Asynchronous Programming With Asyncio by ArjanCodes\n\n\n\n\nIt’s important to note that concurrent code does not necessarily mean parallel.\n\nConcurrent does necessarily mean parallel execution. By Ten thousand meters\n\nIn Python, the only way to run parallel code is to use multiple CPU cores by running multiple Python interpreters that interact. See multiprocessing module.\n\nAsyncio is used to run multiple tasks or multiple threads concurrently but never at the same time (in parallel).\n\n\n\nIn typical programming, most tasks and processes are synchronous, meaning, they run one at the time, always waiting for the previous taks to finish\nSynchronous is disadvantageous when there are many tasks that need to wait for external input or output before it can proceed. For example:\n\nThe code was asked to sleep.\nWaiting for a HTTP response.\nWaiting for a Database call.\nWaiting to write to a file.\n\n\nSynchronous code execution by RealPython.com\nAsynchronous execution enables the execution of the next task while waiting for the external input/output.\n\nAsynchronous code execution by RealPython.com\n\nNote that the amount CPU processing (blue area in the diagrams above) is the same in both sync and async executions.\nHowever, the async execution spends less time waiting idle.\n\n\n\n\nasyncio uses the concept of tasks to manage and share the CPU processing time.\n\nEach task must explicitly yield (or pass) execution time back to the main loop so that the next task can start/continue.\n\n\nTasks yielding execution time with the await keyword. By Adafruit.com\nIn the image above, Task 1 (running function f()) yields execution time to Task 2 (running function g()) by using the await keyboard and vice-versa.\n\n\n\nIn Asyncio, there are 3 types of awaitable objects:\n\nCoroutines\nTasks\nFutures\n\n\n\nCoroutines are functions that are awaitable.\nThey must be declared with the async keyword\nasync def countdown(task_name, seconds):\n    for i in range(seconds, 0, -1):\n        print(task_name, i)\n        await asyncio.sleep(1)\n    print(\"done!\")\n\n\nasync def main():\n    await countdown(\"simple count\", 4)\n\n\nasyncio.run(main())\n\n\"\"\" Output\nsimple count 4\nsimple count 3\nsimple count 2\nsimple count 1\ndone!\n\"\"\"\nIn the example above, countdown(), asyncio.sleep() and main() are coroutines.\ncountdown() and asyncio.sleep() were called with await , however, main() was called with asyncio.run()\n\nIn asyncio, all awaitable objects must run inside an event loop.\n\nThe method asyncio.run() creates a “top-level” event loop\n\n\n\n\n\nTasks are used to schedule coroutines concurrently.\nOnce a task is scheduled, it will run as soon as the event loop is available.\n\nTasks are scheduled (wrapped) with asyncio.create_task()\nIn order to run a task to completion, it must be awaited.\n\nasync def main():\n    # Scheduling and starting tasks\n    taskA = asyncio.create_task(countdown(\"task A\", 3))\n    taskB = asyncio.create_task(countdown(\"task B\", 2))\n\n    # Doing something else while they run\n    await asyncio.sleep(1)\n    print(\"Everything, everywhere, all at once\")\n\n    # Awaiting for tasks to complete before ending main()\n    # Try running this code without awaiting\n    await taskA\n    await taskB\n\n\nasyncio.run(main())\n\n\"\"\" Output\ntask A 3\ntask B 2\nEverything, everywhere, all at once\ntask A 2\ntask B 1\ntask A 1\ndone!\ndone!\n\"\"\"\nInstead of creating Tasks and awaiting them individually, it’s possible to run multiple coroutines concurrently and wait for all of them:\nasync def main():\n    # Running and awaiting multiple coroutines\n\n    await asyncio.gather(countdown(\"task A\", 4), countdown(\"task B\", 3))\n\n    await asyncio.sleep(1)\n    print(\"Everything, everywhere, all at once\")\n\n\nasyncio.run(main())\n\"\"\"\ntask A 4\ntask B 3\ntask A 3\ntask B 2\ntask A 2\ntask B 1\ntask A 1\ndone!\ndone!\nEverything, everywhere, all at once\n\"\"\"\n\n\n\nA Future in Asyncio is the equivalent of a Promise in JavaScript.\n\nIt is rare that developers need to use a Future directly.\n\n\n\n\n\nThe following are excellent resources for learning more about Asyncio:\n\nAsync IO in Python: A Complete Walkthrough by RealPython.com",
    "crumbs": [
      "Asynchronous progamming in Python"
    ]
  },
  {
    "objectID": "notes/python-package-management/index.html",
    "href": "notes/python-package-management/index.html",
    "title": "Python package management",
    "section": "",
    "text": "This lecture looks at the ways to acquire use and install external python packages.\n\nInstalling system python packages with apt on debian\nCreating virtual environments with venv\nInstalling packages locally with pip",
    "crumbs": [
      "Python package management"
    ]
  },
  {
    "objectID": "notes/python-package-management/index.html#overview",
    "href": "notes/python-package-management/index.html#overview",
    "title": "Python package management",
    "section": "",
    "text": "This lecture looks at the ways to acquire use and install external python packages.\n\nInstalling system python packages with apt on debian\nCreating virtual environments with venv\nInstalling packages locally with pip",
    "crumbs": [
      "Python package management"
    ]
  },
  {
    "objectID": "notes/python-package-management/index.html#python-on-raspberry-pi",
    "href": "notes/python-package-management/index.html#python-on-raspberry-pi",
    "title": "Python package management",
    "section": "2 Python on Raspberry Pi",
    "text": "2 Python on Raspberry Pi\nNote: most of these notes were adapted directly from the Raspberry Pi docs: Python on Raspberry Pi 1\n1 Raspberry Pi documentation is copyright © 2012-2024 Raspberry Pi Ltd and is licensed under a Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA) licence. Some content originates from the eLinux wiki, and is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported licence.Python 3 is installed by default on Raspberry Pi OS, and is used for many important functions. Interfering with the system Python installation can cause problems for your operating system, so it’s important that if you install third-party Python libraries, you use the correct package-management tools.\nThere are two routes to installing libraries into the default python distribution. You can use apt and install pre-configured system packages, or you can use pip to install packages which are not distributed as part of Raspberry Pi OS.",
    "crumbs": [
      "Python package management"
    ]
  },
  {
    "objectID": "notes/python-package-management/index.html#installing-python-packages-using-apt",
    "href": "notes/python-package-management/index.html#installing-python-packages-using-apt",
    "title": "Python package management",
    "section": "3 Installing Python packages using apt",
    "text": "3 Installing Python packages using apt\n\n\n\n\n\n\nImportant\n\n\n\nInstalling packages using apt is the preferred method for installing Python libraries under Raspberry Pi OS.\n\n\nPackages installed via apt are tested, are usually pre-compiled so they install faster, and are designed for Raspberry Pi OS. They won’t break your system. Installing via this route also means that all required dependencies are also installed, and a log of installation is maintained by the OS so installation can be easily rolled back (libraries can be uninstalled) if needed.\nYou can see a complete list of Python apps maintained by Debian developers on the stable repositories here. Many, but not all, popular open-source Python packages are maintained for Debian by professionals and hobbyists alike.\nTo see an example: you may find yourself wanting to install the Python 3 library to support the Raspberry Pi BuildHAT component. To install this using apt, you would:\n$ sudo apt install python3-build-hat\nIf you want to install a Python library called \"foobar\" you can useapt search foobar to find the exact package name – partial search matches are supported, so you don’t need to know the whole name in advance. Very useful!\nUsing apt makes installing larger packages, like numpy (which has many native dependencies including a Fortran compiler), much simpler and more predictable than installing individual packages using Python’s own package-management system.\n\n\n\n\n\n\nNote\n\n\n\nIn apt, Python packages have a consistent naming scheme: you’ll find that the a given package “foobar” is going to be called python-foobar or python3-foobar in the apt repositories – this helps distinguish python packages from other packages on a debian system.",
    "crumbs": [
      "Python package management"
    ]
  },
  {
    "objectID": "notes/python-package-management/index.html#installing-python-packages-using-pip",
    "href": "notes/python-package-management/index.html#installing-python-packages-using-pip",
    "title": "Python package management",
    "section": "4 Installing python packages using Pip",
    "text": "4 Installing python packages using Pip\nThe goal of operating systems like Debian is to be stable, that is, unlikely to crash due to a poorly tested/implemented program. That does come at a cost of not always offering the latest versions of packages, nor having a complelely comprehensive set of the bleeding edge packages that are available.\nMany developer depend on packages that are under active development. And, there are many packages that there are no Debian maintainers for. For cases like these (and many other cases, as we’ll see) it is useful for programming languages like Python to have an independent packaging manager.\nFor python, the most commonly used package manager is pip.\n\nPip installs packages from the Python Package Index repository (pypi.org) rather than the Debian or any other package repositories.\n\nEven though Raspberry Pi OS comes with pip installed, we can’t just use it right away, as we we’ll see in the next section.\n\n4.1 About Python virtual environments\nIn previous versions of the operating system, it was possible to install libraries directly, system-wide, using the package installer for Python, pip. You’ll find the following sort of command in many tutorials online – including in our Labs.\n$ pip install buildhat\nIn newer versions of Raspberry Pi OS, and most other operating systems, this is disallowed. If you try and install a Python package system-wide you’ll receive an error similar to this:\n$ pip install buildhat\nerror: externally-managed-environment\n\n× This environment is externally managed\n╰─&gt; To install Python packages system-wide, try apt install\n  python3-xyz, where xyz is the package you are trying to\n  install.\n\n  If you wish to install a non-Debian-packaged Python package,\n  create a virtual environment using python3 -m venv path/to/venv.\n  Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make\n  sure you have python3-full installed.\n\n  For more information visit http://rptl.io/venv\n\nnote: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\nhint: See PEP 668 for the detailed specification.\nThis error is generated because you’re trying to install a third-party package into the system Python. A long-standing practical problem for Python users has been conflicts between OS package managers like apt and Python-specific package management tools like pip. These conflicts include both Python-level API incompatibilities and conflicts over file ownership.\nTherefore from Debian Bookworm onwards, packages installed via pip must be installed into a Python virtual environment using venv. A virtual environment is a container where you can safely install third-party modules so they won’t interfere with, or break, your system Python.\n\n\n4.2 Using pip with virtual environments\n\n\n\n\n\n\nNote\n\n\n\nAny packages that cannot be installed using apt should instead installed onto a Python Virtual Environment using venv and pip.\n\n\nTo use a virtual environment you will need to create a container to store the environment. There are several ways you can do this depending on how you want to work with Python.\n\n\n4.3 Using a separate environment for each project\nOne way you can proceed is to create a new virtual environment for each Python project you make. Here, you’ll create a directory to hold your own code along with a virtual environment directory:\n$ mkdir my_project\n$ cd my_project\n$ python -m venv env\nIf you now look inside the my_project directory you’ll see a directory called env.\n$ ls -la\ntotal 12\ndrwxr-xr-x  3 pi pi 4096 Oct  3 14:34 .\ndrwx------ 20 pi pi 4096 Oct  3 14:34 ..\ndrwxr-xr-x  5 pi pi 4096 Oct  3 14:34 env\n$\n\n\n\n\n\n\nImportant\n\n\n\nIf you want to inherit the currently installed packages from the system Python, you should create your virtual environment using python -m venv --system-site-packages env.\nThis will include the important Raspberry PI python modules that come with RaspberryPi OS in your venv.\n\n\nInside this directory is a full Python distribution. How do we use it?\nA nice feature of venvs is that they must be activated to be used. This allows you to maintained several different venvs all over your machines to maintain many different projects with different versions of python.\nTo activate your virtual environment and make that version of Python the one you’re currently using, you should type:\n$ source env/bin/activate\n(env) $\nYou’ll see that your prompt is now prepended with (env) to indicate that you’re no longer using the system Python. Instead, you’re using the version of Python contained inside your virtual environment. Any changes you make here won’t cause problems for your system Python; nor will any new modules you install into your environment.\n(env) $ which python\n/home/username/my_project/env/bin/python\nIf you install a third-party package, it’ll install into the Python distribution in your virtual environment:\n(env) $ pip install buildhat\nLooking in indexes: https://pypi.org/simple, https://www.piwheels.org/simple\nCollecting buildhat\n  Downloading https://www.piwheels.org/simple/buildhat/buildhat-0.5.12-py3-none-any.whl (57 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.8/57.8 kB 2.8 MB/s eta 0:00:00\nCollecting gpiozero\n  Downloading https://www.piwheels.org/simple/gpiozero/gpiozero-2.0-py3-none-any.whl (150 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 150.5/150.5 kB 6.9 MB/s eta 0:00:00\nCollecting pyserial\n  Downloading https://www.piwheels.org/simple/pyserial/pyserial-3.5-py2.py3-none-any.whl (90 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 90.6/90.6 kB 7.5 MB/s eta 0:00:00\nCollecting colorzero\n  Downloading https://www.piwheels.org/simple/colorzero/colorzero-2.0-py2.py3-none-any.whl (26 kB)\nRequirement already satisfied: setuptools in ./env/lib/python3.11/site-packages (from colorzero-&gt;gpiozero-&gt;buildhat) (66.1.1)\nInstalling collected packages: pyserial, colorzero, gpiozero, buildhat\nSuccessfully installed buildhat-0.5.12 colorzero-2.0 gpiozero-2.0 pyserial-3.5\n(env) $\nNow, if you pip list, you’ll see that your current version of Python includes your new modules.\n(env) $ pip list\nPackage    Version\n---------- -------\nbuildhat   0.5.12\ncolorzero  2.0\ngpiozero   2.0\npip        23.0.1\npyserial   3.5\nsetuptools 66.1.1\nAfter writing your code, you can run it from the command line inside the virtual environment as you’d expect, by invoking Python as usual.\n(env) $ ls -la\ntotal 12\ndrwxr-xr-x  3 pi pi 4096 Oct  3 14:34 .\ndrwx------ 20 pi pi 4096 Oct  3 14:34 ..\ndrwxr-xr-x  5 pi pi 4096 Oct  3 14:34 env\n-rw-r--r--  1 pi pi    0 Oct  3 14:45 my_code.py\n(env) $ python my_code.py\nHello World!\n(env) $\nYou can leave your virtual environment and return to using the system Python by typing:\n(env) $ deactivate\n$\n…​and demonstrate to yourself you’ve done so by checking the installed packages using pip list.\n\n\n4.4 Including system packages in your venv\nThis section has been adapted directly from realPython.com: Virtual Environments: Include the System Site-Packages. The whole article is worth skimming for reference.\nIn this class, we will often use the RaspberryPi python packages that we install system-wide (gpio, RPi, etc), and we may want to access them in the same environment where we create local virtual environments.\nThat is, while you still want to keep your projects in separate environments, you can save the effort of re-installing gpio or RPi into each of these projects.\nYou can access all modules you’ve installed to your base Python’s site-packages directory by adding the --system-site-packages flag when creating your virtual environment.\n\n\n\n\n\n\nNote\n\n\n\nIf you install any additional external packages, then Python will put them into the site-packages directory of your virtual environment. You only get read access to the system site-packages directory.\n\n\nCreate a new virtual environment while passing this argument. You’ll see that in addition to your local site-packages directory, the path to your base Python’s site-packages directory will stick around in sys.path.\nTo test this, you can create and activate a new virtual environment using the --system-site-packages argument:\n$ python3 -m venv env --system-site-packages\n$ source venv/bin/activate\n(venv) $\nOnce again, you’ve created a new virtual environment named venv, but this time you passed the --system-site-packages argument. Adding this optional argument resulted in a different setting in your venv/pyvenv.cfg file:\n$ cat env/pyvenv.cfg\nhome = /usr/bin\ninclude-system-site-packages = true\nversion = 3.11\nBy default, the include-system-site-packages is set to false.\n\n\n\n\n\n\nNote\n\n\n\nIt is common to forget to include the --system-site-packages flag when creating a venv. If you create a virtual environment and forget to include system site packages, you can include them again by changing include-system-site-packages in your environment env/pyvenv.cfg to true in your venv.\n\n\nThis change means that you’ll see an additional entry to sys.path, which allows the Python interpreter in your virtual environment to also access the system site-packages directory. Make sure your virtual environment is active, then start the Python interpreter to check the path variables:\n(env) $ python\n&gt;&gt;&gt; import sys\n&gt;&gt;&gt; from pprint import pp\n&gt;&gt;&gt; pp(sys.path)\n['',\n '/usr/local/lib/python311.zip',\n  'etc.'\n]\n\n\n4.5 Using packages that require root permissions\nThe venv pattern is very useful for maintaining packages that do not require root permissions. This is a feature, not a bug – that way, you can have many developers independently install packages on one machine without needing to give them root permissions.\nSometimes (and particularly, in this couse) the packages we install require root permissions to work. Consider the packages that edit LED /sys/ files, like seeed-python-reterminal we install in Lab 2. We will run into an issue if we try to use these packages as normal users:\n(env) user@hostname:~/lab2/python $ python ./leds.py\n# ... output redacted\n&lt;Permissions error&gt;\nIt is unfortunately not fixable by simply using sudo:\n(env) user@hostname:~/lab2/python $ sudo python ./leds.py\n# ... output redacted\n&lt;Module seeed-python-terminal not included/available&gt;\nWhy isn’t the package available? Well, sudo runs python as the root user. The root user does not have your venv of python in its path. Try the commands below on your system to see if you understand what I mean.\n\nnormal usersudo user\n\n\n(env)jyourusername@yourhostname:~ $ which python\n# &lt;double check that the python executable comes from your venv&gt;\n\n\n(env)jyourusername@yourhostname:~ $ sudo su\n(env)root@yourhostname:~ # which python\n# &lt;you should see that the python executable is in a different place!, even though we still have \"env\"&gt;\n\n\n\nHow to resolve this problem? It is similar to the sudo echo &gt; file problem we saw with Bash. We need to make sure sudo is applying to the correct python executable. There are a few ways we can do this, all of which will look something like this:\n(env)jyourusername@yourhostname:~ $ sudo /absolute/path/to/venv/python ./leds.py\n(env)jyourusername@yourhostname:~ $ sudo ./env/relative/path/to/venv/python ./leds.py\n(env)jyourusername@yourhostname:~ $ sudo $(shell command for finding where python is installed) ./leds.py # use command substitution\nTry a few of these options out (will not work if you copy paste! Make sure you understand the commands)\n\n\n4.6 Using a separate environment for each user\nAn alternative method to creating a virtual environment for each of your Python projects is to create a single virtual environment for your user account, and then activate that environment before running any of your Python code. This approach may be preferred if you commonly install the same set of modules for each project, and don’t want to have to bother creating individual Python environments for each project, essentially just duplicating your environment.\n$ python -m venv ~/.env\n$ source ~/.env/bin/activate\n(.env) $\nYou can check again that you’re in a separate environment by using pip list:\n(.env) $ pip list\nPackage    Version\n---------- -------\npip        23.0.1\nsetuptools 66.1.1\n…​and leave it using deactivate.\n(.env) $ deactivate\n$\n\n\n4.7 Check library location\nCheck the location of where the module was installed with pip show &lt;module-name&gt;:\n(env) pi@raspberrypi:~ $ pip show seeed-python-reterminal\n\nName: seeed-python-reterminal\nVersion: 0.2\nSummary: seeed-python-reterminal\nHome-page: https://github.com/Seeed-Studio/Seeed_Python_ReTerminal\nAuthor: Takashi Matsuoka (matsujirushi)\nAuthor-email: matsujirushi@live.jp\nLicense: MIT License\nLocation: /usr/local/lib/python3.7/dist-packages      # Library location\nRequires: evdev\nRequired-by:\nThe library is installed at: Location: /usr/local/lib/python3.7/dist-packages",
    "crumbs": [
      "Python package management"
    ]
  },
  {
    "objectID": "notes/python-package-management/index.html#troubleshooting",
    "href": "notes/python-package-management/index.html#troubleshooting",
    "title": "Python package management",
    "section": "5 Troubleshooting",
    "text": "5 Troubleshooting\n\n5.1 Check python’s library paths\nSimilarly to Linux, there are environmental variables that determine where python will look for installed modules/libraries.\nSee the paths where python is looking for libraries with sys.path. They should look something like:\n\nsystemvenv\n\n\npi@raspberrypi:~ $ python\n\n&gt;&gt;&gt; import sys      # Exposes configuration used by the python interpreter.\n&gt;&gt;&gt; sys.path        # Lists all paths where interpreter looks for modules.\n['', '/usr/lib/python311.zip', '/usr/lib/python3.11', '/usr/lib/python3.11/lib-dynload', '/usr/local/lib/python3.11/dist-packages', '/usr/lib/python3/dist-packages', '/usr/lib/python3.11/dist-packages']\n\n\n(env) pi@raspberrypi:~ $ python\n\n&gt;&gt;&gt; import sys      # Exposes configuration used by the python interpreter.\n&gt;&gt;&gt; sys.path        # Lists all paths where interpreter looks for modules.\n['', '/usr/lib/python311.zip', '/usr/lib/python3.11', '/usr/lib/python3.11/lib-dynload', '/home/pi/lab2/python/env/lib/python3.11/site-packages', '/usr/local/lib/python3.11/dist-packages', '/usr/lib/python3/dist-packages', '/usr/lib/python3.11/dist-packages']\n\n\n\nIf the output of pip show seeed-python-reterminal is not in this list, python will not find it when you import it.\n\nNote: the python shell above was started as the user pi. If you start python as root, sys.path might have different paths.\n\nNote that the user pi does not have permission to add content inside /usr.\nThus, all pip installations as the regular user go into /home/pi/.local/lib/python3.11/site-packages.",
    "crumbs": [
      "Python package management"
    ]
  },
  {
    "objectID": "notes/azure-cli-cheatsheet/index.html",
    "href": "notes/azure-cli-cheatsheet/index.html",
    "title": "Azure CLI Cheatsheet",
    "section": "",
    "text": "Here is a list of commands we’ll commonly use with Azure CLI.\n\n\n\nAzure CLI is installed: see Azure CLI Installation Instructions\nAzure IoT extension enabled: az extension add --name azure-iot\n\nYou can check that both requirements are met with the command az --version.",
    "crumbs": [
      "Azure CLI Cheatsheet"
    ]
  },
  {
    "objectID": "notes/azure-cli-cheatsheet/index.html#azure-cli-cheatsheet",
    "href": "notes/azure-cli-cheatsheet/index.html#azure-cli-cheatsheet",
    "title": "Azure CLI Cheatsheet",
    "section": "",
    "text": "Here is a list of commands we’ll commonly use with Azure CLI.\n\n\n\nAzure CLI is installed: see Azure CLI Installation Instructions\nAzure IoT extension enabled: az extension add --name azure-iot\n\nYou can check that both requirements are met with the command az --version.",
    "crumbs": [
      "Azure CLI Cheatsheet"
    ]
  },
  {
    "objectID": "notes/azure-cli-cheatsheet/index.html#create-a-device",
    "href": "notes/azure-cli-cheatsheet/index.html#create-a-device",
    "title": "Azure CLI Cheatsheet",
    "section": "2 Create a Device",
    "text": "2 Create a Device\nTo register a device on IoTHub in Azure CLI, use the az iot hub device-identity create command:\n\n\n\n\n\n\n\nNote\n\n\n\nRecall that Bash environment variables allow you to store strings in a shell session. You can save environment variables in a project .env file that you can source later.\n\n# It's convenient to store these names in environment variables for later re-use\n$ export IOT_DEVICE_NAME=simDevice\n$ export IOTHUB_NAME=YourIoTHubName\n\n# Create a device with the name \"simDevice\"\n$ az iot hub device-identity create -d ${IOT_DEVICE_NAME} -n ${IOTHUB_NAME}",
    "crumbs": [
      "Azure CLI Cheatsheet"
    ]
  },
  {
    "objectID": "notes/azure-cli-cheatsheet/index.html#simulate-a-device",
    "href": "notes/azure-cli-cheatsheet/index.html#simulate-a-device",
    "title": "Azure CLI Cheatsheet",
    "section": "3 Simulate a Device",
    "text": "3 Simulate a Device\nYou can “simulate” an IoT Device (that is, create a temporary stream that sends IoT messages similar to real IoT Devices) using the commands below:\n# Begin device simulation for an existing device\n$ az iot device simulate -d ${IOT_DEVICE_NAME} -n ${IOTHUB_NAME}\n\n\n\n\n\n\n\nNote\n\n\n\nThe az iot device simulate command runs for a few minutes unless interrupted by the user. Use Bash Process Control keyboard shortcuts to suspend, resume, scroll through the output, etc.!\n\nBy default, this command will:\n\nSend D2C messages with a payload of Ping from Az CLI IoT Extension\nAutomatically receive and acknowledge C2D messages.\n\nSee az iot device simulate documentation for details.",
    "crumbs": [
      "Azure CLI Cheatsheet"
    ]
  },
  {
    "objectID": "notes/azure-cli-cheatsheet/index.html#monitor-messages",
    "href": "notes/azure-cli-cheatsheet/index.html#monitor-messages",
    "title": "Azure CLI Cheatsheet",
    "section": "4 Monitor Messages",
    "text": "4 Monitor Messages\nYou can monitor all actions in an Azure IOT Hub using the az iot hub monitor-events command:\n\n\n\n\n\n\n\nNote\n\n\n\nBash Process Control keyboard shortcuts and Bash redirection and pipes are useful for pausing/continuing/scrolling/parsing the az iot hub monitor-events command.\n\n# List all message details for all devices.\n$ az iot hub monitor-events --output table -p all -n ${IOTHUB_NAME}\n\n# List message details for a specific device:\n$ az iot hub monitor-events --output table --device-id ${IOT_DEVICE_NAME} --hub-name ${IOTHUB_NAME}\nSee az iot hub monitor-events documentation for details.",
    "crumbs": [
      "Azure CLI Cheatsheet"
    ]
  },
  {
    "objectID": "notes/azure-cli-cheatsheet/index.html#device-commands",
    "href": "notes/azure-cli-cheatsheet/index.html#device-commands",
    "title": "Azure CLI Cheatsheet",
    "section": "5 Device Commands",
    "text": "5 Device Commands\n\n5.1 Send C2D Message to Simulated Device\nThe az iot device c2d-message send command sends a cloud-to-device message from your IoT hub to an IoT device. The message can include a data string, representing a payload for the command, and a set of key-value pairs, representing command properties.\naz iot device c2d-message send -d ${IOT_DEVICE_NAME} --data \"Hello World\" --props \"key0=value0;key1=value1\" -n ${IOTHUB_NAME}\nIf simDevice is the default simulated device as seen in the Simulate a Device section, then you should see the following log printed when a c2d-message is sent to it:\nC2D Message Handler [Received C2D message]:\n{ 'Message Properties': { 'content_encoding': 'utf-8',\n                          'key0': 'value0',\n                          'key1': 'value1',\n                          'message_id': '1bbb2dd3-7b24-4e62-ab3a-79a8b13cb1fe'},\n  'Payload': 'Hello World',\n  'Topic': '/devices/simDevice/messages/devicebound'}\nNote that the keys, values, and payload are set by the --props and --data arguments of the c2d-message command.\nSee az iot device c2d-message send documentation for more details.\n\n\n5.2 Invoke Direct Method on Device\nThe az iot hub invoke-device-method command calls a method (specified by name) on a chosen device, and returns a payload.\naz iot hub invoke-device-method --mn ${METHOD_NAME} -d ${IOT_DEVICE_NAME} -n ${IOTHUB_NAME}\nIf simDevice is the default simulated device as seen in the Simulate a Device section, then you should see the following log printed when the SetTelemetryInterval direct method is invoked:\n\nDevice consoleService console\n\n\nMethod Request Handler [Received direct method invocation request]:\n{ 'Device Id': 'simDevice',\n  'Method Request Id': '1',\n  'Method Request Name': 'SetTelemetryInterval',\n  'Method Request Payload': {}}\n\n\n{\n  \"payload\": {\n    \"methodName\": \"SetTelemetryInterval\",\n    \"methodRequestId\": \"1\",\n    \"methodRequestPayload\": {}\n  },\n  \"status\": 200\n}\n\n\n\n\n\n5.3 Update Device Twin Properties\naz iot hub device-twin update -d ${IOT_DEVICE_NAME} --desired '{\"conditions\":{\"temperature\":{\"warning\":98, \"critical\":107}}}' -n ${IOTHUB_NAME}\n\n\n5.4 Get Device Twin Properties\naz iot hub device-twin show -d ${IOT_DEVICE_NAME} --query properties.reported -n ${IOTHUB_NAME}",
    "crumbs": [
      "Azure CLI Cheatsheet"
    ]
  },
  {
    "objectID": "notes/azure-cli-cheatsheet/index.html#connection-strings",
    "href": "notes/azure-cli-cheatsheet/index.html#connection-strings",
    "title": "Azure CLI Cheatsheet",
    "section": "6 Connection Strings",
    "text": "6 Connection Strings\nConnection Strings provide authentication for most Azure SDK commands, both in Python and C#.\nHere are the main Azure CLI methods for getting Connection Strings:\n\n\n\n\n\n\n\nNote\n\n\n\nBash redirection and pipes are useful tools for saving the output of these connection string commands as environment variables. You can save environment variables in a project .env file that you can source later.\n\n# Device string: Recommended string for a single device\naz iot hub device-identity connection-string show \\\n  --device-id ${IOT_DEVICE_NAME}\n\n# Service string: Recommended string for external applications\naz iot hub connection-string show \\\n  --policy-name service \\\n  --hub-name ${IOTHUB_NAME}\n\n# EventHub compatible string: needed for certain applications\naz iot hub connection-string show \\\n  -n ${IOTHUB_NAME} \\\n  --default-eventhub\n\n# Owner string: NOT recommended, provides access to entire resource group.\naz iot hub connection-string  show --hub-name ${IOTHUB_NAME}\nSee detailed documentation at learn.microsoft.com:\n\naz iot hub connection-string\naz iot hub device-identity connection-string show",
    "crumbs": [
      "Azure CLI Cheatsheet"
    ]
  },
  {
    "objectID": "notes/reterminal-devices/index.html",
    "href": "notes/reterminal-devices/index.html",
    "title": "reTerminal built-in devices",
    "section": "",
    "text": "A diagram showing how the electrostatic field changes caused by touch is processed by devices like the reTerminal. Image source",
    "crumbs": [
      "reTerminal built-in devices"
    ]
  },
  {
    "objectID": "notes/reterminal-devices/index.html#programmable-interfaces",
    "href": "notes/reterminal-devices/index.html#programmable-interfaces",
    "title": "reTerminal built-in devices",
    "section": "1 Programmable interfaces",
    "text": "1 Programmable interfaces\nThis section is based on the official documentation for the reTerminal: Hardware and Interfaces Usage\nAll programmable data can be passed in file streams that can be read and/or written to.\nFor example, keyboard inputs and communication over web-sockets are all read as a file streams.\nThe reTerminal has 3 programmable LED’s and a light sensor that can be controlled like a regular file.\n\n\n\n\n\nreTerminal interface overview. Image source\n\n\nYou can see, there are 3 programmable LEDs in the reTerminal:\n\nSTA light can be turned on as red or green.\nUSR light can only be turned on as green.\n\n\n\n\n\n\nThe reTerminal LEDs and their corresponding filenames. Image source\n\n\nThe lights can be controlled at the OS level by editing files in the /sys/class/leds/ directory. Use ls -al to list the files in this directory:\n\n\nbash\n\nusername@hostname:/sys/class/leds/usr_led0 $ ls -al\ntotal 0\ndrwxr-xr-x 3 root root    0 Jan 25 20:33 .\ndrwxr-xr-x 8 root root    0 Jan 25 20:33 ..\n-rw-r--r-- 1 root root 4096 Jan 26 22:02 brightness\n\n\n\n.\nThe brightness file inide the usr_led0 controls the brightness of LED0. But, because only root has write permissions to this file, we will likely run into permissions errors if we try to edit the value directly:\n$ nano /sys/class/leds/usr_led0/brightness\nPermission denied\n\n$ echo 255 &gt; /sys/class/leds/usr_led0/brightness\nPermission denied\n\n$ sudo echo 255 &gt; /sys/class/leds/usr_led0/brightness\nPermission denied\nThere are a few possible approaches to this problem:\n\n1.1 Use sudo + text-editor\nYou can open a text-editor with root permissions using sudo:\n# nano text editor\nsudo nano /sys/class/leds/usr_led0/brightness\n\n# or, you can use vi/vim text editor\nsudo vim /sys/class/leds/usr_led0/brightness\nEdit the brightness file to a value between 0-255 using the editor. When you save, you should see the change immediately.\n\n\n1.2 Use sudo + su\nUsing an editor is perfectly reasonable for a one-off change, but annoying if you want to make the change more often.\nWe can use the echo value &gt; /path/to/file pattern, but only if we have permissions to write to /path/to/file – we can obtain these permissions if we run the entire command as the root user.\n\nNOTE: Running commands as the root user can have unintended consequences, ranging from annoying/tedious to fix, to completely devastating/permanently ruinous for your machine. You should avoid being root wherever possible (see Use sudo + tee section below for how to avoid it for this problem).\n\nYou can enter a root shell instance using the command su (switch user), or by using sudo -i or sudo -s:\nuser@hostname $ sudo su\nroot@hostname #\n\nuser@hostname $ sudo -i\nroot@hostname #\n\nuser@hostname $ sudo -s\nroot@hostname #\nYour shell should now display root@hostname:~#.\nTurn on the LED with maximum brightness\n# echo 255 &gt; brightness\nTurn off the LED\n# echo 0 &gt; brightness\nSimilarly, you can control usr_led1 and usr_led2 and even the buzzer on /sys/class/leds/usr_buzzer\nWhen done working as root, you can exit the root shell and return to your user shell using the exit command (or Ctrl-D as a hotkey):\nroot@hostname # exit\nuser@hostname $\n\n\n1.3 Use sudo + tee\nIf we want to avoid using a text editor, AND avoid logging into a root shell instance, it would be great if something like this worked:\n$ sudo echo 255 &gt; /sys/class/leds/usr_led0/brightness\nBut it doesn’t! Can you understand why? Consider which part of the instruction sudo applies to. Unfortunately, there is no way to sudo &gt; or sudo filename since sudo executes commands as the root user, NOT filenames/redirects.\nWhat if there were a command that we could put after the redirect, and sudo that command? What would that command need to do? It’s time to introduce a “new” command: tee.\nTo be clear, tee is a 50 year old program and an absolute classic – it is as ubiquitous/essential as the other OG unix programs like ls, cat, echo. We will see why in the following example.\nTry running man tee in your reTerminal. You should see something like the following:\nTEE(1)\n\nNAME\n       tee - read from standard input and write to standard output and files\n\nSYNOPSIS\n       tee [OPTION]... [FILE]...\n\nDESCRIPTION\n       Copy standard input to each FILE, and also to standard output.\nWe have here a command that will read from standard input, and write that same content from standard input into BOTH standard output AND files – the files we can specify as arguments.\n$ echo 255 | /sys/class/leds/usr-led0/brightness\n# this command is missing `tee` and `sudo`.... find where to put them, and you've learned a very important unix pattern!\n\n\n1.4 Luminosity Sensor\nThe digital light sensor can read the surrounding light levels.\n\nEnter the following directory\n\ncd /sys/bus/iio/devices/iio:device0\n\nRead the following file to obtain the light intensity value in Lux\n\ncat in_illuminance_input\nOutput:\npi@raspberrypi:/sys/bus/iio/devices/iio:device0 $ cat in_illuminance_input\n2719\nNote: We don’t need to be root to read this file. Its permissions are set to let all users read it, even-though it belongs to the root user:\nrw-r--r-- 1 root root 4096 Jan 30 22:16 in_illuminance_input",
    "crumbs": [
      "reTerminal built-in devices"
    ]
  },
  {
    "objectID": "notes/reterminal-devices/index.html#python-library-for-reterminal",
    "href": "notes/reterminal-devices/index.html#python-library-for-reterminal",
    "title": "reTerminal built-in devices",
    "section": "2 Python Library for reTerminal",
    "text": "2 Python Library for reTerminal\nSeeed Studio provided a python library to access most of the sensors and actuators of the reTerminal.\nInstall the library seeed-python-reterminal (see official Github repo) using pip:\nNOTE: See Package Management in Python for a clear explanation of how to use venv and pip.\n# NOTE: run this with a venv activated! \npip install seeed-python-reterminal\n\n# NOTE: you will also need this dependency\npip install RPi-GPIO\nNow you can import it to a test script (eg. buzz.py)\nimport seeed_python_reterminal.core as rt\nimport time\n\nprint(\"BUZZER ON\")\nrt.buzzer = True\ntime.sleep(1)\n\nprint(\"BUZZER OFF\")\nrt.buzzer = False\nThis will sound the buzzer of the reTerminal for 1 second.\nTo run the script:\nsudo $(which python) buzz.py\nAlternatively, first elevate your shell, then execute the script normally:\nuser@hostname:~ $ sudo -i\nroot@hostname:~# python buzz.py\n\nNote: this library is simply a wrapper to the OS operations we did in the previous section.\n\nSee the official seeed-python-reterminal Github repo for API reference on how to control:\n\nLED’s\nAccelerometer\nProgrammable Buttons\nLight Sensor",
    "crumbs": [
      "reTerminal built-in devices"
    ]
  },
  {
    "objectID": "notes/python-scripting/index.html",
    "href": "notes/python-scripting/index.html",
    "title": "Python scripting",
    "section": "",
    "text": "This section explores intermediate Python topics to improve our technique at specific linux/IoT scripting tasks.\nWe’ll explore:\n\nPassing terminal arguments to a script\n__main__ and top-level environment\nRunning modules as scripts using __main__\nTip for managing gpio dependencies",
    "crumbs": [
      "Python scripting"
    ]
  },
  {
    "objectID": "notes/python-scripting/index.html#overview",
    "href": "notes/python-scripting/index.html#overview",
    "title": "Python scripting",
    "section": "",
    "text": "This section explores intermediate Python topics to improve our technique at specific linux/IoT scripting tasks.\nWe’ll explore:\n\nPassing terminal arguments to a script\n__main__ and top-level environment\nRunning modules as scripts using __main__\nTip for managing gpio dependencies",
    "crumbs": [
      "Python scripting"
    ]
  },
  {
    "objectID": "notes/python-scripting/index.html#passing-script-arguments",
    "href": "notes/python-scripting/index.html#passing-script-arguments",
    "title": "Python scripting",
    "section": "2 Passing Script Arguments",
    "text": "2 Passing Script Arguments\nIt’s possible to pass command line arguments to a Python script directly from your shell:\n$ python myscript.py first 2 True\nThe arguments passed are first, 2 and True .\n\n2.1 sys.argv\nThe Python sys module provides access to these arguments via sys.argv:\n\nsys.argv is a Python list of arguments.\n\n# myscript.py\n\nprint(f\"Argument List: {sys.argv}\")\nOutput:\n$ python myscript.py first 2 True\n\nArgument List: ['myscript.py', 'first', '2', 'True']\nNotice the following:\n\nThe first element in the list is the name of the script.\nArguments are available as strings.\n\nUnfortunately, we can’t trust that the user will always pass arguments in the correct order and using appropriate data types. We would still need to parse the arguments and make sure they are valid.\nFortunately, there is a built-in module that can help us do that.\n\n\n2.2 argparse\nargparse is a built-in module that makes it easy to write user-friendly command-line interfaces.\nOnce the script defines what arguments are required, argparse will figure out how to parse those out of sys.argv. The argparse module also automatically generates help and usage messages, and issues errors when users give the program invalid arguments.\n\nFor a basic tutorial of argparse, see this page.\n\nIn its simplest form, argparse must be imported and a parser must be instantiated:\n# myscript.py\n\nimport argparse\n\nparser = argparse.ArgumentParser()\nargs = parser.parse_args()\nprint(args)\nHowever, this will generate an error because we must tell argparse what argument flags and what data types to parse.\n$ python myscript.py first 2 True\n\nusage: myscript.py [-h]\nmyscript.py: error: unrecognized arguments: first 2 True\nTo “teach” argparse how to parse an argument we use the command parser.add_argument()\n# myscript.py\nimport argparse\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument(\"word\", type=str)\nparser.add_argument(\"number\", type=int)\nparser.add_argument(\"toggle\", type=bool)\n\nargs = parser.parse_args()\n\nprint(args)\nprint(args.word)\nprint(args.number)\nprint(args.toggle)\n$ python myscript.py first 2 True\n\nNamespace(word='first', number=2, toggle=True)\nfirst\n2\nTrue\nNotice how in the example above, all arguments were positional, which makes them mandatory. In other words, the order in which they are passed determines which variable they were being assigned to.\nIt’s also possible to make arguments optional. In this case, they must be specified with the correct “Flag”.\n# myscript.py\nimport argparse\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument(\"--word\", type=str)\nparser.add_argument(\"--number\", type=int)\nparser.add_argument(\"--toggle\", type=bool)\n\nargs = parser.parse_args()\n\nprint(args)\nprint(args.word)\nprint(args.number)\nprint(args.toggle)\n$ python myscript.py --word first --number 2\n\nNamespace(word='first', number=2, toggle=None)\nfirst\n2\nNone\n\n\n2.3 argparse References\n\nArticle 10 tips for passing arguments to Python script",
    "crumbs": [
      "Python scripting"
    ]
  },
  {
    "objectID": "notes/python-scripting/index.html#main__-top-level-environment",
    "href": "notes/python-scripting/index.html#main__-top-level-environment",
    "title": "Python scripting",
    "section": "3 __main__ & top-level environment",
    "text": "3 __main__ & top-level environment\nWhy include a if __name__ == \"__main__\": in your script?\nThere are two ways of executing Python code. Depending on how the code is executed, the global string variable __name__ will take one of two values:\n\nIf the script is run by the Python interpreter :\n\n__name__ has the value of __main__\n\nImporting script as a separate Python module:\n\n__name__ has the value of the module name.\n\n\nLet’s illustrate these two cases below.\n\nTL:DR\n\nOfficial docs:\n\n__main__— Top-level code environment\n\nSummary discussion at Stackoverflow:\n\nWhat does if name == “main”: do?\n\n\n\n\n3.1 Executing as a Script\nConsider the following script:\n# file my_script.py\n\nprint(\"Inside my_script.py, variable `__name__` is: \", __name__)\nExecuting this file with the Python interpreter:\n$ python my_script.py\n\n# Output\n# Inside my_script, variable `__name__` is: __main__\n\n\n3.2 Executing as a Module\nConsider a new file:\n# file your_script.py\n\nprint(\"Inside your_script.py, variable `__name__` is: \", __name__)\nNow modify my_script.py to import your_script.py:\n# file my_script.py\n\nimport your_script\n\nprint(\"Inside my_script.py, variable `__name__` is: \", __name__)\n\nWhen a file is imported as a module, it’s top-level code gets executed immediately.\n\nWhen we execute my_script.py again using the Python interpreter:\n$ python my_script.py\n\n# Output\n# Inside your_script, variable `__name__` is: your_script\n# Inside my_script, variable `__name__` is: __main__\nThe first print statement came from your_script.py when it was imported by my_script.py.\nNotice how inside your_script.py, the variable __name__ was the file name because it was being run as a module - from a different script (my_script.py).\n\n\n3.3 Top-level Code\nAll of the code that is at indentation level 0 gets executed is called the top-level.\n__main__ is the name of the environment where top-level code is run.\nIf a module is being run as a script (as in mys_cript.py above), then __name__ is instead set to the string \"__main__\".\nYou can test whether your script is being run directly or being imported by something else by testing what __name__ evaluates to:\n# another_script.py\n\ndef main():\n    # Include whatever code here.\n\nif __name__ == \"__main__\":\n    main()  # Will only run if executing file as a script.",
    "crumbs": [
      "Python scripting"
    ]
  },
  {
    "objectID": "notes/analog-vs-digital/index.html",
    "href": "notes/analog-vs-digital/index.html",
    "title": "Signals: Analog and digital",
    "section": "",
    "text": "We will use analog and digital signals to communicate with sensors.\nThis section briefly compares these two types of signals and illustrates how we might encounter while using IoT hardware.\nTo compare analog and digital signals, let’s look at how we can track 3 different sources of information:",
    "crumbs": [
      "Signals: Analog and digital"
    ]
  },
  {
    "objectID": "notes/analog-vs-digital/index.html#analog-the-natural-world",
    "href": "notes/analog-vs-digital/index.html#analog-the-natural-world",
    "title": "Signals: Analog and digital",
    "section": "1 Analog: the natural world",
    "text": "1 Analog: the natural world\nWe live in an analog world. Most of the naturally occurring events that we experience happen in an analog form.\n\nAnalog signals varies continuously in time and amplitude.\nThis means that changes happen over a range of values with infinite “in between” steps.\n\nFor example, the following events have an infinite number of transitions. It only depends on how accurately you can track them:\n\nThe colors in a rainbow.\nThe position of a swinging pendulum.\nThe exact time of the day.\n\n\n1.1 Temperature\nThe A/C and heating systems in a house are typically activated once the temperature reaches some minimum and maximum values.\nThe chart below is tracking the temperature in a room as the A/C and heating systems are activated over a few days.\n\n\n Temperature changes over time   Adapted from poster: Analog and Digital Signals by Digikey.\n\nLet’s say the maximum temperature reached in the room is 30C and the minimum temperature is 15C. A person in the room would have observed every possible temperature in between that range (15.01, 15.0101, 15.0102, 15.01021, etc).\nThe change in temperature produces analog information since it is changing over an infinite range of values.\n\n\n1.2 Sound\nSound is also an example of an analog signal.\nWhen a volume of air is displaced quickly, air molecules are compressed against each other creating a high-pressure region. The high pressure region expands and compresses the air in the neighbouring area. Sound can travel thanks to this “chain effect”.\n\nBy observing how the air pressure changes over time, we can draw a sound wave.\nThis is how speakers and microphones interact with air to generate sound.\n\n\n\n\nAnimation of acoustic wave travelling in air\n\n\n\n Sound waves travelling through air   - Waves and Acoustics Animations by isvr.\n\nSound waves are also an example of an analog signal because the amplitude of the wave transitions through the entire range of possible values between the Min and Max.\n\n\n1.3 Analog Signals Graphs\nA signal varies over time. It’s helpful to plot it on a graph where time is plotted on the horizontal, x-axis, and the value being tracked on the vertical, y-axis.\nLooking at a graph of a signal is usually the easiest way to identify if it’s analog or digital.\n\nWhen dealing with electricity we track voltage levels (amplitude).\nA time-versus-voltage graph of an analog signal should be smooth and continuous.\n\n\n\n\nAnalog Sine Wave\n\n\nWhile these signals may be limited to a range of maximum and minimum values, there are still an infinite number of possible values within that range.\nFor example, the analog voltage coming out of your wall socket oscillates between -120V and +120V. As you increase the resolution more and more, you discover an infinite number of values that the signal can actually be (64.4V, 64.42V, 64.424V, and other increasingly precise values).",
    "crumbs": [
      "Signals: Analog and digital"
    ]
  },
  {
    "objectID": "notes/analog-vs-digital/index.html#digital-discrete-values",
    "href": "notes/analog-vs-digital/index.html#digital-discrete-values",
    "title": "Signals: Analog and digital",
    "section": "2 Digital: discrete values",
    "text": "2 Digital: discrete values\nDigital signals are discrete, which means that at any given moment, the signal strength must be represented by a integer number. There are no half numbers.\nTypically, digital signals will be one of two values, a high voltage and a low voltage.\n\nThe specific values of the high and low voltages depend on the hardware being used.\n\nBelow is a the timing graph of a signal whose low voltage is 0 volts and high voltage is 5 volts.\nThis type of signal is also known as a square waves.\n\n\n\nSquare wave signal. Two values, either 0V or 5V.\n\n\n\n 0 to 5 volt “pulses” forming a square ware   - Analog vs. Digital by Digikey.\n\n\n2.1 Digital Simulating Analogue\nA digital signal might be a discrete representation of an analog waveform.\nViewed from afar, the wave function below may seem smooth and analog, but when you look closely there are tiny discrete steps as the signal tries to approximate analog values.\n\n\n\nDigital Sine Wave\n\n\n\n Oscillating voltage from -120V to 120V represented as a digital signal   - Analog vs. Digital by Digikey.\n\nThe number of discrete steps that a digital signal can use depends on how many bits are available for each recorded value. This is also called bit resolution (see Sampling below).\nFor example, if a data point can only use one bit of definition, then it can only track on/off states, resulting in the red square wave seen above.\nHowever, if a data point can use 8 bits to store a value, then it can represent a value ranging from 0 to 255 (see the green sinusoidal voltage wave above).",
    "crumbs": [
      "Signals: Analog and digital"
    ]
  },
  {
    "objectID": "notes/analog-vs-digital/index.html#analog-to-digital-conversion-adc",
    "href": "notes/analog-vs-digital/index.html#analog-to-digital-conversion-adc",
    "title": "Signals: Analog and digital",
    "section": "3 Analog to Digital Conversion (ADC)",
    "text": "3 Analog to Digital Conversion (ADC)\nIf most natural events are analog, how do we represent them in digital format? For example, if audio is analog, how can we get a computer to play music?\n\nAn analog to digital converter (ADC) is a device that can read a analog signal and generate a digital representation of this signal.\n\nADC’s are specific to the type of analog signal they are trying to read (sound, voltage, temperature, light, etc).\nReferring to the room temperature example used earlier, if we used a digital thermometer to record the temperature, our graph might look like the following:\n\n\n Temperature represented as a digital reading   Poster: Analog and Digital Signals by Digikey.\n\nIn the example above, a digital temperature reading was taken every 3hr.\n\nThe digital signal representation of the temperature is not smooth or accurate.\nHowever, depending on the application it might be sufficient.\n\nTo make the signal smoother or more representative of the real analog signal we need to take samples more regularly (eg. every 1min).\nThe process of reading an analog signal over time to generate its digital representation is called sampling.",
    "crumbs": [
      "Signals: Analog and digital"
    ]
  },
  {
    "objectID": "notes/analog-vs-digital/index.html#sampling",
    "href": "notes/analog-vs-digital/index.html#sampling",
    "title": "Signals: Analog and digital",
    "section": "4 Sampling",
    "text": "4 Sampling\nSampling is the process of inspecting the value of an analog signal at regular time intervals.\nWhen an ADC is sampling an analog signal, there are two variables that will characterize the digital output:\n\nSampling rate.\nBit resolution.\n\n\nMusic sampling Some music genres like rap and hip-hop commonly sample song segments from other artists to play in the background.\n\nThe term sampling is used because traditionally song segments were extracted from vinyl records which produce analog sound and saved in digital formats.\n\n\n\n\n\n4.1 Sampling rate\nThe time between samples is the sample period (T, in seconds), and the number of samples taken per second is the sample frequency or sample rate (fs, in samples/second or Hz).\nBasically, sampling is taking snap-shot values of the analog signal at regular time intervals.\n\n\n\nAnalog signal showing sample period and sample frequency\n\n\n\n Sample period and fequency for an electric signal   - US Naval Academy.\n\nSampling period examples:\n\nIf a signal is sampled every half second (0.5s sampling period, T), the sample frequency (*f**) is 1/0.5s = 2 Hz (times per second).\nIf the sample frequency (f) is 2000 Hz, the sampling period (T) is = 1/2000Hz = 0.0005 secs (0.5 milliseconds).\n\nThe higher the sample rate, the more accurate the digital signal is.\n\n\n\nSignals of increasing sample rate\n\n\n\n Same signal sampled with different sample rates   - US Naval Academy.\n\n\n\n4.2 Encoding & Bit resolution\nEncoding is the process of mapping the sampled analog signal value to discrete binary numbers (digital information).\n\nADC devices are characterized by the number of bits available to represent a signal.\n\nFor example, consider an electrical signal that ranges from -1 volts to to +1 volts and must be converted to a 3 bit number.\n\n\n4.3 Encoding Procedure\nTo approximate the numeric conversion from a voltage signal to a digital output, follow the steps below:\n\nIdentify the minimum and maximum voltage values that can be observed in the input.\n\nExample: -1 to +1 volts (2 volt range)\n\nIdentify the minimum and maximum binary values that can generated as the output. Note the amount of unique numbers represented by this range\n\nExample: For a 3-bit number: 000 (decimal 0) is the minimum and 111 (decimal 7) is the maximum, making a total of 8 unique numbers.\n\nDivide the voltage range that will be observed (max voltage - min voltage) by the amount of unique binary numbers available. This will determine the voltage increment that each binary number represents.\n\nExample: (+1V - (-1V)) / 8 = 2V/8 = 0.25V\n\nLayout each binary number and their increments and read where the voltage values lie.\n\nAssuming the ADC has a resolution of 3 bits, all voltage values (-1V to 1V) must be represented with at most 3 bits, or 2^3 = 8 unique values (0 is a unique value).\n\n\n\nvoltage signal converted to a bit resolution of 3 bits\n\n\n\n Voltage signal between -1V and 1V converted to digital with a 3 bit resolution   - US Naval Academy.\n\nIn this case:\n\n000 is assigned to the voltages from -0.75 V to -1.0 V,\n001 is assigned to the voltages from -0.5 V to -0.749 V,\n010 is assigned to the voltages from -0.25 V to -0.49 V,\nand so on.\n\nThe binary representation of the above signal is:\n110 101 100 011 011 100 110 110 100 010 000 000 001\nIf the sampling is happening 2000 times per second (Hz) and for every sample we generate 3 bits, we are generating 2000 x 3 bits = 6000 bits/sec of information. This is also know as the bitrate.\nIf a song of 3 minutes is sampled with the same specs, how big will this file be in kB?\nWhat is the bitrate of music on Spotify?\n\n\n4.4 Sensor voltages & bit resolution\nIoT systems typically work with electrical signals encoded in voltage levels.\nEvery hardware that converts from analog to digital (and vice-versa) needs to have an appropriate ACD device.\nMicrocontrollers are often used to process these signals because they often have an integrated ACD and they are very inexpensive.",
    "crumbs": [
      "Signals: Analog and digital"
    ]
  },
  {
    "objectID": "notes/analog-vs-digital/index.html#exercises",
    "href": "notes/analog-vs-digital/index.html#exercises",
    "title": "Signals: Analog and digital",
    "section": "5 Exercises",
    "text": "5 Exercises\n\n5.1 Exercise 1\nConsidering the analog signal below. The signal is being sampled in a program at 2Hz (2 times/sec) with the help of an ADC that has 6-bit resolution. The ADC can handle up to 8V signals.\n\nWhat are the voltages being sampled by the ADC?\nIf the signal reached max voltage, what would be the integer number passed to the program by the ADC?\n\n\n\n\nGraph for exercise 1\n\n\n\n\nSolution\n\n\n\nSampled voltages: 4, 6, 6, 0, 2, 2, 2, 5, 6, 4\n\n\n\nSince the ADC has a 6-bit resolution, it can represent values up to 63 (all 6 bits ON at the same time). Max voltage would result in the integer 63.",
    "crumbs": [
      "Signals: Analog and digital"
    ]
  },
  {
    "objectID": "notes/analog-vs-digital/index.html#references",
    "href": "notes/analog-vs-digital/index.html#references",
    "title": "Signals: Analog and digital",
    "section": "6 References",
    "text": "6 References\n\nAnalog vs. Digital by Sparkfun.\nLesson 20: Analog to Digital Conversion, Course EC312, US Naval Academy.\nLessons 4 and 5, Telecom course, Sandy Bultena.\n\n\n6.1 Video Reference",
    "crumbs": [
      "Signals: Analog and digital"
    ]
  },
  {
    "objectID": "notes/gpios/index.html",
    "href": "notes/gpios/index.html",
    "title": "GPIOs: Inputs and Outputs",
    "section": "",
    "text": "Note: most of these notes were adapted directly from the Raspberry Pi docs: GPIO-Pinout 1\n1 Raspberry Pi documentation is copyright © 2012-2024 Raspberry Pi Ltd and is licensed under a Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA) licence. Some content originates from the eLinux wiki, and is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported licence.The Raspberry Pi can read and generate digital signals using General Purpose Input and Output (GPIO) pins.\nAny of the GPIO pins can be designated (in software) as an input or output pin and used for a wide range of purposes.\n\n\n\nDiagram of Pi’s 40 GPIO pins\n\n\n\n GPIO and the 40-pin headers of the Raspberry Pi   - Official docs, Raspberry Pi Foundation.\n\nThe reTerminal exposes the same 40-pin header as the Pi on it’s side:\n\n\n\nBreakout of 40-pin header for reTerminal\n\n\n\n GPIO and pin diagram of the reTerminal   - reTerminal Official Wiki, Seeed.\n\n\n\nTwo 5V pins and two 3V3 pins are present on the board, as well as a number of ground pins (0V), which are unconfigurable. The remaining pins are all general purpose 3V3 pins, meaning outputs are set to 3V3 and inputs are 3V3-tolerant.\n\n\n\nA GPIO pin designated as an output pin can be set to high (3V3) or low (0V).\n\n\n\nA GPIO pin designated as an input pin can be read as high (3V3) or low (0V).\nWe will learn how to use the GPIOs in a future lesson.\n\n\n\nIn addition to simple input and output devices, the GPIO pins can be used with a variety of alternative functions and digital communication protocols.\nThese digital communication protocols are:\n\nPWM (pulse-width modulation)\nSPI (serial peripheral interface)\nI2C (inter-integrated circuit)\nSerial\nPCM (pulse-code modulation)\n\nSome digital functions are available on all pins, others on specific pins.\n\n\n\n\nThe Raspberry Pi does not have an ADC.\nIn order to process analog electrical signals an external ADC must be used.\n\nHat to the Rescue\nIn this course we will use the integrate ADC of the Grove Base Hat for the Raspberry Pi.\n\n\n\nGrove Base Hat for the Raspberry Pi\n\n\n\n Seeed’s Grove Base Hat for the Raspberry Pi has an integrated ADC   Base Hat official wiki, Seeed.\n\nPi HATs is the term for expansion boards for the Raspberry Pi.\n\nSeeed’s Grove Base Hat for the Raspberry Pi has 4 connectors with integrated ADC.\nEach ADC connector has 12-bit resolution.\n\nIn addition to the 4 ADC connectors, the Base Hat also exposes the original 40-pin header and other digital connectors.\n\n\n\nConnectors of the Grove Base Hat\n\n\n\n Specialized connectors of the Raspberry Pi available via the Base Hat   Base Hat official wiki, Seeed.",
    "crumbs": [
      "GPIOs: Inputs and Outputs"
    ]
  },
  {
    "objectID": "notes/gpios/index.html#intro-to-pis-gpios",
    "href": "notes/gpios/index.html#intro-to-pis-gpios",
    "title": "GPIOs: Inputs and Outputs",
    "section": "",
    "text": "Note: most of these notes were adapted directly from the Raspberry Pi docs: GPIO-Pinout 1\n1 Raspberry Pi documentation is copyright © 2012-2024 Raspberry Pi Ltd and is licensed under a Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA) licence. Some content originates from the eLinux wiki, and is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported licence.The Raspberry Pi can read and generate digital signals using General Purpose Input and Output (GPIO) pins.\nAny of the GPIO pins can be designated (in software) as an input or output pin and used for a wide range of purposes.\n\n\n\nDiagram of Pi’s 40 GPIO pins\n\n\n\n GPIO and the 40-pin headers of the Raspberry Pi   - Official docs, Raspberry Pi Foundation.\n\nThe reTerminal exposes the same 40-pin header as the Pi on it’s side:\n\n\n\nBreakout of 40-pin header for reTerminal\n\n\n\n GPIO and pin diagram of the reTerminal   - reTerminal Official Wiki, Seeed.\n\n\n\nTwo 5V pins and two 3V3 pins are present on the board, as well as a number of ground pins (0V), which are unconfigurable. The remaining pins are all general purpose 3V3 pins, meaning outputs are set to 3V3 and inputs are 3V3-tolerant.\n\n\n\nA GPIO pin designated as an output pin can be set to high (3V3) or low (0V).\n\n\n\nA GPIO pin designated as an input pin can be read as high (3V3) or low (0V).\nWe will learn how to use the GPIOs in a future lesson.\n\n\n\nIn addition to simple input and output devices, the GPIO pins can be used with a variety of alternative functions and digital communication protocols.\nThese digital communication protocols are:\n\nPWM (pulse-width modulation)\nSPI (serial peripheral interface)\nI2C (inter-integrated circuit)\nSerial\nPCM (pulse-code modulation)\n\nSome digital functions are available on all pins, others on specific pins.\n\n\n\n\nThe Raspberry Pi does not have an ADC.\nIn order to process analog electrical signals an external ADC must be used.\n\nHat to the Rescue\nIn this course we will use the integrate ADC of the Grove Base Hat for the Raspberry Pi.\n\n\n\nGrove Base Hat for the Raspberry Pi\n\n\n\n Seeed’s Grove Base Hat for the Raspberry Pi has an integrated ADC   Base Hat official wiki, Seeed.\n\nPi HATs is the term for expansion boards for the Raspberry Pi.\n\nSeeed’s Grove Base Hat for the Raspberry Pi has 4 connectors with integrated ADC.\nEach ADC connector has 12-bit resolution.\n\nIn addition to the 4 ADC connectors, the Base Hat also exposes the original 40-pin header and other digital connectors.\n\n\n\nConnectors of the Grove Base Hat\n\n\n\n Specialized connectors of the Raspberry Pi available via the Base Hat   Base Hat official wiki, Seeed.",
    "crumbs": [
      "GPIOs: Inputs and Outputs"
    ]
  },
  {
    "objectID": "notes/gpios/index.html#gpio-pinout",
    "href": "notes/gpios/index.html#gpio-pinout",
    "title": "GPIOs: Inputs and Outputs",
    "section": "2 GPIO pinout",
    "text": "2 GPIO pinout\nA GPIO reference can be accessed on your Raspberry Pi by opening a terminal window and running the command pinout. This tool is provided by the GPIO Zero Python library, which is installed by default in Raspberry Pi OS.\n$ pinout\n::: {.callout-important} While connecting up simple components to the GPIO pins is perfectly safe, it’s important to be careful how you wire things up. LEDs should have resistors to limit the current passing through them. Do not use 5V for 3.3V components. Do not connect motors directly to the GPIO pins, instead use an H-bridge circuit or a motor controller board. :::\n\n2.1 Permissions\nIn order to use the GPIO ports, your user must be a member of the gpio group. The default user account is a member by default, other users need to be added manually.\n# Only do this to add a NEW user to the gpio group\nsudo usermod -a -G gpio &lt;username&gt;",
    "crumbs": [
      "GPIOs: Inputs and Outputs"
    ]
  },
  {
    "objectID": "notes/gpios/index.html#gpio-in-python",
    "href": "notes/gpios/index.html#gpio-in-python",
    "title": "GPIOs: Inputs and Outputs",
    "section": "3 GPIO in Python",
    "text": "3 GPIO in Python\nUsing the GPIO Zero library makes it easy to control GPIO devices with Python. The library is comprehensively documented at gpiozero.readthedocs.io.\n\n3.1 LED\nTo control an LED connected to GPIO17:\nfrom gpiozero import LED\nfrom time import sleep\n\nled = LED(17)\n\nwhile True:\n    led.on()\n    sleep(1)\n    led.off()\n    sleep(1)\nLED methods include on(), off(), toggle(), and blink().\n\n\n3.2 Button\nTo read the state of a button connected to GPIO2:\nfrom gpiozero import Button\nfrom time import sleep\n\nbutton = Button(2)\n\nwhile True:\n    if button.is_pressed:\n        print(\"Pressed\")\n    else:\n        print(\"Released\")\n    sleep(1)\nButton functionality includes the properties is_pressed and is_held; callbacks when_pressed, when_released, and when_held; and methods wait_for_press() and wait_for_release.\n\n\n3.3 Button and LED\nTo connect the LED and button together, you can use this code:\nfrom gpiozero import LED, Button\n\nled = LED(17)\nbutton = Button(2)\n\nwhile True:\n    if button.is_pressed:\n        led.on()\n    else:\n        led.off()\nAlternatively:\nfrom gpiozero import LED, Button\n\nled = LED(17)\nbutton = Button(2)\n\nwhile True:\n    button.wait_for_press()\n    led.on()\n    button.wait_for_release()\n    led.off()\nor:\nfrom gpiozero import LED, Button\n\nled = LED(17)\nbutton = Button(2)\n\nbutton.when_pressed = led.on\nbutton.when_released = led.off",
    "crumbs": [
      "GPIOs: Inputs and Outputs"
    ]
  },
  {
    "objectID": "notes/bash-essentials/index.html",
    "href": "notes/bash-essentials/index.html",
    "title": "Bash essentials",
    "section": "",
    "text": "Image source",
    "crumbs": [
      "Bash essentials"
    ]
  },
  {
    "objectID": "notes/bash-essentials/index.html#using-cli-effectively",
    "href": "notes/bash-essentials/index.html#using-cli-effectively",
    "title": "Bash essentials",
    "section": "1 Using CLI effectively",
    "text": "1 Using CLI effectively\nFirst things first: the terminal can feel awkward to use. What can we do about this?\nEach section below is some set of tips for using the interactive bash CLI effectively.\n\n1.1 Keyboard shortcuts\n\n\n\n\n\n\nKeyboard Shortcuts: details & examples\n\n\n\n\n\nThis section was adapted from (“Bash Keyboard Shortcuts - Linux - SS64.com” n.d.).\n\n1.1.1 Completions\nUse TAB completion for file/directory names. Type just enough characters to uniquely identify the item.\nFor example, to move to a directory sample1; Type cd sam. Then press TAB and ENTER.\n\n\n1.1.2 Moving the cursor\n\nCtrl+a: Go to the beginning of the line (Home).\nCtrl+e: Go to the End of the line (End).\nCtrl+p: Previous command (Up arrow).\nCtrl+n: Next command (Down arrow).\nAlt+b: Back (left) one word.\nAlt+f: Forward (right) one word.\nCtrl+f: Forward one character.\nCtrl+b: Backward one character.\n\n\n\n1.1.3 While using man or command --help | less\n\nk: Scroll up one line\nj: Scroll down one line\nCtrl+u: Page up\nCtrl+d: Page down\n/: Begin forward search\n?: Begin reverse search\nn/N: Find next/previous match\nq: close the less pager\n\n\n\n1.1.4 Editing\n\nCtrl+L: Clear the Screen, similar to the clear command.\nAlt+Del: Delete the Word before the cursor.\nAlt+d: Delete the Word after the cursor.\nCtrl+d: Delete character under the cursor.\nCtrl+h: Delete character before the cursor (Backspace).\nCtrl+w: Cut the Word before the cursor to the clipboard.\nCtrl+k: Cut the Line after the cursor to the clipboard.\nCtrl+u: Cut/delete the Line before the cursor to the clipboard.\nAlt+t: Swap current word with previous.\nCtrl+t: Swap the last two characters before the cursor (typo).\nctrl+y: Paste the last thing to be cut (yank).\nAlt+u: UPPER capitalize every character from the cursor to the end of the current word.\nAlt+l: Lower the case of every character from the cursor to the end of the current word.\nAlt+c: Capitalize the character under the cursor and move to the end of the word.\nAlt+r: Cancel the changes and put back the line as it was in the history (revert).\nctrl+_: Undo.\n\n\n\n1.1.5 Special keys\n\nCtrl+v tells the terminal to not interpret the following character\n\nso Ctrl+v TAB will display a tab character rather than attempting completion.\nsimilarly Ctrl+v ENTER will display the escape sequence for the Enter key: ^M\n\n\n\n\n1.1.6 History\n\nCtrl+r: Recall the last command including the specified character(s).\nCtrl+p: Previous command in history (walk back).\nCtrl+n: Next command in history (walk forward).\nCtrl+o: Execute the command found via Ctrl+r or Ctrl+s Ctrl+o\nCtrl+g: Escape from history searching mode.\n\n\n\n1.1.7 Process Control\n\nCtrl+c: Interrupt/Kill whatever you are running (SIGINT).\nCtrl+l: Clear the screen.\nCtrl+s: Stop output to the screen (for long running verbose commands). Then use PgUp/PgDn for navigation.\nCtrl+q: Allow output to the screen (if previously stopped using command above).\nCtrl+d: Send an EOF marker, unless disabled by an option, this will close the current shell (EXIT).\nCtrl+z: Send the signal SIGTSTP to the current task, which suspends it. To return to it later enter fg 'process name'\n\n\n\n\n\n\n\n1.2 Configuration\n\n\n\n\n\n\n.bashrc details & examples\n\n\n\n\n\nEvery time you open a new terminal window/tab in the bash shell, the ~/.bashrc file is read and executed.\nThe typical usecases for customising ~/.bashrc are:\n\nsetting a custom Command prompt\nsetting various useful shopts (shell options)\nsetting environment variables/aliases\nsourcing other bash files\n\nExamples of each of these are shown below.\n\n\n~/.bashrc\n\n# Set a prompt like: [username@hostname:~/CurrentWorkingDirectory]$\nexport PS1='[\\u@\\h:\\w]\\$ '\n#   Explanation:\n#   \\u: username\n#   \\h: hostname\n#   \\w: the current working directory\n#   \\$: the character $\n#   all other characters are interpreted literally\n#   See https://ss64.com/bash/syntax-prompt.html for more examples\n\n# Set useful shell options\nshopt -s autocd # auto-appends `cd` to directory, so you can cd to a path without writing `cd`\nshopt -s globstar # enables the pattern '**' for recursive file/directory wildcard matching\nshopt -s extglob # fancier pattern matching\n# See https://www.gnu.org/software/bash/manual/html_node/The-Shopt-Builtin.html for more options\n\n# Set environment variables/aliases\nexport EDITOR=\"nvim\" # neovim\nexport EDITOR=\"code\" # vscode (overwrites previous line)\n# See https://ss64.com/bash/export.html for more information\n\nalias ll=\"ls -l\" # create new alias ll for a long list\nalias cp=\"cp -iv\" # replace default cp command with interactive/verbose cp\n# See https://ss64.com/bash/alias.html for more alias info and examples\n# Note that aliases cannot handle complex logic or accept positional parameters\n# For that, we would need functions.\n\n# Source all bash files in ~/.bashrc.d/\n# This lets you define functions in various shell files in this folder and source them at startup.\nif [ -d ~/.bashrc.d ]; then\n    for rc in ~/.bashrc.d/*; do\n        if [ -f \"$rc\" ]; then\n            source \"$rc\"\n        fi\n    done\nfi\nunset rc\n# See https://ss64.com/bash/source.html for more info on sourcing\n\n\n1.2.1 Ever Wonder Why it’s Called .bashrc?\nThere are many files that end with the mysterious suffix rc like .bashrc, .vimrc, etc. Why is that? It’s a holdover from ancient Unix. Its original meaning was “run commands,” but it later became “run-control.” A run-control file is generally some kind of script or configuration file that prepares an environment for a program to use. In the case of .bashrc for example, it’s a script that prepares a user’s bash shell environment.\n\n\n\n\n\n\n\n\n\n\n.profile details & examples\n\n\n\n\n\nEvery time you log in to a linux user, the ~/.profile file is read and executed.\nThe typical usecases for customizing ~/.profile are:\n\nsetting environment variables INDEPENDENT of bash instances\n\ni.e., these variables will work in sh, zsh, and other shells\n\nsetting environment variables once per session\n\nparticularly useful for PATH, since setting it in ~/.bashrc will cause it to be updated more frequently than useful\n\n\nExamples of these are shown below:\n\n\n~/.profile\n\n# Add a directory to PATH, checking if that directory is not already in PATH first\nif ! [[ \"$PATH\" =~ \"$HOME/bin:\" ]]; then\n  export PATH=\"$PATH:$HOME/bin\"  # Adds ~/bin to your path\nfi\n\n# Source all profile files in ~/.profile.d/\n# This is useful for programs like npm, you can put its bashrc/path stuff in here instead.\nfor script in $HOME/.profile.d/*.sh ; do\n    if [ -r \"$script\" ] ; then\n        . \"$script\"\n    fi\ndone\nunset script\n# See https://ss64.com/bash/source.html for more info on sourcing\n\n\n\n\n\n\n\n\n\n\n.inputrc details & examples\n\n\n\n\n\nThis section was adapted from (“How-To: Bash Startup Files Inputrc - Linux - SS64.com” n.d.).\nThe library that is used to implement a command line interface for bash is called the Readline library.\nWhile it comes with a set of default keybindings (see the #keyboard-shortcuts section), it is possible to modify these and other behaviors of the CLI interface by putting commands into a .inputrc file, typically in the home directory.\nThe configuration options in .inputrc are particularly useful for customising the way Tab-completion works, e.g. with the ls command.\nThe inputrc variable syntax is simple:\nset variable value\nBelow are a list of variables I find particularly useful, as well as a sample .inputrc file showing how each of these are set.\n\nbell-style\nControls what happens when Readline wants to ring the terminal bell. If set to ‘none’, Readline never rings the bell. If set to ‘visible’, Readline uses a visible bell if one is available. If set to ‘audible’ (the default), Readline attempts to ring the terminal’s bell.\ncompletion-ignore-case\nIf set to ‘on’, Readline performs filename matching and completion in a case-insensitive fashion. The default value is ‘off’.\nediting-mode\nThe editing-mode variable controls which default set of key bindings is used. By default, Readline starts up in Emacs editing mode, where the keystrokes are most similar to Emacs. This variable can be set to either ‘emacs’ or ‘vi’.\nmark-symlinked-directories\nIf set to ‘on’, completed names which are symbolic links to directories have a slash appended (subject to the value of mark-directories). The default is ‘off’.\nshow-all-if-ambiguous\nThis alters the default behavior of the completion functions. If set to ‘on’, words which have more than one possible completion cause the matches to be listed immediately instead of ringing the bell. The default value is ‘off’.\n\nA sample ~/.inputrc file with these variables in use:\n\n\n~/.inputrc\n\nset bell-style none\nset completion-ignore-case On\nset editing-mode vi\nset mark-symlinked-directories On\nset show-all-if-ambiguous On\n\nYou can find many more configuration options in (“How-To: Bash Startup Files Inputrc - Linux - SS64.com” n.d.).",
    "crumbs": [
      "Bash essentials"
    ]
  },
  {
    "objectID": "notes/bash-essentials/index.html#bare-necessities",
    "href": "notes/bash-essentials/index.html#bare-necessities",
    "title": "Bash essentials",
    "section": "2 Bare necessities",
    "text": "2 Bare necessities\nThe following sections explain the purpose of each command and show a few use cases and useful options.\nThese are commands you probably already know – if you don’t, you’ll know by the end of lab-0, as you’ll need them all!\n\n2.1 Getting around: cd and ls\nNAME\n  cd - change the current directory\n  ls - list directory contents\n\nSYNOPSIS\n  cd [DIR]\n  ls [OPTION]... [FILE]...\n\n\n\n\n\n\ncd & ls details & examples\n\n\n\n\n\n\n2.1.1 cd\nUseful shorthands for cd to know:\n# Change to user home directory \n# (usually: /home/username)\n$ cd ~\n\n# WSL: Change to Windows mounted directory\n$ cd /mnt/c/\n\n# Return to previous directory\n$ cd -    # in this case, /home/username\n\n\n2.1.2 ls\nUseful ls options:\n-l                     use a long listing format\n-a, --all              do not ignore entries starting with .\n-d, --directory        list directories themselves, not their contents\n-s, --size             print the allocated size of each file, in blocks\n-t                     sort by time, newest first; see --time\n-h, --human-readable   with -l and -s, print sizes like 1K 234M 2G etc.\n    --si               likewise, but use powers of 1000 not 1024\n-R, --recursive        list subdirectories recursively\n\n\n\n\n\n\n2.2 Viewing files: cat and tac\nNAME\n  cat - concatenate files and print on the standard output\n  tac - concatenate and print files in reverse\n\nSYNOPSIS\n  cat [OPTION]... [FILE]...\n  tac [OPTION]... [FILE]...\n\n\n2.3 Creating files: touch and mkdir\nNAME\n  touch - Update the modification times of each `FILE` to the current time.\n          Creates the files if they do not exist.\n  mkdir - Create the given DIRECTORY(ies) if they do not exist\n\nSYNOPSIS\n  touch [FILE]...\n  mkdir [-p/--parents] [DIRECTORY]...\n\n\n2.4 Moving files: mv and cp\n\nNAME\n  mv - Move `SOURCE` to `DEST`, or multiple `SOURCE`(s) to `DIRECTORY`.\n  cp - Copy SOURCE to DEST, or multiple SOURCE(s) to DIRECTORY.\n\nSYNOPSIS\n  mv  [-f/--force] [-i/--interactive] [-g/--progress] [SOURCE]... [DEST]\n  cp  [-f/--force] [-i/--interactive] [-g/--progress] [-R/--recursive] [SOURCE]... [DEST]\n\n\n\n2.5 Managing permissions: chmod and chown\nNAME\n  chmod - Change the permissions mode of each FILE to MODE.\n  chown - Change file owner and group of each FILE to USER:GROUP\n\nSYNOPSIS\n  chmod [-R/--recursive] [MODE] [FILE]\n  chown [-R/--recursive] [USER:GROUP] [FILE]\n\n\n2.6 Deleting files: rm\nNAME\n  rm - Remove the FILE(s)\n\nSYNOPSIS\n  rm [-f/--force] [-i/--interactive] [-r/--recursive] [FILE]...",
    "crumbs": [
      "Bash essentials"
    ]
  },
  {
    "objectID": "notes/bash-essentials/index.html#the-five-fingers-of-death",
    "href": "notes/bash-essentials/index.html#the-five-fingers-of-death",
    "title": "Bash essentials",
    "section": "3 The five fingers of death",
    "text": "3 The five fingers of death\n\n\n\n\n\nFive Fingers of Death, or King Boxer as it is known on Wikipedia, is a martial-arts movie I have not seen, but I have heard referenced in many songs. It speaks to me that the mastery of a seemlingly small set tools (five fingers) can lead to drastic increases in capability (the ability to inflict death) and I believe this spirit applies directly to working with unix tools. Image source\n\n\nThe following 5 sets of commands are indispensable GNU Coreutils that are included on all linux systems.\nThere are many more coreutils that I have not included – I have chosen these 5 sets as I believe that mastering them, above all, will bring you in harmony with your linux system, and therefore closer to truth, happiness, and the meaning of life – or, if not, at least they will help you solve the labs that I give you in this course.\nAlmost all of these notes are adapted from a resource I found that’s pretty much exactly what I wanted to write myself: (“CLI Text Processing with GNU Coreutils” n.d.). It comes with great explanations and exercises and solutions. I may base some quizzes and tests on it!\n\n3.1 find files and grep content\nNAME\n  find - search for files that match a given expression\n  grep - print lines in file(s) that match a given pattern\n\nSYNOPSIS\n  find [STARTING-POINT...] [OPTION...] [EXPRESSION]\n  grep [OPTION...] PATTERNS [FILE...]\n\n\n\n\n\n\nfind details & examples\n\n\n\n\n\n3.1.1 find\nThis section was adapted from (“A Practical Guide to GNU Find With Examples” 2023)\nLet’s begin by looking first at find’s general syntax:\nfind [STARTING-POINT...] [OPTION...] [EXPRESSION]\nWhat are these different elements?\n\n\n\n\n\n\n\n\nElement\nDescription\nDefault\n\n\n\n\n[OPTION...]\nOptions are arguments about symlinks and search optimization.\nNone\n\n\n[STARTING-POINT...]\nList of directories to search through. The subdirectories are recursively included.\nCurrent directory\n\n\n[EXPRESSION]\nList of expressions with their (often required) values.\nNone\n\n\n\nNothing is mandatory here: running find alone will give you some output.\nHere are the different categories of [EXPRESSION]. Each of these are queries describing how to match files, or what action to perform on these files. They’re always prefixed with a single dash - (like -name for example).\n\n\n\n\n\n\n\nCategory\nDescription\n\n\n\n\nTest expressions\nMost common expressions. They’re used to filtering your files.\n\n\nAction expressions\nExpressions used to perform an action on each file found.\n\n\nOperators\nBoolean operators to manage the relationships between the different expressions.\n\n\n\nLet’s see an example that demonstrates these categories:\n\n\nbash\n\nfind . -name '*.png' -or -perm '664' -delete\n\nThis will recursively search the current directory for all files that EITHER have a filename ending with .png OR that has the permissions 664, then will delete those files. (see the course notes on permissions for more details on the meaning of 664 here.)\nLet’s see what category each of these expressions is:\n\n\n\nExpression\nCategory\n\n\n\n\n-name and -perm\nTest expressions\n\n\n-delete\nAction expression\n\n\n-or\nOperator expression\n\n\n\nThere are, of course, many different Test/Action/Operator expressions, and the beauty of the find command is combining each of these types of expressions to create stunningly efficient file search commands.\nI recommend reading/bookmarking the following resources for great explanations and examples of the various uses for the find command:\n\n“Why is using a shell loop to process text considered bad practice?” on unix.stackexchange\n“Why looping over find’s output bad practice?” on unix.stackexchange\n\n\n\n\n\n\n\n\n\n\ngrep details & examples\n\n\n\n\n\n3.1.2 grep\nThis section was adapted from (“Mastering Linux ‘Grep’ Command Guide with Practical Examples” 2024)\nThe Linux grep command is one of the most powerful and frequently used tools for text search and data filtering. Whether you’re managing system logs, searching through files, or debugging code, grep helps you find specific patterns within large sets of data quickly and efficiently.\nThe basic syntax of grep is as follows:\ngrep [OPTION...] PATTERNS [FILE...]\n\n[OPTION...]: various options you can provide grep to modify the default behavior.\nPATTERNS: the string or regular expression you want to search for.\n[FILE...]: the file(s) where you want to search.\n\n\n3.1.2.1 Practical examples\nEssentially, grep searches for a pattern and displays the matching lines. Here are a few common use-cases:\n\n3.1.2.1.1 Example 1: Searching for a Word in a File\nIf you want to search for a specific word in a file, the most basic command would be:\ngrep \"word\" filename.txt\nThis will return all lines in filename.txt that contain the word “word.”\n\n\n3.1.2.1.2 Example 2: Case-Insensitive Search\nBy default, grep is case-sensitive. If you want to ignore case distinctions, you can use the -i option:\ngrep -i \"word\" filename.txt\nThis command will return matches for both “Word” and “word” in filename.txt.\n\n\n3.1.2.1.3 Example 3: Searching Across Multiple Files\nTo search for a pattern in multiple files at once, you can use wildcards *:\ngrep \"word\" *.txt\nThis will search for “word” in all .txt files in the current directory.\n\n\n3.1.2.1.4 Example 4: Displaying Line Numbers\nTo see the line numbers where the matches occur, use the -n option:\ngrep -n \"word\" filename.txt\nThis command will display the line numbers along with the matching lines.\n\n\n3.1.2.1.5 Example 5: Recursive Search in Directories\nIf you want to search for a pattern across all files in a directory and its subdirectories, use the -r (recursive) option:\ngrep -r \"word\" /path/to/directory/\nThis will search for “word” in all files within /path/to/directory/, including subdirectories.\n\n\n3.1.2.1.6 Example 6: Inverting Search (Exclude a Pattern)\nIf you want to exclude lines that contain a specific pattern, you can use the -v option:\ngrep -v \"word\" filename.txt\nThis command will return all lines that do not contain “word”.\n\n\n3.1.2.1.7 Example 7: Counting Matches\nTo count how many times a pattern appears in a file, use the -c option:\ngrep -c \"word\" filename.txt\nThis will output the number of lines that contain “word” in filename.txt.\n\n\n\n\n\n\n\n3.2 tr characters and cut fields\nNAME\n  tr - Translate characters matching STRING1 in stdin/FILE to STRING2,\n       writing to stdout\n  cut - Prints specified columns from each line of stdin, writes to stdout\n\nSYNOPSIS\n  tr [OPTION]... STRING1 STRING2\n  cut [-d/--delimiter] [-f/--fields] [FILE]\n\n\n\n\n\n\ntr details & examples\n\n\n\n\n\n3.2.1 tr\nThe following section was adapted from (“CLI Text Processing with GNU Coreutils” n.d.)\ntr helps you to map one set of characters to another set of characters. Features like range, repeats, character sets, squeeze, complement, etc makes it a must know text processing tool.\nHere are some examples that map one set of characters to another. As a good practice, always enclose the sets in single quotes to avoid issues due to shell metacharacters.\n\n\nbash\n\n# 'l' maps to '1', 'e' to '3', 't' to '7' and 's' to '5'\n$ echo 'leet speak' | tr 'lets' '1375'\n1337 5p3ak\n\n# example with shell metacharacters\n$ echo 'apple;banana;cherry' | tr\n:\ntr: missing operand\nTry 'tr --help' for more information.\n$ echo 'apple;banana;cherry' | tr ';' ':'\napple:banana:cherry\n\n\n3.2.1.1 Character ranges\nYou can use - between two characters to construct a range (ascending order only).\n\n\nbash\n\n# uppercase to lowercase\n$ echo 'HELLO WORLD' | tr 'A-Z' 'a-z'\nhello world\n\n# swap case\n$ echo 'Hello World' | tr 'a-zA-Z' 'A-Za-z'\nhELLO wORLD\n\n# rot13\n$ echo 'Hello World' | tr 'a-zA-Z' 'n-za-mN-ZA-M'\nUryyb Jbeyq\n$ echo 'Uryyb Jbeyq' | tr 'a-zA-Z' 'n-za-mN-ZA-M'\nHello World\n\n\n\n3.2.1.2 Deleting characters\nUse the -d option to specify a set of characters to be deleted.\n$ echo ‘2024-08-12’ | tr -d ‘-’\n\n\nbash\n\n20240812\n\n# delete all punctuation characters\n$ s='\"Hi\", there! How *are* you? All fine here.'\n$ echo \"$s\" | tr -d '[:punct:]'\nHi there How are you All fine here\n\n\n\n3.2.1.3 Squeezing characters\nThe -s option changes consecutive repeated characters to a single copy of that character.\n\n\nbash\n\n$ echo 'HELLO... hhoowwww aaaaaareeeeee yyouuuu!!' | tr -s 'a-z'\nHELLO... how are you!!\n\n# translate and squeeze\n$ echo 'hhoowwww aaaaaareeeeee yyouuuu!!' | tr -s 'a-z' 'A-Z'\nHOW ARE YOU!!\n\n# delete and squeeze\n$ echo 'hhoowwww aaaaaareeeeee yyouuuu!!' | tr -sd '!' 'a-z'\nhow are you\n\n# squeeze other than lowercase alphabets\n$ echo 'apple    noon     banana!!!!!' | tr -cs 'a-z'\napple noon banana!\n\nYou can see more examples and explanations here: (“CLI Text Processing with GNU Coreutils” n.d.).\n\n\n\n\n\n\n\n\n\n\ncut details & examples\n\n\n\n\n\n3.2.2 cut\nThe following section was adapted from (“CLI Text Processing with GNU Coreutils” n.d.)\nBy default, cut splits the input content into fields based on the tab (\\t) character. You can use the -f option to select a desired field from each input line. To extract multiple fields, specify the selections separated by the comma character.\n\n\nbash\n\n# only the second field\n$ printf 'apple\\tbanana\\tcherry\\n' | cut -f2\nbanana\n\n# first and third fields\n$ printf 'apple\\tbanana\\tcherry\\n' | cut -f1,3\napple cherry\n\n\n3.2.2.1 Field ranges\nYou can use the - character to specify field ranges. You can skip the starting or ending range, but not both.\n\n\nbash\n\n# 2nd, 3rd and 4th fields\n$ printf 'apple\\tbanana\\tcherry\\tfig\\tmango\\n' | cut -f2-4\nbanana cherry fig\n\n# all fields from the start till the 3rd field\n$ printf 'apple\\tbanana\\tcherry\\tfig\\tmango\\n' | cut -f-3\napple banana cherry\n\n# all fields from the 3rd one till the end\n$ printf 'apple\\tbanana\\tcherry\\tfig\\tmango\\n' | cut -f3-\ncherry fig mango\n\n\n\n3.2.2.2 Input Delimiter\nUse the -d option to change the input delimiter. Only a single byte character is allowed. By default, the output delimiter will be same as the input delimiter.\n\n\nbash\n\n$ cat scores.csv\nName,Maths,Physics,Chemistry\nIth,100,100,100\nCy,97,98,95\nLin,78,83,80\n\n$ cut -d, -f2,4 scores.csv\nMaths,Chemistry\n100,100\n97,95\n78,80\n\n# use quotes if the delimiter is a shell metacharacter\n$ echo 'one;two;three;four' | cut -d -f3\ncut: option requires an argument -- 'd'\nTry 'cut --help' for more information.\n-f3: command not found\n$ echo 'one;two;three;four' | cut -d';' -f3\nthree\n\n\n\n3.2.2.3 Output Delimiter\nUse the --output-delimiter option to customize the output separator to any string of your choice. The string is treated literally. Depending on your shell you can use ANSI-C quoting to allow escape sequences.\n\n\nbash\n\n$ printf 'apple\\tbanana\\tcherry\\n' | cut --output-delimiter=, -f1-\napple,banana,cherry\n\n# example for multicharacter output separator\n$ echo 'one;two;three;four' | cut -d';' --output-delimiter=' : ' -f1,3-\none : three : four\n\n# ANSI-C quoting example\n# depending on your environment, you can also press Ctrl+v and then the Tab key\n$ echo 'one;two;three;four' | cut -d';' --output-delimiter=$'\\t' -f1,3-\none three four\n\n# newline as the output field separator\n$ echo 'one;two;three;four' | cut -d';' --output-delimiter=$'\\n' -f2,4\ntwo\nfour\n\nYou can see more examples and explanations here: (“CLI Text Processing with GNU Coreutils” n.d.).\n\n\n\n\n\n\n3.3 sort data and uniq duplicates\nNAME\n  sort - Display sorted concatenation of all FILE(s).\n         With no FILE, or when FILE is -, read stdin\n  uniq - Report or omit repeated lines.\n\nSYNOPSIS\n  sort [FILE]...\n  uniq [-d/--repeated] [FILE]...\n\n\n\n\n\n\nsort details & examples\n\n\n\n\n\n3.3.1 sort\nThe following section was adapted from (“CLI Text Processing with GNU Coreutils” n.d.)\nThe sort command provides a wide variety of features. In addition to lexicographic ordering, it supports various numerical formats. You can also sort based on particular columns. And there are nifty features like merging already sorted input, debugging, determining whether the input is already sorted and so on.\nBy default, sort orders the input in ascending order:\n\n\nbash\n\n$ cat greeting.txt\nHi there\nHave a nice day\n\n# extract and sort space separated words\n$ &lt;greeting.txt tr ' ' '\\n' | sort\na\nday\nHave\nHi\nnice\nthere\n\n\n3.3.1.1 Dictionary sort\nThe -d option will consider only alphabets, numbers and blanks for sorting. Space and tab characters are considered as blanks, but this would also depend on the locale.\n\n\nbash\n\n$ printf '(banana)\\n{cherry}\\n[apple]' | LC_ALL=C sort -d\n[apple]\n(banana)\n{cherry}\n\n\n\n3.3.1.2 Reversed order\nThe -r option will reverse the output order. Note that this doesn’t change how sort performs comparisons, only the output is reversed. You’ll see an example later where this distinction becomes clearer.\n\n\nbash\n\n$ printf 'peace\\nrest\\nquiet' | sort -r\nrest\nquiet\npeace\n\n\n\n3.3.1.3 Numeric sort\nThe sort command provides various options to work with numeric formats. For most cases, the -n option is enough. Here’s an example:\n\n\nbash\n\n# lexicographic ordering isn't suited for numbers\n$ printf '20\\n2\\n3\\n111\\n314' | sort\n111\n2\n20\n3\n314\n\n# -n helps in this case\n$ printf '20\\n2\\n3\\n111\\n314' | sort -n\n2\n3\n20\n111\n314\n\n\n\n\n\n\n\n\n\n\n\nuniq details & examples\n\n\n\n\n\n3.3.2 uniq\nThe following section was adapted from (“CLI Text Processing with GNU Coreutils” n.d.)\nThe uniq command identifies similar lines that are adjacent to each other. There are various options to help you filter unique or duplicate lines, count them, group them, etc.\n\n3.3.2.1 Retain single copy of duplicates\nThis is the default behavior of the uniq command. If adjacent lines are the same, only the first copy will be displayed in the output.\n\n\nbash\n\n# only the adjacent lines are compared to determine duplicates\n# which is why you get 'red' twice in the output for this input\n$ printf 'red\\nred\\nred\\ngreen\\nred\\nblue\\nblue' | uniq\nred\ngreen\nred\nblue\n\nYou’ll need sorted input to make sure all the input lines are considered to determine duplicates. For some cases, sort -u is enough, like the example shown below:\n\n\nbash\n\n# same as sort -u for this case\n$ printf 'red\\nred\\nred\\ngreen\\nred\\nblue\\nblue' | sort | uniq\nblue\ngreen\nred\n\nSometimes though, you may need to sort based on some specific criteria and then identify duplicates based on the entire line contents. Here’s an example:\n\n\nbash\n\n# can't use sort -n -u here\n$ printf '2 balls\\n13 pens\\n2 pins\\n13 pens\\n' | sort -n | uniq\n2 balls\n2 pins\n13 pens\n\n\n\n3.3.2.2 Duplicates only\nThe -d option will display only the duplicate entries. That is, only if a line is seen more than once.\n\n\nbash\n\n$ cat purchases.txt\ncoffee\ntea\nwashing powder\ncoffee\ntoothpaste\ntea\nsoap\ntea\n\n$ sort purchases.txt | uniq -d\ncoffee\ntea\n\nTo display all the copies of duplicates, use the -D option.\n\n\nbash\n\n$ sort purchases.txt | uniq -D\ncoffee\ncoffee\ntea\ntea\ntea\n\n\n\n3.3.2.3 Unique only\nThe -u option will display only the unique entries. That is, only if a line doesn’t occur more than once.\n\n\nbash\n\n$ sort purchases.txt | uniq -u\nsoap\ntoothpaste\nwashing powder\n\n# reminder that uniq works based on adjacent lines only\n$ printf 'red\\nred\\nred\\ngreen\\nred\\nblue\\nblue' | uniq -u\ngreen\nred\n\nYou can see more examples and explanations here: (“CLI Text Processing with GNU Coreutils” n.d.).\n\n\n\n\n\n\n3.4 know head from tail\nNAME\n  head - Print the first 10 lines of each `FILE` to standard output.\n         With no `FILE`, or when `FILE` is `-`, read stdin\n  tail - Print the last 10 lines of each `FILE` to standard output.\n         With no `FILE`, or when `FILE` is `-`, read stdin\n\nSYNOPSIS\n  head [-n/--lines] [FILE]...\n  tail [-n/--lines] [-f/--follow] [FILE]...\n\n\n\n\n\n\nhead & tail details & examples\n\n\n\n\n\n3.4.1 head and tail\nThe following section was adapted from (“CLI Text Processing with GNU Coreutils” n.d.)\nhead and tail, or a combination of both, are used to extract text content that you know is at the beginning, end, or specific line number of a file.\n\n3.4.1.1 Leading and trailing lines\nConsider this sample file, with line numbers prefixed for convenience.\n\n\nbash\n\n$ cat sample.txt\n 1) Hello World\n 2) \n 3) Hi there\n 4) How are you\n 5) \n 6) Just do-it\n 7) Believe it\n 8) \n 9) banana\n10) papaya\n11) mango\n12) \n13) Much ado about nothing\n14) He he he\n15) Adios amigo\n\nBy default, head and tail will display the first and last 10 lines respectively.\n\n\nbash\n\n$ head sample.txt\n 1) Hello World\n 2) \n 3) Hi there\n 4) How are you\n 5) \n 6) Just do-it\n 7) Believe it\n 8) \n 9) banana\n10) papaya\n\n$ tail sample.txt\n 6) Just do-it\n 7) Believe it\n 8) \n 9) banana\n10) papaya\n11) mango\n12) \n13) Much ado about nothing\n14) He he he\n15) Adios amigo\n\nNote: If there are less than 10 lines in the input, only those lines will be displayed.\nYou can use the -nN option to customize the number of lines:\n\n\nbash\n\n# first three lines\n# space between -n and N is optional\n$ head -n3 sample.txt\n 1) Hello World\n 2) \n 3) Hi there\n\n# last two lines\n$ tail -n2 sample.txt\n14) He he he\n15) Adios amigo\n\n\n\n3.4.1.2 Excluding N lines\nBy using a “subtraction” style syntax, like head -n -N, you can invert the selection – that is, get all the input lines EXCEPT the last -N lines in the case of head, or the first -N lines in the case of tail.\n\n\nbash\n\n# except the last 11 lines\n$ head -n -11 sample.txt\n 1) Hello World\n 2) \n 3) Hi there\n 4) How are you\n\n# except the first 11 lines\n$ tail -n -11 sample.txt\n12) \n13) Much ado about nothing\n14) He he he\n15) Adios amigo\n\nYou can see more examples and explanation at (“CLI Text Processing with GNU Coreutils” n.d.).\n\n\n\n\n\n\n3.5 tree and tee\nNAME\n  tree - list contents of DIRECTORIES in a tree-like format.\n  tee - Copy standard input to each FILE, and also to standard output.\n\nSYNOPSIS\n  tree [-L level] [DIRECTORY]...\n  tee [FILE]...\n\n\n\n\n\n\ntree & tee details & examples\n\n\n\n\n\nThere’s nothing here yet… stay tuned!",
    "crumbs": [
      "Bash essentials"
    ]
  },
  {
    "objectID": "notes/bash-essentials/index.html#redirection-and-pipes",
    "href": "notes/bash-essentials/index.html#redirection-and-pipes",
    "title": "Bash essentials",
    "section": "4 Redirection and Pipes",
    "text": "4 Redirection and Pipes\nThis section was adapted from (“How-To: Redirection and Process Substitution - Linux - SS64.com” n.d.)\nWhen Bash starts, normally, 3 file descriptors are opened, 0, 1 and 2 also known as standard input (stdin), standard output (stdout) and standard error (stderr).\nYou can use the &gt; operator to “redirect” the output of commands (which normally goes to stdout) to different files or other file descriptors. Some common examples are shown below:\ncommand  &gt;  filename     Redirect command output (stdout) into a file\ncommand  &gt;  /dev/null    Discard stdout of command\ncommand  2&gt; /dev/null    Discard stderr of command\n\ncommand  &gt;&2             Redirect command output (stdout) to stderr\n\ncommand  &gt;&gt; filename     Redirect command output and APPEND into a file\ncommand  &lt;  filename     Redirect a file into a command\n\ncommandA | commandB       Pipe stdout of commandA to commandB\ncommandA | tee filename   Pipe stdout of commandA into filename AND stdout\n\n\n\n\n\n\nRedirection explained further\n\n\n\n\n\nThis section was adapted from (“Illustrated Redirection Tutorial [Bash Hackers Wiki]” 2023)\n\n4.1 Output Redirection n&gt; file\n&gt; is probably the simplest redirection.\necho foo &gt; file\nthe &gt; file after the command alters the file descriptors belonging to the command echo. It changes the file descriptor 1 (&gt; file is the same as 1&gt;file) so that it points to the file file. They will look like:\n                  ---       +-----------------------+\nstandard input   ( 0 ) ----&gt;| /dev/pts/5            |\n                  ---       +-----------------------+\n\n                  ---       +-----------------------+\nstandard output  ( 1 ) ----&gt;| file                  |\n                  ---       +-----------------------+\n\n                  ---       +-----------------------+\nstandard error   ( 2 ) ----&gt;| /dev/pts/5            |\n                  ---       +-----------------------+\nNow characters written by our command, echo, that are sent to the standard output, i.e., the file descriptor 1, end up in the file named file.\nIn the same way, command 2&gt; file will change the standard error and will make it point to file. For example, command 2&gt; /dev/null will delete all errors outputted by command:\n                  ---       +-----------------------+\nstandard input   ( 0 ) ----&gt;| /dev/pts/5            |\n                  ---       +-----------------------+\n\n                  ---       +-----------------------+\nstandard output  ( 1 ) ----&gt;| /dev/pts/5            |\n                  ---       +-----------------------+\n\n                  ---       +-----------------------+\nstandard error   ( 2 ) ----&gt;| /dev/null             |\n                  ---       +-----------------------+\n\n\n4.2 Input Redirection n&lt; file\nWhen you run a command using command &lt; file, it changes the file descriptor 0 so that it looks like:\n                  ---       +-----------------------+\nstandard input   ( 0 ) &lt;----| file                  |\n                  ---       +-----------------------+\n\n                  ---       +-----------------------+\nstandard output  ( 1 ) ----&gt;| /dev/pts/5            |\n                  ---       +-----------------------+\n\n                  ---       +-----------------------+\nstandard error   ( 2 ) ----&gt;| /dev/pts/5            |\n                  ---       +-----------------------+\nIf the command reads from stdin, it now will read from file and not from the console.\n\n\n4.3 Pipes |\nWhat does this | do? Among other things, it connects the standard output of the command on the left to the standard input of the command on the right. That is, it creates a special file, a pipe, which is opened as a write destination for the left command, and as a read source for the right command.\ncommand:   echo foo               |                cat\n\n ---       +--------------+               ---       +--------------+\n( 0 ) ----&gt;| /dev/pts/5   |     ------&gt;  ( 0 ) ----&gt;|pipe (read)   |\n ---       +--------------+    /          ---       +--------------+\n                              /\n ---       +--------------+  /            ---       +--------------+\n( 1 ) ----&gt;| pipe (write) | /            ( 1 ) ----&gt;| /dev/pts     |\n ---       +--------------+               ---       +--------------+\n\n ---       +--------------+               ---       +--------------+\n( 2 ) ----&gt;| /dev/pts/5   |              ( 2 ) ----&gt;| /dev/pts/    |\n ---       +--------------+               ---       +--------------+\nThis is possible because the redirections are set up by the shell before the commands are executed, and the commands inherit the file descriptors.",
    "crumbs": [
      "Bash essentials"
    ]
  },
  {
    "objectID": "notes/bash-essentials/index.html#core-utilities",
    "href": "notes/bash-essentials/index.html#core-utilities",
    "title": "Bash essentials",
    "section": "5 Core utilities",
    "text": "5 Core utilities\n\n5.1 ssh\nNAME\n  ssh - OpenSSH remote login client\n\nSYNOPSIS\n  ssh [-l login_name] [-p port] DESTINATION [command [argument...]\nssh is a program for logging into a remote machine and for executing commands on a remote machine. It is intended to provide secure encrypted communications between two untrusted hosts over an insecure network.\nssh connects and logs into the specified destination, which may be specified as either [user@]hostname or a URI of the form ssh://[user@]hostname[:port].\nIf a command is specified, it will be executed on the remote host instead of a login shell.\n\n\n\n\n\n\nssh details & examples\n\n\n\n\n\n\nThere’s nothing here yet… stay tuned!\n\n\n\n\n\n5.2 rsync\nNAME\n  rsync - a fast, versatile, remote (and local) file-copying tool\n\nSYNOPSIS\n  Local:\n    rsync [OPTION...] SRC... [DEST]\n  Access via remote shell:\n    Pull:\n        rsync [OPTION...] [USER@]HOST:SRC... [DEST]\n    Push:\n        rsync [OPTION...] SRC... [USER@]HOST:DEST\nRsync is a fast and extraordinarily versatile file copying tool. It can copy locally, to/from another host over any remote shell, or to/from a remote rsync daemon.\nIt is famous for sending only the differences between the source files and the existing files in the destination, increasing efficiency for repetitive synchronization between source and destination.\nRsync is widely used for backups and mirroring, and as an improved cp command for everyday use.\n\n\n\n\n\n\nrsync details & examples\n\n\n\n\n\n\nThere’s nothing here yet… stay tuned!\n\n\n\n\n\n5.3 tar, zip, and unzip\nNAME\n  tar - a general archiving utility for creation/extraction/compression and more\n  zip - package and compress files into a ZIP archive\n  unzip - list, test and extract compressed files from a ZIP archive\n\nSYNOPSIS\n  tar --create/--extract [--file ARCHIVE] [OPTIONS] [FILE...]\n  zip [OPTIONS] [ARCHIVE] [FILE...]\n  unzip [ARCHIVE] [-d OUTPUTDIR]\nThe tar, zip, and unzip programs provide the ability to create, extract, and otherwise manipulate archives of files, where an archive of files is simply a file that stores a collection of other files.\n\n\n\n\n\n\ntar, zip, and unzip details & examples\n\n\n\n\n\nThis section was adapted from (“GNU Tar 1.35: 2 Tutorial Introduction to Tar” n.d.).\nThe specific usecases for tar/zip/unzip are similar but vary slightly.\nAll three tools are used for efficient storage, transfer, and backup of collections of files, particularly large files via compression.\n\ntar:\n\ndefault: create/extract an uncompressed archive (.tar) of a collection of files\nwith --gzip/-z: create/extract a compressed archive (.tar.gz) of a collection of files\nwith --bzip2/-j: create/extract a compressed archive (.tar.bz2) of a collection of files\n\nzip:\n\ncreate a compressed collection of files (.zip)\n\nunzip:\n\nextract a compressed collection of files (.zip)\n\n\n\n5.3.1 Operations\nThere are two main operations of interest for archiving programs:\n\ncreate: create a new archive (.zip, .tar, .tar.gz, tar.bz2)\nextract: extract the files of an archive to a directory\n\nExamples of each follow below:\n\nCreateExtract\n\n\n# Assume you have a directory called music/ and three folders inside it:\n$ tree music\nmusic/\n├── blues\n│   └── nina-simone\n├── folk\n│   └── phil-ochs\n└── jazz\n    └── charles-mingus\n\n# Create an uncompressed archive (.tar) of all three files\n$ tar --create --file=collection.tar music\n\n# Creates a compressed archive (.zip, .tar.gz, .tar.bz2)\n$ zip -r collection.zip music\n$ tar --create --gzip --file=collection.tar.gz music\n$ tar --create --bzip2 --file=collection.tar.bz2 music\n\n# tar has shorthand versions of the above parameters\n$ tar -c -f collection.tar music\n$ tar -c -z -f collection.tar.gz music\n$ tar -cjf collection.tar.bz2 music\n\n\n\n# Assume you have the archives from the Create example:\n$ tar --list collection.tar\nmusic/\n├── blues\n│   └── nina-simone\n├── folk\n│   └── phil-ochs\n└── jazz\n    └── charles-mingus\n\n# Extract all files from an uncompressed archive (.tar) to the current directory\n$ tar --extract --file=collection.tar\n\n# Extract all files from a compressed archive (.zip, .tar.gz, .tar.bz2) to the current directory\n$ unzip collection.zip\n$ tar --extract --gzip --file=collection.tar.gz\n$ tar --extract --bzip2 --file=collection.tar.bz2\n\n# Extract all files from a compressed archive, specifying a different output directory\n$ unzip collection.zip -d ~/some-folder\n$ tar --extract --gzip --file=collection.tar.gz --directory ~/music\n$ tar --extract --bzip2 --file=collection.tar.bz2 --directory /tmp/music\n\n# tar has shorthand versions of the above parameters\n$ tar -x -f collection.tar\n$ tar -x -z -f collection.tar.gz -C ~/music\n$ tar -xjf collection.tar.bz2 -C /tmp/music\n\n\n\nEach of these operations is mutually exclusive, which makes some sense. You cannot create and extract an archive at the same time, that doesn’t make sense!\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen using --create or -c, tar will overwrite current contents of the file named by -f. To add files to an existing archive, you need to use --append or -r.\n\nYou can read more:\n\nthe usecases and history of tar at (gnu.org)\nA helpful comparison between tar and zip (stackoverflow)\nA reallyy thorough breakdown of compression in tar and zip (stackoverflow)\n\n\n\n5.4 git\nNAME\n  git - the stupid content tracker\n\nSYNOPSIS\n  git &lt;command&gt; [&lt;args&gt;]\nGit is a fast, scalable, distributed revision control system with an unusually rich command set that provides both high-level operations and full access to internals.\nSee man 7 gittutorial to get started, then see man 7 giteveryday for a useful minimum set of commands.\n\n\n\n\n\n\ngit details & examples\n\n\n\n\n\n\nThere’s nothing here yet… stay tuned!",
    "crumbs": [
      "Bash essentials"
    ]
  },
  {
    "objectID": "notes/bash-essentials/index.html#more-resources",
    "href": "notes/bash-essentials/index.html#more-resources",
    "title": "Bash essentials",
    "section": "6 More resources",
    "text": "6 More resources\n\nLinuxCommand.org\n\nShort guides on learning bash shell and bash scripting.\nLinks to interactive learning games under “Adventures”. Basic Shell Features\nComplete reference with examples.",
    "crumbs": [
      "Bash essentials"
    ]
  },
  {
    "objectID": "notes/index.html",
    "href": "notes/index.html",
    "title": "All course notes",
    "section": "",
    "text": "Default viewTable viewGrid view\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Modified - Oldest\n        \n         \n          Modified - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nreTerminal built-in devices\n\n\nInstalling initial reTerminal packages. Reading and controlling the reTerminal hardware interfaces, e.g. touchscreen, LEDs, light sensor, screen backlight, etc.\n\n\n\nHardware\n\n\n\n\n\n\nMar 10, 2024\n\n\n5 min\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPython package management\n\n\nHow to use OS package managers, virtual environments, and pip to manage python applications effectively.\n\n\n\nPython\n\n\n\n\n\n\nMar 10, 2024\n\n\n12 min\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nGPIOs: Inputs and Outputs\n\n\nUnderstanding the General Purpose Input and Output (GPIO) pins our reTerminal makes available to us. Voltage, digital vs. analog, python libraries for GPIO.\n\n\n\nHardware\n\n\n\n\n\n\nMar 14, 2024\n\n\n4 min\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nSignals: Analog and digital\n\n\nComparing the two paradigms for measuring electronic information and understanding how each are used for distributed applications.\n\n\n\nHardware\n\n\n\n\n\n\nMar 14, 2024\n\n\n9 min\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nSerial protocols\n\n\nA deeper dive into the protocols used to communicate between GPIO and the kernel\n\n\n\n\n\nMar 14, 2024\n\n\n10 min\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAzure SDKs in Python and C#\n\n\nAzure Libraries in Python and C# Installation instructions for WSL, how to use the libraries in your code. \n\n\n\n\n\nApr 21, 2024\n\n\n3 min\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCourse hardware\n\n\nList of hardware available for the course as well as their respective specifications and documentation. \n\n\n\nlab-3\n\n\n\n\n\n\nJan 20, 2025\n\n\n3 min\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDeveloper environment setup\n\n\nInstructions for setting up classroom and personal computers for programming in this course, including: setting up a Debian container on WSL; restoring your WSL container from backup quickly; installing needed dependencies on WSL, Mac/OSX, and Linux; verifying that needed dependencies have been installed. \n\n\n\nlab-0\n\n\nlab-1\n\n\n\n\n\n\nJan 20, 2025\n\n\n7 min\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBash essentials\n\n\nTips for using the bash interactive shell CLI effectively. The core bash commands everyone should know. How to combine commands using pipes and redirects. \n\n\n\nbash\n\n\nlab-0\n\n\nlab-1\n\n\n\n\n\n\nJan 31, 2025\n\n\n8 min\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nGitHub basics\n\n\nBasics of cloning a repository hosted on Github and creating branches. Using a personal access token for authentication on GitHub outside of VSCode. Using the pass password manager to store access tokens safely. \n\n\n\ngit\n\n\nlab-1\n\n\n\n\n\n\nFeb 10, 2025\n\n\n6 min\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBash scripting\n\n\nTips for scripting in bash effectively. Managing environment and permissions in bash. The capabilities and limitations of programming in bash. \n\n\n\nbash\n\n\nlab-2\n\n\n\n\n\n\nFeb 17, 2025\n\n\n21 min\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nReterminal setup\n\n\nGuide to setting up reterminal for the first time. \n\n\n\nhardware\n\n\nlab-3\n\n\n\n\n\n\nFeb 24, 2025\n\n\n18 min\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPython environment management\n\n\nManaging python development and runtime dependencies. Using virtual environments. Configuring python linters and formatters. \n\n\n\npython\n\n\n\n\n\n\nMar 10, 2025\n\n\n5 min\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPython project management\n\n\nPreviously, we have focused on installing dependencies from other projects – we expand on that knowledge to learn how to create our own projects\n\n\n\nPython\n\n\n\n\n\n\nApr 4, 2025\n\n\n4 min\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPython scripting\n\n\nTips and tricks for effective scripting in Python \n\n\n\npython\n\n\n\n\n\n\nApr 4, 2025\n\n\n4 min\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPython collection types and operations\n\n\nUseful python collection types and operations\n\n\n\nPython\n\n\nCollections\n\n\n\n\n\n\nApr 4, 2025\n\n\n5 min\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAsynchronous progamming in Python\n\n\nUsing the asyncio library to control concurrentprocesses in Python.\n\n\n\n\n\nApr 4, 2025\n\n\n4 min\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPython OOP\n\n\nBasics of Object Oriented Programming in Python.\n\n\n\nPython\n\n\nA1\n\n\n\n\n\n\nApr 4, 2025\n\n\n7 min\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDebugging Reterminal Issues\n\n\nSometimes, the reTerminal display drivers fail. Here is an (incomplete) set of debugging steps you can take to try fixing the issue. \n\n\n\n\n\nApr 13, 2025\n\n\n4 min\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAzure Portal Setup\n\n\nCreating an Azure account, managing your Azure account budget, Azure IoT Hub documentation, creating an Azure IoT Hub resource, and more. \n\n\n\n\n\nApr 19, 2025\n\n\n6 min\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAzure Features\n\n\nAn overview of the main feautres of Azure we will use for implementing an IoT system. Background information on “Platform as a Service” (PaaS), Azure IoT Hub vs IoT Central, Azure Resource Groups, Intro to Azure Dev Tools (Azure Portal, Azure CLI). \n\n\n\n\n\nApr 19, 2025\n\n\n7 min\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAzure CLI Cheatsheet\n\n\nReference for common Azure commands\n\n\n\n\n\nApr 21, 2025\n\n\n4 min\n\n\nApr 22, 2025\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Modified - Oldest\n        \n         \n          Modified - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nSubtitle\n\n\nModified\n\n\n\n\n\n\nMar 10\n\n\nreTerminal built-in devices\n\n\nInstalling initial reTerminal packages. Reading and controlling the reTerminal hardware interfaces, e.g. touchscreen, LEDs, light sensor, screen backlight, etc.\n\n\nApr 22\n\n\n\n\nMar 10\n\n\nPython package management\n\n\nHow to use OS package managers, virtual environments, and pip to manage python applications effectively.\n\n\nApr 22\n\n\n\n\nMar 14\n\n\nGPIOs: Inputs and Outputs\n\n\nUnderstanding the General Purpose Input and Output (GPIO) pins our reTerminal makes available to us. Voltage, digital vs. analog, python libraries for GPIO.\n\n\nApr 22\n\n\n\n\nMar 14\n\n\nSignals: Analog and digital\n\n\nComparing the two paradigms for measuring electronic information and understanding how each are used for distributed applications.\n\n\nApr 22\n\n\n\n\nMar 14\n\n\nSerial protocols\n\n\nA deeper dive into the protocols used to communicate between GPIO and the kernel\n\n\nApr 22\n\n\n\n\nApr 21\n\n\nAzure SDKs in Python and C#\n\n\nAzure Libraries in Python and C# Installation instructions for WSL, how to use the libraries in your code. \n\n\nApr 22\n\n\n\n\nJan 20\n\n\nCourse hardware\n\n\nList of hardware available for the course as well as their respective specifications and documentation. \n\n\nApr 22\n\n\n\n\nJan 20\n\n\nDeveloper environment setup\n\n\nInstructions for setting up classroom and personal computers for programming in this course, including: setting up a Debian container on WSL; restoring your WSL container from backup quickly; installing needed dependencies on WSL, Mac/OSX, and Linux; verifying that needed dependencies have been installed. \n\n\nApr 22\n\n\n\n\nJan 31\n\n\nBash essentials\n\n\nTips for using the bash interactive shell CLI effectively. The core bash commands everyone should know. How to combine commands using pipes and redirects. \n\n\nApr 22\n\n\n\n\nFeb 10\n\n\nGitHub basics\n\n\nBasics of cloning a repository hosted on Github and creating branches. Using a personal access token for authentication on GitHub outside of VSCode. Using the pass password manager to store access tokens safely. \n\n\nApr 22\n\n\n\n\nFeb 17\n\n\nBash scripting\n\n\nTips for scripting in bash effectively. Managing environment and permissions in bash. The capabilities and limitations of programming in bash. \n\n\nApr 22\n\n\n\n\nFeb 24\n\n\nReterminal setup\n\n\nGuide to setting up reterminal for the first time. \n\n\nApr 22\n\n\n\n\nMar 10\n\n\nPython environment management\n\n\nManaging python development and runtime dependencies. Using virtual environments. Configuring python linters and formatters. \n\n\nApr 22\n\n\n\n\nApr 4\n\n\nPython project management\n\n\nPreviously, we have focused on installing dependencies from other projects – we expand on that knowledge to learn how to create our own projects\n\n\nApr 22\n\n\n\n\nApr 4\n\n\nPython scripting\n\n\nTips and tricks for effective scripting in Python \n\n\nApr 22\n\n\n\n\nApr 4\n\n\nPython collection types and operations\n\n\nUseful python collection types and operations\n\n\nApr 22\n\n\n\n\nApr 4\n\n\nAsynchronous progamming in Python\n\n\nUsing the asyncio library to control concurrentprocesses in Python.\n\n\nApr 22\n\n\n\n\nApr 4\n\n\nPython OOP\n\n\nBasics of Object Oriented Programming in Python.\n\n\nApr 22\n\n\n\n\nApr 13\n\n\nDebugging Reterminal Issues\n\n\nSometimes, the reTerminal display drivers fail. Here is an (incomplete) set of debugging steps you can take to try fixing the issue. \n\n\nApr 22\n\n\n\n\nApr 19\n\n\nAzure Portal Setup\n\n\nCreating an Azure account, managing your Azure account budget, Azure IoT Hub documentation, creating an Azure IoT Hub resource, and more. \n\n\nApr 22\n\n\n\n\nApr 19\n\n\nAzure Features\n\n\nAn overview of the main feautres of Azure we will use for implementing an IoT system. Background information on “Platform as a Service” (PaaS), Azure IoT Hub vs IoT Central, Azure Resource Groups, Intro to Azure Dev Tools (Azure Portal, Azure CLI). \n\n\nApr 22\n\n\n\n\nApr 21\n\n\nAzure CLI Cheatsheet\n\n\nReference for common Azure commands\n\n\nApr 22\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Modified - Oldest\n        \n         \n          Modified - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nreTerminal built-in devices\n\n\nInstalling initial reTerminal packages. Reading and controlling the reTerminal hardware interfaces, e.g. touchscreen, LEDs, light sensor, screen backlight, etc.\n\n\n5 min\n\n\n\nHardware\n\n\n\n\nMar 10, 2024\n\n\n\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\nPython package management\n\n\nHow to use OS package managers, virtual environments, and pip to manage python applications effectively.\n\n\n12 min\n\n\n\nPython\n\n\n\n\nMar 10, 2024\n\n\n\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\nGPIOs: Inputs and Outputs\n\n\nUnderstanding the General Purpose Input and Output (GPIO) pins our reTerminal makes available to us. Voltage, digital vs. analog, python libraries for GPIO.\n\n\n4 min\n\n\n\nHardware\n\n\n\n\nMar 14, 2024\n\n\n\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\nSignals: Analog and digital\n\n\nComparing the two paradigms for measuring electronic information and understanding how each are used for distributed applications.\n\n\n9 min\n\n\n\nHardware\n\n\n\n\nMar 14, 2024\n\n\n\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\nSerial protocols\n\n\nA deeper dive into the protocols used to communicate between GPIO and the kernel\n\n\n10 min\n\n\n\nMar 14, 2024\n\n\n\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\nAzure SDKs in Python and C#\n\n\nAzure Libraries in Python and C# Installation instructions for WSL, how to use the libraries in your code.\n\n\n3 min\n\n\n\nApr 21, 2024\n\n\n\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nCourse hardware\n\n\nList of hardware available for the course as well as their respective specifications and documentation.\n\n\n3 min\n\n\n\nlab-3\n\n\n\n\nJan 20, 2025\n\n\n\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\nDeveloper environment setup\n\n\nInstructions for setting up classroom and personal computers for programming in this course, including: setting up a Debian container on WSL; restoring your WSL container from backup quickly; installing needed dependencies on WSL, Mac/OSX, and Linux; verifying that needed dependencies have been installed.\n\n\n7 min\n\n\n\nlab-0\n\n\nlab-1\n\n\n\n\nJan 20, 2025\n\n\n\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nBash essentials\n\n\nTips for using the bash interactive shell CLI effectively. The core bash commands everyone should know. How to combine commands using pipes and redirects.\n\n\n8 min\n\n\n\nbash\n\n\nlab-0\n\n\nlab-1\n\n\n\n\nJan 31, 2025\n\n\n\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\nGitHub basics\n\n\nBasics of cloning a repository hosted on Github and creating branches. Using a personal access token for authentication on GitHub outside of VSCode. Using the pass password manager to store access tokens safely.\n\n\n6 min\n\n\n\ngit\n\n\nlab-1\n\n\n\n\nFeb 10, 2025\n\n\n\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nBash scripting\n\n\nTips for scripting in bash effectively. Managing environment and permissions in bash. The capabilities and limitations of programming in bash.\n\n\n21 min\n\n\n\nbash\n\n\nlab-2\n\n\n\n\nFeb 17, 2025\n\n\n\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nReterminal setup\n\n\nGuide to setting up reterminal for the first time.\n\n\n18 min\n\n\n\nhardware\n\n\nlab-3\n\n\n\n\nFeb 24, 2025\n\n\n\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\nPython environment management\n\n\nManaging python development and runtime dependencies. Using virtual environments. Configuring python linters and formatters.\n\n\n5 min\n\n\n\npython\n\n\n\n\nMar 10, 2025\n\n\n\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\nPython project management\n\n\nPreviously, we have focused on installing dependencies from other projects – we expand on that knowledge to learn how to create our own projects\n\n\n4 min\n\n\n\nPython\n\n\n\n\nApr 4, 2025\n\n\n\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\nPython scripting\n\n\nTips and tricks for effective scripting in Python\n\n\n4 min\n\n\n\npython\n\n\n\n\nApr 4, 2025\n\n\n\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\nPython collection types and operations\n\n\nUseful python collection types and operations\n\n\n5 min\n\n\n\nPython\n\n\nCollections\n\n\n\n\nApr 4, 2025\n\n\n\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\nAsynchronous progamming in Python\n\n\nUsing the asyncio library to control concurrentprocesses in Python.\n\n\n4 min\n\n\n\nApr 4, 2025\n\n\n\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\nPython OOP\n\n\nBasics of Object Oriented Programming in Python.\n\n\n7 min\n\n\n\nPython\n\n\nA1\n\n\n\n\nApr 4, 2025\n\n\n\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\nDebugging Reterminal Issues\n\n\nSometimes, the reTerminal display drivers fail. Here is an (incomplete) set of debugging steps you can take to try fixing the issue.\n\n\n4 min\n\n\n\nApr 13, 2025\n\n\n\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\nAzure Portal Setup\n\n\nCreating an Azure account, managing your Azure account budget, Azure IoT Hub documentation, creating an Azure IoT Hub resource, and more.\n\n\n6 min\n\n\n\nApr 19, 2025\n\n\n\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\nAzure Features\n\n\nAn overview of the main feautres of Azure we will use for implementing an IoT system. Background information on “Platform as a Service” (PaaS), Azure IoT Hub vs IoT Central, Azure Resource Groups, Intro to Azure Dev Tools (Azure Portal, Azure CLI).\n\n\n7 min\n\n\n\nApr 19, 2025\n\n\n\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\nAzure CLI Cheatsheet\n\n\nReference for common Azure commands\n\n\n4 min\n\n\n\nApr 21, 2025\n\n\n\n\n\nApr 22, 2025\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\n\n Back to topReuseCC BY-NC 4.0. ©2022-25 Mauricio Buschinelli & Michael Haaf.\n(View License)",
    "crumbs": [
      "All course notes"
    ]
  },
  {
    "objectID": "notes/azure-portal/index.html",
    "href": "notes/azure-portal/index.html",
    "title": "Azure Portal Setup",
    "section": "",
    "text": "Image source",
    "crumbs": [
      "Azure Portal Setup"
    ]
  },
  {
    "objectID": "notes/azure-portal/index.html#getting-started-with-azure",
    "href": "notes/azure-portal/index.html#getting-started-with-azure",
    "title": "Azure Portal Setup",
    "section": "1 Getting started with Azure",
    "text": "1 Getting started with Azure\nThese notes cover the basics for setting up your Azure account, and the main Azure resource we will use in class, the IoT Hub.\n\n1.1 Creating an account\nIf you have not already, create an Azure account using your student email.\nOtherwise, log in to your azure account at https://portal.azure.com\n\n\n1.2 Account cost overview\nThe Azure services we need for this course (creating an IoTHub Resource group and sending messages) are free to use, within a daily message limit (~8000).\nThis means that we need to take care to either use student credits, or ensure we have a budget-warning system in place, to prevent incurring unnecessary costs.\nNote that John Abbott students get $100 of credit for 12 months. After 12 months, or when your student credit runs out, you have two options:\n\nupgrade your account to “pay-as-you-go”\n\nPay-as-you-go is will still be free for our course, as long as you set up a budget alert in case you exceed the free-usage-limits\n\ncreate a new non-student account using a non-student email address, comes with $200 credit\n\nIn the big picture, no matter what your situation is, your Azure plan should have the following properties:\n\nNo upfront costs – your account should be free to create, and every service/resource you create should be free to create.\nNo monthly costs – your costs should be covered entirely by one of the following:\n\nyour student account credit\n$200 free credit for\npay-as-you-go that is free per month, but will charge your credit card per usage beyond the threshold of free tools.\n\nA budget alert system that warns you when your usage exceeds &gt;$5 . Even if you are in situation 2.3 (pay-as-you-go free services), you should not expect to pay more than a couple dollars. See the Setting budget alerts section.\n\nSee the FAQ section if you have further questions.",
    "crumbs": [
      "Azure Portal Setup"
    ]
  },
  {
    "objectID": "notes/azure-portal/index.html#managing-your-account",
    "href": "notes/azure-portal/index.html#managing-your-account",
    "title": "Azure Portal Setup",
    "section": "2 Managing your account",
    "text": "2 Managing your account\nBelow are some useful account management how-tos.\n\n2.1 Checking your balance\nAs you experiment with Azure you might want to know your current balance.\n\nSign in to the Azure portal\nSearch for Cost Management\nSelect Payment methods\nCheck your Azure credits panel.\n\n\n\n\n\n\nCredit check panel in Azure\n\n\nFor more details, see Track Microsoft Customer Agreement Azure credit balance\n\n\n2.2 Setting budget alerts\nIt’s possible to setup email alerts when your Azure spending exceeds a certain threshold. This avoids unexpected spending of Azure credits.\n\nIn the Azure Portal Home, select the Resource groups service.\nSelect the resource group you want to monitor with a budget.\nIn the left panel, select Budgets under the Cost Management group and select Add.\nGive your budget a name and an amount.\nSelect the percent spending for the alert and the email to receive the notification.\nClick Create.\n\n\n\n\n\n\nMenu for creating a budget for a resource group.",
    "crumbs": [
      "Azure Portal Setup"
    ]
  },
  {
    "objectID": "notes/azure-portal/index.html#using-the-tool",
    "href": "notes/azure-portal/index.html#using-the-tool",
    "title": "Azure Portal Setup",
    "section": "3 Using the tool",
    "text": "3 Using the tool\nBelow are some how-tos for services we will be using in this class.\n\n3.1 Documentation\nThe Azure resource we will be using the most, IoT Hub, has complete documentation/how-to-guides/tutorials at https://learn.microsoft.com/en-us/azure/iot-hub/.\n\nFamiliarize yourself with the left-hand menu, including the “Quickstarts” and “How-to guides”.\n\n\n3.2 Creating an IoT Hub Resource\nThere are two ways to create resources in Azure:\n\nusing the Azure portal website: https://portal.azure.com\nusing Azure CLI\n\nThe Azure documentation typically has instructions for both methods in all their tutorials – you can select which method you want to choose by clicking the tabset button for it:\n\nAzure PortalAzure CLI\n\n\nCreate and manage Azure IoT Hubs using Azure Portal.\nNote that each section has the “Azure Portal” option selected.\n\n\nCreate and manage Azure IoT Hubs using Azure CLI.\nNote that each section has an “Azure CLI” option selected.\n\n\n\nNo matter which method you choose, you should make sure the resource you’re making is free and in a nearby region.\nSuggested choices for the following options:\n\nSubscription: Your active subscription (probably “Azure for Students”)\nResource group: Create ONE resource group for this class (any name is fine)\nIoT hub name: Recommended: yourname-iot-hub\nRegion: Central US or East US or Canada East\nTier: Free\nDaily message limit: 8,000 ($0/month)\n\nYou can keep the defaults for all subsequent menus (Networking, Management, Add-ons, Tags).\nOnce you’re done creating your IoT Hub, it should appear in your Azure portal. See the figure below:\n\n\n\n\n\n\nFigure 1: In this figure, the Resource Group is named 6P3, and a IoT Hub Resource named 6P3-IoT-Hub can be seen in the list of its resources. You can view your resource groups on Azure Portal by clicking on the “Resource groups” menu item on the left.",
    "crumbs": [
      "Azure Portal Setup"
    ]
  },
  {
    "objectID": "notes/azure-portal/index.html#faq",
    "href": "notes/azure-portal/index.html#faq",
    "title": "Azure Portal Setup",
    "section": "4 FAQ",
    "text": "4 FAQ\n\nQ: I’m not eligible for a student account?\n\nStudent accounts last for 12 months – if you have taken courses in the past that require an Azure account, and it was more than 12 months since you created that account, your student credits have unfortunately expired.\nSee What happens after I use my $100 credit or when I’m at the end of 12 months?\n\nQ: I’m not eligible for the $200 free account credit for my non-student account?\n\nThe $200 credit is only available if you have not used that email address for an Azure account in the past (and it only lasts for one month).\nIf you are not eligible for the $200 credit on the email address you have provided, you have two choices, both of which are fine:\n\nuse a new email address\nregister for a free-tier pay-as-you-go.\n\n\nQ: I tried to register for the pay-as-you-go and I’m told it costs $35 per month?\n\nThe pay-as-you-go account should not cost any upfront or monthly charges. Double check that you do not select options like “tech support plans” or “standard-tier service” – ALWAYS opt out of any paid options, ALWAYS select free-tier for any choices provided to you by Azure.\n\nQ: if I’m on free-tier and there’s no monthly costs, why do I need to monitor the budget?\n\nThe pay-as-you-go account may charge your credit card, only when you use the service beyond the defined free-tier limits.\nIn our class, we will have IoT Hub services that handle communication between your Mobile App from App Dev III, and your reTerminal devices from Connected Objects. The free-tier limit for the IoTHub is 8000 messages per day – I don’t think there will be a reason to exceed this, so by default, you may never have to pay any money for this service.\nHowever, there may be small charges to your credit card, ONLY IF you exceed the free-tier limits of the services we use. These charges will be small, but it is possible for them to get out-of-hand by accident. To make sure they do not get out of hand by accident, you must set up a Budget for your Resource Group to alert you when charges exceed a certain number. Please review the course notes about budgets and set up an alert if you have not done so.\n\nQ: I have some other question?\n\nPlease let me know if you have any questions, or if there’s something else that doesn’t seem to work. I will update these instructions if there are more troubles you run into that I can share answers for.",
    "crumbs": [
      "Azure Portal Setup"
    ]
  },
  {
    "objectID": "about/syllabus/index.html",
    "href": "about/syllabus/index.html",
    "title": "Syllabus",
    "section": "",
    "text": "Room, times, etc.: see Course Outline\nOffice: Penfield 311\nOffice hours: Mon/Fri 11:00 - 12:30pm or so\nEmail: michael DOT haaf AT johnabbott DOT qc DOT ca\nCourse webpage: This website for all content, Moodle for assignment/project/quiz submissions.\n\n\n\n\n\n\nForgive me the smarmy comic, this is usually true…\n\n\n\n\n\nTeams - For communicating (fastest, most reliable that I will check it same-day)\nMIO - For communicating (non-time-critical)\nEmail - For communicating (time-critical). Will aim for &lt;24hr response.\nMoodle - For receiving & submitting exercises/assignments/project and getting marks\n\n\n\n\n\n45% Assignments and Labs\n20% Test (first week of April)\n35% Project Milestones"
  },
  {
    "objectID": "about/syllabus/index.html#logistics",
    "href": "about/syllabus/index.html#logistics",
    "title": "Syllabus",
    "section": "",
    "text": "Room, times, etc.: see Course Outline\nOffice: Penfield 311\nOffice hours: Mon/Fri 11:00 - 12:30pm or so\nEmail: michael DOT haaf AT johnabbott DOT qc DOT ca\nCourse webpage: This website for all content, Moodle for assignment/project/quiz submissions.\n\n\n\n\n\n\nForgive me the smarmy comic, this is usually true…\n\n\n\n\n\nTeams - For communicating (fastest, most reliable that I will check it same-day)\nMIO - For communicating (non-time-critical)\nEmail - For communicating (time-critical). Will aim for &lt;24hr response.\nMoodle - For receiving & submitting exercises/assignments/project and getting marks\n\n\n\n\n\n45% Assignments and Labs\n20% Test (first week of April)\n35% Project Milestones"
  },
  {
    "objectID": "about/syllabus/index.html#course-material",
    "href": "about/syllabus/index.html#course-material",
    "title": "Syllabus",
    "section": "2 Course material",
    "text": "2 Course material\nThere are no required textbooks for this course – this website will contain all of the content that you need to complete course deliverables. Additionally, each lecture will contain references to additional resources for exploring each topic in further detail beyond the scope of the course."
  },
  {
    "objectID": "about/this-site/index.html",
    "href": "about/this-site/index.html",
    "title": "About this website",
    "section": "",
    "text": "Course content, both for lectures and assignments, has been adapted by me from a variety of sources. This section serves the purpose of both acknowledging these references as well as pointing the way for curious students to begin investigating further into course material than we had time to cover.\n\n\nAll course content was either written by Mauricio Buschinelli, Michael Haaf, or explicitly adapted from external resources with attributions made clear. This course content retains the licenses of the original works where relevant, and is otherwise licensed under the Creative Commons Attribution 4.0 International License.\n\nTerms of use\nYou are free to:\n\nShare: copy and redistribute the material in any medium or format\nAdapt: remix, transform, and build upon the material for any purpose, even commercially.\n\nUnder the following terms:\n\nAttribution: You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.\n\nCreative Commons Attribution 4.0 International License"
  },
  {
    "objectID": "about/this-site/index.html#course-content",
    "href": "about/this-site/index.html#course-content",
    "title": "About this website",
    "section": "",
    "text": "Course content, both for lectures and assignments, has been adapted by me from a variety of sources. This section serves the purpose of both acknowledging these references as well as pointing the way for curious students to begin investigating further into course material than we had time to cover.\n\n\nAll course content was either written by Mauricio Buschinelli, Michael Haaf, or explicitly adapted from external resources with attributions made clear. This course content retains the licenses of the original works where relevant, and is otherwise licensed under the Creative Commons Attribution 4.0 International License.\n\nTerms of use\nYou are free to:\n\nShare: copy and redistribute the material in any medium or format\nAdapt: remix, transform, and build upon the material for any purpose, even commercially.\n\nUnder the following terms:\n\nAttribution: You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.\n\nCreative Commons Attribution 4.0 International License"
  },
  {
    "objectID": "about/this-site/index.html#course-website",
    "href": "about/this-site/index.html#course-website",
    "title": "About this website",
    "section": "2 Course website",
    "text": "2 Course website\nDetails about how this website was built follow.\n\n2.1 Colophon\n\n\n\n\n\nFrom Wikipedia: In publishing, a colophon is a brief statement containing information about the publication of a book… Some web pages also have colophons, which frequently contain (X)HTML, CSS, or usability standards compliance information and links to website validation tests.\n\n\n\nMarkup: Markdown\nFramework: Quarto\nDeployment: GitHub Pages\n\nYou can follow along directly with course updates at the course repository."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "This companion site for the 420-6P3 course includes:\n\n\nLecture slides / documents\nExercises\nReferences & Resources\n\n\nWebsite accessible at john-abbott-college.github.io/6P3-Notes\n\n\nThis course will introduce students to the principles of the Internet of Things (IoT). Students will use Linux and Python to program a Raspberry Pi in order to read data from sensors and control actuators.\nBy the end of the course, students build and deploy an IoT product that is securely connected to the Azure ecosystem in order to collect and analyse telemetry data as well as respond to remote commands.\n\n\n\n\nExperience using Object Oriented Programming to create applications in any language.\nFamiliarity using the Bash shell and basic Linux CLI.\nFamiliarity with Python.\nA Raspberry Pi with various sensors and actuators (see page Hardware List).\nA Microsoft Azure account: create a free Azure for Students account if necessary.\nVS Code configured for Python programming and remote development (see page Dev Setup).\n\n\n\n\nThis webpage is written in Markdown using the Quarto framework. The website is hosted via GitHub Pages\nSource code is open source and available on GitHub.\n\n\nCreate a local copy of these notes:\n\nInstall Quarto for your system\nClone the course GitHub repository.\nPreview the website:\nquarto preview .\n\n\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License. See the Copyright statement on the course webpage."
  },
  {
    "objectID": "index.html#description",
    "href": "index.html#description",
    "title": "Home",
    "section": "",
    "text": "This course will introduce students to the principles of the Internet of Things (IoT). Students will use Linux and Python to program a Raspberry Pi in order to read data from sensors and control actuators.\nBy the end of the course, students build and deploy an IoT product that is securely connected to the Azure ecosystem in order to collect and analyse telemetry data as well as respond to remote commands."
  },
  {
    "objectID": "index.html#requirements",
    "href": "index.html#requirements",
    "title": "Home",
    "section": "",
    "text": "Experience using Object Oriented Programming to create applications in any language.\nFamiliarity using the Bash shell and basic Linux CLI.\nFamiliarity with Python.\nA Raspberry Pi with various sensors and actuators (see page Hardware List).\nA Microsoft Azure account: create a free Azure for Students account if necessary.\nVS Code configured for Python programming and remote development (see page Dev Setup)."
  },
  {
    "objectID": "index.html#source-code",
    "href": "index.html#source-code",
    "title": "Home",
    "section": "",
    "text": "This webpage is written in Markdown using the Quarto framework. The website is hosted via GitHub Pages\nSource code is open source and available on GitHub.\n\n\nCreate a local copy of these notes:\n\nInstall Quarto for your system\nClone the course GitHub repository.\nPreview the website:\nquarto preview .\n\n\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License. See the Copyright statement on the course webpage."
  },
  {
    "objectID": "about/calendar/index.html",
    "href": "about/calendar/index.html",
    "title": "Calendar",
    "section": "",
    "text": "This is the authoritative calendar for content covered in class, and for content you should expect to see covered as we continue through the course.\nYou can compare this calendar with the calendar from the Course Outline to see where this course has deviated from the original outline."
  },
  {
    "objectID": "about/calendar/index.html#deliverables",
    "href": "about/calendar/index.html#deliverables",
    "title": "Calendar",
    "section": "1 Deliverables",
    "text": "1 Deliverables\n\nLab 0: dev env setup and bash review: Due January 31 (Demo: in-class. Code: end of day 11:59pm)\nLab 1: git good, bash better: Due February 10 end of day 11:59pm\nLab 2: Reusing functions in bash: Due February 24 end of day 11:59pm"
  },
  {
    "objectID": "about/calendar/index.html#lectures",
    "href": "about/calendar/index.html#lectures",
    "title": "Calendar",
    "section": "2 Lectures",
    "text": "2 Lectures\n\nJan 20: Introduction to course\nJan 24: Setting up developer environment\nJan 27: Review bash, begin Lab 0\nJan 31: Work on Lab 0\nFeb 3: Begin Lab 1\nFeb 7: (class cancelled, continue lab 1)\nFeb 10: Finish Lab 1\nFeb 14: Start Lab 2\nFeb 17: Class cancelled due to storm\nFeb 21: Continue work on Lab 2\nFeb 24: Begin Lab 3"
  },
  {
    "objectID": "notes/azure-sdks/index.html",
    "href": "notes/azure-sdks/index.html",
    "title": "Azure SDKs in Python and C#",
    "section": "",
    "text": "Azure offers Software Development Kits (SDKs) for most of its services. Below you will find references to the SDKs used in the course.",
    "crumbs": [
      "Azure SDKs in Python and C#"
    ]
  },
  {
    "objectID": "notes/azure-sdks/index.html#azure-iot-sdks",
    "href": "notes/azure-sdks/index.html#azure-iot-sdks",
    "title": "Azure SDKs in Python and C#",
    "section": "1 Azure IoT SDKs",
    "text": "1 Azure IoT SDKs\nAzure IoT provides SDKs for several languages such as Python, Java, .NET, C and Node.js.\n\n\n\n\n\n\n\nFigure 1: Azure IoT SDKs provide tooling and examples to develop devices, service applications and to manage the IoT Hub itself.\n\n\nThere are a few categories of SDKs to know, which are managed and packaged separately:\n\nDevice SDKs\n\nFunctionality to build device clients that communicate with and are controlled by an IoT Hub.\n\nService SDKs\n\nFunctionality for applications and services that communicate with an IoT Hub and registered devices. Examples are a back-end service running on a VM or a mobile application.\n\nIoT Hub Management SDKs\n\nHelp build back-end applications that manage an IoT Hub.\n\n\n\n\n\n\n\n\nNote\n\n\n\nRemember that the IoT Hub is only managing the device registration, authentication and routing of messages and data.\nThe application logic and device control is typically done by a back-end service and user applications using data routed via the IoT Hub’s end-points.",
    "crumbs": [
      "Azure SDKs in Python and C#"
    ]
  },
  {
    "objectID": "notes/azure-sdks/index.html#sdks-by-language",
    "href": "notes/azure-sdks/index.html#sdks-by-language",
    "title": "Azure SDKs in Python and C#",
    "section": "2 SDKs by language",
    "text": "2 SDKs by language\n\nPythonC#\n\n\n\nPackages (pip)\n\nDevice: azure-iot-device\n\n\nService: azure-iot-hub\n\n\nIoTHub Management: azure-mgmt-iothub\n\nSource (GitHub)\n\nDevice: azure/azure-iot-sdk-python\n\n\nService: azure/azure-iot-hub-python\n\n\nIoTHub Management: azure/azure-sdk-for-python\n\nTutorials\n\nAll: Connect to IoT Hub (Python)\n\nExamples\n\nDevice: azure-iot-sdk-python/samples\n\n\nService: azure-iot-hub-python/samples\n\nReference\n\nDevice: learn.microsoft.com\n\n\nService: learn.microsoft.com\n\n\nIoTHub Management: learn.microsoft.com\n\n\n\n\n\nPackage (NuGet)\n\nDevice: Microsoft.Azure.Devices.Client\n\n\nService: Microsoft.Azure.Devices\n\n\nIoTHub Management: Azure.ResourceManager.IotHub\n\nSource (GitHub)\n\nDevice/Service: azure/azure-iot-sdk-csharp\n\n\nIoTHub Management: azure/azure-iot-for-net\n\nTutorials\n\nAll: Connect to IoT Hub (C#)\n\nExamples\n\nDevice: azure-iot-sdk-csharp/iothub/device/samples\n\n\nService: azure-iot-sdk-csharp/iothub/service/samples\n\nReference\n\nDevice/Service: learn.microsoft.com\n\n\nIoTHub management: learn.microsoft.com",
    "crumbs": [
      "Azure SDKs in Python and C#"
    ]
  },
  {
    "objectID": "notes/azure-sdks/index.html#installing-dotnet-on-developer-environment",
    "href": "notes/azure-sdks/index.html#installing-dotnet-on-developer-environment",
    "title": "Azure SDKs in Python and C#",
    "section": "3 Installing dotnet on developer environment",
    "text": "3 Installing dotnet on developer environment\n\n3.1 WSL\nThese instructions were adatped from learn.microsoft.com – see the linked document for more details if you run into issues.\n# Update system\n$ sudo apt update && sudo apt upgrade -y\n\n# Install required dependencies\n$ sudo apt install libc6 libgcc-s1 libgssapi-krb5-2 libicu72 libssl3 libstdc++6 zlib1g\n\n# Add dotnet repository to debian package manager\nwget https://packages.microsoft.com/config/debian/12/packages-microsoft-prod.deb -O packages-microsoft-prod.deb\nsudo dpkg -i packages-microsoft-prod.deb\nrm packages-microsoft-prod.deb\n\n# Install .NET 8\nsudo apt update && sudo apt install -y dotnet-sdk-8.0\n\n# Verify .NET8 installed\ndotnet --version\n\n3.1.1 Setup tab completions using .bashrc\nTo get tab completions set up for dotnet, add the following snippet to your .bashrc:\n\nfunction _dotnet_bash_complete()\n{\n  local cur=\"${COMP_WORDS[COMP_CWORD]}\" IFS=$'\\n' # On Windows you may need to use use IFS=$'\\r\\n'\n  local candidates\n  read -d '' -ra candidates &lt; &lt;(dotnet complete --position \"${COMP_POINT}\" \"${COMP_LINE}\" 2&gt;/dev/null)\n  read -d '' -ra COMPREPLY &lt; &lt;(compgen -W \"${candidates[*]:-}\" -- \"$cur\")\n}\n\ncomplete -f -F _dotnet_bash_complete dotnet\nSee learn.miscrosoft.com for more details.\n\n\n\n3.2 OSX\nTry the instructions here https://learn.microsoft.com/en-us/dotnet/core/install/macos\nNOTE: I haven’t tested these instructions myself.",
    "crumbs": [
      "Azure SDKs in Python and C#"
    ]
  },
  {
    "objectID": "notes/azure-features/index.html",
    "href": "notes/azure-features/index.html",
    "title": "Azure Features",
    "section": "",
    "text": "So far in this course, we have developed applications that are almost entirely local to our reTerminal (production) and workstation (development) environments.\nWe did not worry ourselves about data storage, application logic, authentication, etc. We could implement these things locally in Python, but we can also automate many of them using a PaaS framework.\nPlatform as a service (PaaS) is a complete development and deployment environment in the cloud:\n\nInfrastructure: servers, storage, and networking,\nPlatforms: development tools, business intelligence services (BI), database management systems, and more.\n\nIt’s really a reduced version of Software as a service (which you may be more familiar with), allowing the flexibility to write customized application/device software while benefitting from infrastructure and analysis services.\nSee the graphic below:\n\n\n\n\n\n\nFigure 1: Compasion of SaaS, PaaS and IaaS platforms. Figure from: SaaS vs PaaS vs IaaS, Microsoft.\n\n\n\nPaaS is designed to support the complete application life-cycle: building, testing, deploying, managing, and updating – while giving the developer flexibility to implement the actual services themselves.\nSee more in What is PaaS (azure.microsoft.com)",
    "crumbs": [
      "Azure Features"
    ]
  },
  {
    "objectID": "notes/azure-features/index.html#platform-as-a-service-paas",
    "href": "notes/azure-features/index.html#platform-as-a-service-paas",
    "title": "Azure Features",
    "section": "",
    "text": "So far in this course, we have developed applications that are almost entirely local to our reTerminal (production) and workstation (development) environments.\nWe did not worry ourselves about data storage, application logic, authentication, etc. We could implement these things locally in Python, but we can also automate many of them using a PaaS framework.\nPlatform as a service (PaaS) is a complete development and deployment environment in the cloud:\n\nInfrastructure: servers, storage, and networking,\nPlatforms: development tools, business intelligence services (BI), database management systems, and more.\n\nIt’s really a reduced version of Software as a service (which you may be more familiar with), allowing the flexibility to write customized application/device software while benefitting from infrastructure and analysis services.\nSee the graphic below:\n\n\n\n\n\n\nFigure 1: Compasion of SaaS, PaaS and IaaS platforms. Figure from: SaaS vs PaaS vs IaaS, Microsoft.\n\n\n\nPaaS is designed to support the complete application life-cycle: building, testing, deploying, managing, and updating – while giving the developer flexibility to implement the actual services themselves.\nSee more in What is PaaS (azure.microsoft.com)",
    "crumbs": [
      "Azure Features"
    ]
  },
  {
    "objectID": "notes/azure-features/index.html#iot-subsystems",
    "href": "notes/azure-features/index.html#iot-subsystems",
    "title": "Azure Features",
    "section": "2 IoT Subsystems",
    "text": "2 IoT Subsystems\nWe can breakdown an IoT solution into subsystems and explore how information flows between them:\n\nIoT Devices: The physical devices and sensors where data originates.\nCloud Gateway: The Cloud Gateway provides a cloud hub for secure connectivity, telemetry, event ingestion and device management (including command and control) capabilities.\nStream Processing: Processes large streams of data records, evaluates rules for those streams, and further routes the data.\nStorage: Storage can be divided into warm path (data that is required to be available for reporting and visualization immediately from devices), and cold path (data that is stored longer term and used for batch processing).\nUser Interface and Reporting: The user interface for an IoT application can be delivered on a wide array of device types, in native applications, and browsers.\nBusiness Process Integration: Facilitates executing actions based on insights garnered from device telemetry data during stream processing. Integration could include storage of informational messages, alarms, sending email or SMS, integration with CRM, and more.\n\n\n\n\n\n\n\nFigure 2: Diagram that shows the core subsystems of the Azure IoT reference architecture. Figure from: Subsystems of IoT architecture, Microsoft.\n\n\n\nWe will use Microsoft Azure to manage each sub-system individually.",
    "crumbs": [
      "Azure Features"
    ]
  },
  {
    "objectID": "notes/azure-features/index.html#azure-iot-services",
    "href": "notes/azure-features/index.html#azure-iot-services",
    "title": "Azure Features",
    "section": "3 Azure IoT Services",
    "text": "3 Azure IoT Services\nAzure offers many IoT-related services (see image below). In this course we’ll only explore a few.\n\n\n\n\n\n\nFigure 3: Azure IoT-related architecture. Data will flow from to/from your reTerminal (Devices), to/from the Azure IoT Hub (Ingestion & provisioning), to/from Azure Data Management (Warm path), to from your .NET mobile app (M&B integration). Figure from Microsoft Learn.",
    "crumbs": [
      "Azure Features"
    ]
  },
  {
    "objectID": "notes/azure-features/index.html#iot-hub-iot-central",
    "href": "notes/azure-features/index.html#iot-hub-iot-central",
    "title": "Azure Features",
    "section": "4 IoT Hub & IoT Central",
    "text": "4 IoT Hub & IoT Central\nMicrosoft Azure has two service offers for deploying and managing IoT systems: IoT Hub and IoT Central.\n\nThe page Overview: Connection options for Azure IoT device developers offers a great summary and comparison between IoT Hub and IoT Central.\n\n\n4.1 IoT Central\nAzure IoT Central is a software-as-a-service (SaaS) application that provides a complete platform for hosting IoT applications. Its main feature is a web UI that streamlines the lifecycle of creating and managing IoT applications.\nThe web UI simplifies the tasks of creating applications, and connecting and managing from a few up to millions of devices.\nThis service has a limited free plan (2 devices with 5000 messages per month). Pricing available here.\nSee official site for details or watch the walk-through for an example.\n\nWe will not use IoT Central in this course because we’ll connect the underlying Azure services ourselves.\n\n\n\n4.2 IoT Hub\nAzure IoT Hub is a platform-as-a-service (PaaS) application that also provides a platform for hosting IoT applications. IoT Hub acts as a central message hub for bi-directional communication between IoT applications and connected devices.\nIoT Hub offers greater control and customization over your application design, and more developer tool options for working with the service. However, it requires more development time and slightly more management complexity.\nThis service also offers a more generous free plan (500 devices with 8000 messages per day).\nSee official site for details and pricing.\n\nWe will use IoT Hub in this course.",
    "crumbs": [
      "Azure Features"
    ]
  },
  {
    "objectID": "notes/azure-features/index.html#azure-resource-groups-zones",
    "href": "notes/azure-features/index.html#azure-resource-groups-zones",
    "title": "Azure Features",
    "section": "5 Azure Resource Groups & Zones",
    "text": "5 Azure Resource Groups & Zones\nTo use Azure IoT Hub, we need to creaet Azure resource groups.\nAn Azure resource group is a “container” that holds related resources for an Azure solution (such as an IoT Hub).\nThe resource group can include all or a subset of the resources for the solution. You decide how to allocate resources to resource groups based on what makes sense for your organization. Generally, group resources share the same lifecycle (deploy, update, and delete).\n\n5.1 Azure Regions\nAzure resources and their groups must be deployed to a particular region.\nAzure operates in multiple datacenters around the world. These datacenters are grouped in to geographic regions.\n\n\n\n\n\n\nFigure 4: Azure infrastructure map. You can find a current list of all Azure regions at the Zzure geographies web tool. Figure from Map of Azure Regions, Thomas Poppelgaard, 2017.\n\n\n\nCanada has two Azure regions:\n\nCanada Central: located in Toronto (with 3 zones).\nCanada East: located in Quebec City (no zones).\n\nThe US has a few nearby regions as well, in particular:\n\nCentral US: allows the use of advanced IoT features still in testing.",
    "crumbs": [
      "Azure Features"
    ]
  },
  {
    "objectID": "notes/azure-features/index.html#azure-iot-dev-tools",
    "href": "notes/azure-features/index.html#azure-iot-dev-tools",
    "title": "Azure Features",
    "section": "6 Azure IoT Dev Tools",
    "text": "6 Azure IoT Dev Tools\nAzure offers several developer tools to create, manage and connect to the IoT Hub service. We’ll be using two: the Azure Portal, and the Azure CLI.\nThere’s also a few extensions worth noting.\n\n6.1 Azure Portal\nThe Azure Portal is a browser-based portal for IoT Hub and devices.\n\nDocumentation\n\nAzure portal documentation\n\nQuick Start\n\nWhat is the Azure portal?\n\nIoT Example\n\nCreate an IoT hub with Azure portal\n\n\n\n\n\n\n\n\n\nFigure 5: The Azure Portal at https://portal.azure.com is a web app that can access and modify all of your IoT Hub data/configurations/actions/etc.\n\n\n\n6.1.1 Metrics\nOne very useful feature of Azure Portal is the data analysis and presentation software that comes built-in.\nWithout any extra configuration, all IoT commands and actions are logged and stored on the Azure Portal, and visualizations of data usage can be easily displayed for verification/debugging/analysis.\n\n\n\n\n\n\n\nFigure 6: An example of the Azure Metrics portal page. You can easily verify that the daily number of messages you’re sending is below the Free Tier limit of 8000 by checking this page when using your IoT Hub devices.\n\n\nTo find the metrics page for a given Azure resource, do the following:\n\nIn the left navigation menu on the Azure portal, select All Resources.\nSelect the link on your IoT hub.\nSelect Monitoring in the left pane of your IoT Hub, and find the Metrics submenu.\nIn the Scope field, enter your IoT hub name.\nIn the Metric Namespace field, select IoT Hub Standard Metrics.\nIn the Metric field, select the desired metric.\nHover your mouse pointer over the chart to see detailed information at a given time.\n\n\n\n\n6.2 Azure CLI\nAzure CLI is a terminal tool to manage Azure services offered as Bash and PowerShell shells.\n\n\n\n\n\n\nWarning\n\n\n\nAn old version of this document recommended using the official installation instructions. Unfortunately, these methods (using apt) depend on unmaintained packages. Instead, install azure-cli in a python virtual environment as described below.\n\n\n\nDocumentation\n\nAzure CLI documentation\n\nInstallation\n\nazure-cli is a python package we can install in a virtual environment:\n\n\n# Create a venv if you do not have one already\n$ python -m venv .venv\n(.venv) $ source .venv/bin/activate\n# Install dependencies and azure-cli\n(.venv) $ pip install wheel # needed to build azure-cli\n(.venv) $ pip install azure-cli\nYou can later add these packages to a requirements.txt or pyproject.toml file for projects where you use these tools regularly.\n\n\n\n\n\n\nNote\n\n\n\nIf you’re having issues installing this package via the above method, make sure your system python is up to date. Revist the course notes for installing developer environment dependencies, and make sure your system packages and python libraries are up to date. Then, delete your existing virtual environment and recreate it with the updated python.\n\n\n\nQuick Start\n\nGet started with Azure CLI\n\nIoT Example\n\nCreate an IoT hub with CLI\n\n\n\n6.2.1 Azure CLI IoT extension\nAzure CLI has a variety of installable extensions to manage different Azure services.\nThe Azure IoT extension gives us access to the az iot subcommand – a very convenient CLI for controlling IoT-related sevices.\n\nGitHub Repo\n\nazure-iot-cli-extension\n\nDocumentation\n\nList of az iot hub commands",
    "crumbs": [
      "Azure Features"
    ]
  },
  {
    "objectID": "notes/azure-features/index.html#some-tutorials-for-reference",
    "href": "notes/azure-features/index.html#some-tutorials-for-reference",
    "title": "Azure Features",
    "section": "7 Some tutorials for reference",
    "text": "7 Some tutorials for reference\nWe cover these tutorials in Lab 6, but here they are again for reference:\n\nQuickstart: Send telemetry from a device to an IoT hub and monitor it with the Azure CLI\nQuickstart: Send telemetry from an IoT Plug and Play device to Azure IoT Hub",
    "crumbs": [
      "Azure Features"
    ]
  },
  {
    "objectID": "notes/reterminal-setup/index.html",
    "href": "notes/reterminal-setup/index.html",
    "title": "Reterminal setup",
    "section": "",
    "text": "The reTerminal device. Image source",
    "crumbs": [
      "Reterminal setup"
    ]
  },
  {
    "objectID": "notes/reterminal-setup/index.html#overview",
    "href": "notes/reterminal-setup/index.html#overview",
    "title": "Reterminal setup",
    "section": "1 Overview",
    "text": "1 Overview\nThis page documents the general steps needed to perform an initial set up, or reset, of the reTerminal device that we will be using throughout the class.\nThe general steps are:\n\nInstall necessary dependencies for reimaging a Pi on a host computer\nReimage and configure the reTerminal’s operating system.\nConnect to the reTerminal remotely and ensure the remote connection services are working:\n\nGraphical desktop session using a VNC client.\nCLI session using ssh\n\n\nThese instructions are mostly adapted from the instructions available at (“Getting Started with reTerminal | Seeed Studio Wiki” 2023).",
    "crumbs": [
      "Reterminal setup"
    ]
  },
  {
    "objectID": "notes/reterminal-setup/index.html#prerequisites",
    "href": "notes/reterminal-setup/index.html#prerequisites",
    "title": "Reterminal setup",
    "section": "2 Prerequisites",
    "text": "2 Prerequisites\nYou can perform this lab on any computer that you are able to run applications with “elevated permissions”.\n\non the Lab computers, you will need to use the “run with elevated permissions” mode at a few steps. This will be explained in class.\non a personal Windows machine, you can follow all of these instructions normally, using “admin” mode when prompted for elevated permissions.\nif you are using a personal macOS or Linux machine, I have not written the instructions with you in mind – you can do this lab, but make sure to adjust the instructions to your OS accordingly\n\n\n2.1 Hardware required\nYou need to prepare the following hardware before getting started with reTerminal:\n\nEthernet cable or Wi-Fi connection\nUSB Type-C cable\nreTerminal kit case, containing\n\nsmall screw driver (black handle)\nreTerminal power adapter\n\n\n\n\n2.2 Software required\nThere are two tools to install for this lab:\n\nRaspberry Pi (RPi) USB Device Boot daemon\nRaspberry Pi Imager\n\nInstructions for installing each follow below.\n\n\n\n\n\n\nNote 1: Elevated Permissions\n\n\n\nWe will sometimes need elevated permissions in order to install or operate software.\n\nOn the Lab Computers, you can achieve elevated permissions by right-clicking on the executable and selecting the option: “Run with elevated access”.\n\nYou will be asked for your college username and password.\n\nClick on the password field to get focus (the program doesn’t focus on the password by default, which is very annoying)\n\nYou will then be asked for a reason for elevating permissions. Copy-paste: “6P3-W25 Raspberry Pi Setup”\n\nOn your personal computers, the options will depend on your OS:\n\nWindows: The same as the Lab Computers, but use “Run as administrator” instead.\nmacOS/Linux: use your terminal environment to run the executable and sudo to elevate your permissions\n\n\n\n\n\n2.2.1 RPi USB Device Boot installation\nThe instructions for installing and using this software vary greatly depending on your host operating system.\nYou can find all instructions in the README of the repository for the software: https://github.com/raspberrypi/usbboot/.\nI’ve adapted those instructions for each possible operating system in the section below.\n\nWindows (recommended)macOS/LinuxWSL\n\n\n\nDownload the repository source code using git clone.\n\nUse your developer environment, i.e. your WSL instance, to do this (not git bash)\n\n(important) Move the cloned usbboot folder to your home directory in the C:\\ drive\n\nPro tip: do this in the terminal using the mv command, e.g. mv /path/to/usbboot /mnt/c/Users/Michael.Haaf/Downloads\n\nin Windows Explorer, locate rpiboot_setup.exe within the usbboot/win32/ directory.\nRun the executable with elevated permissions (see Note 1)\n\nIf the software has already been installed, press Yes to overwrite the existing installation\nBy doing so, we will ensure that the software is the latest version (which will be important).\n\nRaspberry pi drivers will begin to be installed on your computer.\n\nThis process takes a few minutes. Keep the window open and move on to the next steps in the lab.\n\nWhen this process is finished, you should now have the folder C:\\Program Files (x86)\\Raspberry Pi\\ on your computer.\n\n\n\n\n(On macOS / Linux): read the README of the repository and follow those instructions instead.\n\n\n\nNot recommended at this time.\n\n\n\n\n\n2.2.2 Raspberry Pi Imager installation\n\nFollow software from the official Raspberry Pi website.. This software is straightforward to install.\nNOTE: this should already be installed on the Lab computers. Check to see if Raspberry Pi Imager is an application you can open before installing.",
    "crumbs": [
      "Reterminal setup"
    ]
  },
  {
    "objectID": "notes/reterminal-setup/index.html#reimage-the-reterminal",
    "href": "notes/reterminal-setup/index.html#reimage-the-reterminal",
    "title": "Reterminal setup",
    "section": "3 Reimage the reTerminal",
    "text": "3 Reimage the reTerminal\nBelow is a brief overview of the three step process for reimaging the reTerminal:\n\nDisassemble the back cover and flip a switch to have direct access to the eMMc storage\n\nDo not disassemble the entire device! It is only necessary to remove the heatsink.\n\nReimage and configure the OS using the Raspberry Pi Imager software.\nReturn the memory selector switch to the original position and reassemble.\n\n\n3.1 reTerminal Disassembly\n\n\n\n\n\n\nFigure 1: Only remove the heatsink in order to access the memory switch. It is not necessary fully disassemble the reTerminal like they do in the video.\n\n\n\n\nWatch the video in Figure 1 to understand the disassembly process (2 mins).\nFollow Steps 1, 2, & 3 in the reTerminal documentation to remove the heatsink. Flash Raspberry Pi OS/ 64-bit Ubuntu OS or Other OS to eMMC. Use the following hardware from your reTerminal kit:\n\nsmall screw driver (black handle)\nkit case (store the plastic nubs and removed screws in your case. Don’t lose the screws!)\n\n\nAfter the following the above steps, you will have:\n\nremoved the heatsink\ntoggled the eMMc memory switch (see Figure 2).\n\nYour reTerminal is now ready for a firmware flash.\n\n\n\n\n\n\n\nFigure 2: Memory select switch behind the reTerminal’s heatsink in the “down” position.\n\n\n\n\n3.2 New OS image & Configuration\nTo re-image the reTerminal, follow the steps below.\n\n3.2.1 Launch rpiboot\n\nDouble check you have finished the installation\nLaunch the rpiboot executable file with elevated permissions (see Note 1)\n\nOn Windows, this should be C:\\Program Files (x86)\\Raspberry Pi\\rpiboot.exe\nOn a personal macOS/Linux: I think it’s rpiboot.sh in the installation directory, but check the project README to be sure\n\nKeep the rpiboot window open throughout the next steps of this lab.\n\nAfter launching, you should see a terminal window with something like the following dialog appear:\nRPIBOOT: build-date Jan 22 2023 version 20221215-105525 864863bc\nWaiting for BCM2835/6/7/2711...\nThe rpiboot program creates a daemon (a dedicated background process) that will detect when a reTerminal device is connected in flash mode (i.e. the eMMc switch toggled “down” as in Figure 2).\n\n\n3.2.2 Connect the reTerminal to the USB port of your machine.\n\nFind a USB-C to USB 3.0 Cable and plug it into a 3.0 port on your computer.\n\nUSB-C to USB-C is also fine.\n\nPlug the other end of the cable into your reTerminal\n\nNOTE: if the screen of the reTerminal turns on when you plug in the USB cable, you have missed a step in the disassembly process. In this case, stop, unplug the device, and read the previous instructions more carefully.\n\nrpiboot will detect and attach the reTerminal’s internal memory as a storage device.\nAt this stage, you should see some dialog appear in the RPIBOOT program:\n\nSending bootcode4.bin…\nReceived 4 bytes\nsomethingsomethingsomething\nEtc. etc.\nLoading startup.elf file\nWarning: file not interpreted as such-and-such\nFinished\nYour output will not exactly match the example I have provided. Here are some guidelines:\n\nThe program may close on its own, or may not. Either way is fine.\nYou can ignore the “warnings” that appear at the end of the logs.\nYou can ignore the “There is a problem with this drive” Windows notification.\nThe program should NOT repeatedly loop at Sending bootcode4.bin...\n\nif this is the case, and no other messages appear, try the previous steps again, make sure you pay attention to details.\n\n\nBasically, unless the program is stuck in a loop, you should continue to the next step.\n\n\n3.2.3 Run Rasperry Pi (RPi) Imager\n\nRun the program on your desktop with elevated permissions (see Note 1)\nBefore making any selections, press Control+Shift+X to open the “OS Customizations” Advanced Options menu.\nMake the following customizations (you will need to click through all 3 tabs at the top).\n\nSet a unique hostname (suggestion: your github username)\nEnable SSH with password authentication.\nSet a unique username and password.\n\nDo not use the defaults or forget these. You will need to reimage your reTerminal if you do.\n\nConfigure the wireless LAN for the lab network:\n\nSSID: P326-hotspot\n\nNOTE: there is no whitespace. Take care your SSID matches exactly.\n\nPassword: 6P3-W25-pallet-overcast\n\nNOTE: take care your password matches exactly\n\nWireless LAN country: CA\n\nSet locale settings: America/Montreal\nDisable telemetry.\nEnable “eject media” and “play sound when finished”.\nTake note of your hostname, username, and password (see Moodle for place to enter this information)\n\nYou will be responsible for maintaining your system.\nIf you get locked out, you may have to re-image the system.\n\nPress “SAVE” when finished.\n\nOnce you’ve finished making the above customizations, there are three main configuration choices to make:\n\nRaspberry Pi Device: Raspberry Pi 4\nOperating System: Raspberry Pi OS 64-bit (Recommended)\nStorage: RPi-MSD-0001 (31.3GB). DO NOT SELECT ANY OTHER STORAGE DEVICE.\n\nIf this storage device does not appear, you need to re-do the rpiboot steps\n\n\nOnce Raspberry Pi Images starts, writing the image to the reTerminal’s memory can take 10-15 minutes.\n\nDo not disconnect the reTerminal during flashing!\n\n\n\n\n\n3.3 Reassembly\n\nOnce the writing and verification process is completed, disconnect the USB-C cable from the reTerminal.\nReturn the memory select switch to the original position. (Do you know why? If not, Reread the part about why we toggled it down in the first place!)\nDon’t re-assemble the heatsink+terminal cover yet – we have a few more steps to take first.\nPlug the raspberry pi into the wall using the Pi Power Supply cable in your reTerminal kit. You may need an extension cord/power bar– you can find one at the front of the class.",
    "crumbs": [
      "Reterminal setup"
    ]
  },
  {
    "objectID": "notes/reterminal-setup/index.html#first-boot",
    "href": "notes/reterminal-setup/index.html#first-boot",
    "title": "Reterminal setup",
    "section": "4 First Boot",
    "text": "4 First Boot\nYour reTerminal has been re-imaged! Make sure you have completed the reassembly steps, particularly that you have toggled the memory select switch back up to normal boot mode.\nAt this point, we are now going to start running commands directly on the reTerminal itself. Follow the steps below:\n\nPlug your reTerminal into the wall using the power supply. There are power bars in the lab that you can use if you need more outlets.\n\nIn general, the reTerminal is powered using the provided power supply in your lab kit.\n\nPlug your reTerminal into the lab monitor using the provided microHDMI to HDMI converter\nPlug your reTerminal into the ethernet using the lab computer ethernet.\nPlug the lab keyboard and mouse into your reterminal USB.\n\nOn first boot, your reTerminal screen will not turn on – this is why we need the microHDMI connection to the external monitor.\nThe first task we need to take care of is fixing the reTerminal screen display drivers.\n\n4.1 Display driver fix\nOnce your are logged into the reTerminal and you can see the display on the lab monitor, follow the steps below:\n\nRead and follow the steps outlined here: “Install reTerminal drivers after flashing new Raspberry Pi OS/ Ubuntu OS or Other OS”, up to and including sudo reboot.\n\nNOTE: recall that you have installed a 64-bit OS on your reTerminal. Do not follow any 32-bit OS steps in the above instructions.\n\nIf the above steps have been completed successfully, your device should reboot and BOTH the raspberry pi screen AND the HDMI connection should work (this takes several seconds, give it a minute before you panic).\n\nMake sure your Pi is connected to the wall power supply, NOT to your computer (the pi screen needs more power than your lab computer can provide).\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf your keyboard is in French mode on the raspberry pi, you can follow the instructions here to set it back into English (US) mode.\n\n\n\n4.2 Update & Upgrade\nA good first step for any OS installation is to ensure all system packages are at the latest version.\nFollow the three steos in the official guide for the reTerminal FAQ Wiki: How to upgrade Raspberry Pi OS and the installed packages - For any steps that ask you to make a choice, just pick the defaults.",
    "crumbs": [
      "Reterminal setup"
    ]
  },
  {
    "objectID": "notes/reterminal-setup/index.html#set-up-remote-connections",
    "href": "notes/reterminal-setup/index.html#set-up-remote-connections",
    "title": "Reterminal setup",
    "section": "5 Set up Remote connections",
    "text": "5 Set up Remote connections\nGoing forward, we want to be able to use the reTerminal without having to plug it into an external monitor.\nFixing the device screen was one step – however, we would also like to be able to use the reTerminal without relying on the small touchscreen either.\nWe are going to rely on remote connections to the reTerminal in general in this class – that is, connecting to the reTerminal using an IP Address.\nBecause the lab network has firewalls, however, we cannot do so directly using the lab ethernet or the campus WIFI.\nTo fix this problem, we are going to use a remote networking tool called Tailscale. Follow the steps below:\n\n5.1 Set up Tailscale\nFirst, create an account on Tailscale. You have the following choices for authentication:\n\n(Recommended) Your GitHub account\nYour school email address\n\nIf you’re curious to know more about Tailscale before you sign up, please ask me! You can also read about it here:\n\nhttps://tailscale.com/why-tailscale\nhttps://tailscale.com/blog/how-tailscale-works\n\nIn the big picture: Tailscale will allow us to establish direct remote connections between our raspberry Pi and our developer environments by creating a Wireguard VPN mesh network for us.\nTo be clear, Tailscale is free, you will not need any of the paid features.\nAfter you have made an account, you will need to set it up on your pi and your developer environments, see instructions below.\n\n5.1.1 On the raspberry pi\nFollow these instructions\nYou can do the “Install with one command” step, OR follow the manual steps if you prefer.\nNOTE: you will probably need to also run the following command on your raspberry pi:\nsudo apt install curl\n\n\n5.1.2 On your lab computer\n\nInstall on your WSL by Following these instructions\n\nYou can do the “Install with one command” step, OR follow the manual steps if you prefer.\n\nALSO install on the main Windows machine by following these instructions\n\nYou may need to use elevated permissions for this, see Note 1\n\n\nNOTE: you will probably need to also run the following command on your WSL:\nsudo apt install curl\n\n\n5.1.3 On your personal computer\nDepending on your operating system:\n\nWindows:\n\nInstall on your WSL by Following these instructions\nYou can do the “Install with one command” step, OR follow the manual steps if you prefer.\n\n\nNOTE: you will probably need to also run the following command on your WSL:\nsudo apt install curl\n\nmacOS: Follow these instructions\nLinux: Follow these instructions\n\nYou can do the “Install with one command” step, OR follow the manual steps if you prefer.\n\n\n5.1.4 Verify your tailscale setup\nOn either your lab/personal developer environment, OR your raspberry pi, run the command:\ntailscale status\nYou should see the IP address for both your reTerminal AND your lab/personal developer environment. Take note of these IP addresses before moving on to the next steps.\n\n\n\n5.2 Set up VNC\nIn this section you will connect to the graphical desktop environment remotely using a VNC session. This will allow you to control the raspberry pi from your lab computer over the graphical shell of the lab computer, in addition to SSH.\n\nOnce connected to the provided power cable, the reTerminal will boot and automatically login into the graphical desktop environment as the default user.\nThe reTerminal has a touch screen which you are welcome to use for the next steps. However, I recommend plugging in your lab keyboard and/or mouse for these next few steps. Let me know if you need a keyboard/mouse.\nEnable the VNC client in the Raspberry Pi Configuration menu.\n\nClick on the Raspberry Pi icon (top right).\nSelect Preferences &gt; Raspberry Pi Configuration.\nOpen the Interfaces tab.\nEnable the VNC server (disabled by default).\n\nOpen a terminal on your Raspberry Pi. Double check your IP address using tailscale status\nAt your lab computer, start the VNC Viewer client (RealVNC). You can run this program without elevated access.\n\nRealVNC is already installed on the lab computers. If working on a personal device, you can install it here\nNOTE: you DO NOT need to make an account or sign up for RealVNC. There is an option to\n\nConnect to your Raspberry Pi using VNC Viewer in your lab computer.\nEnter the hostname you assigned to your Raspberry Pi in Part 1, step 5 or the IP address you noted in step 5.\n\nUse the username and password you configured in Part 1, step 5.\n\n\n\n\n5.3 Set up SSH\nThe SSH server inside your Raspberry Pi should already be enabled by default (from Part 1, step 5).\n\nTo double check that the ssh server is enabled on your Pi: follow the official instructions on Setting up the SSH Server on the Raspberry Pi.\n\n\n5.3.1 Connecting over CLI\nYou can establish an SSH connection to the reTerminal from your developer environment. If your connection is successful you should see the a similar prompt:\nuser-name@hostname:~ $\nwhere hostname and user-name correspond to the choices you made during your imaging of the pi.\n\nFollow the official Raspberry Pi instructions (NOTE: Linux instructions apply to WSL!) Secure Shell from Linux or Mac OS\n\nFor Lab 3, I ask you to obtain the reTerminal’s MAC address.\nYou can do so running ifconfig command in an ssh session, and checking the properties of the wireless network card (wlan0):\n\nRun the command ifconfig or the command ip address\nLook for the wireless network adapter wlan0:\nThe MAC address will be listed there.I\n\nStyle points: use grep and pipe to grab the MAC address directly to your clipboard\n\n\n\n\n5.3.2 Connecting over VSCode\nYou can use VS Code in your lab workstation to create a development environment inside the Raspberry Pi which will be controlled from the lab workstation.\nIf you would like to know more about how this extension works, visit Remote Development using SSH.\nBelow is a 5min video that illustrates how the Remote - SSH extension works:\nVS Code Remote Development using SSH to a Raspberry Pi\n\nIn your lab workstation, ensure you have installed the following VS Code extensions:\n\nRemote - SSH, by Microsoft.\nPython extension, by Microsoft.\n\nConnect your lab workstation to your Raspberry Pi by following the Remote - SSH extension’s official instructions: Getting started. Once VS Code is connected to the reTerminal, you are now in a new development environment inside the reTerminal. Complete the following tasks:\nInstall the VS Code Python extension (this time inside the reTerminal, not in your lab workstation like in step 1).\n\nIf necessary, follow the guide: Getting Started with Python in VS Code\n\nOn the Raspberry Pi, open the folder lab1 in the home directory of the reTerminal (created in Part 3, step 4).\nCreate a new file named lab-script.py inside the folder lab1 and include the code:\n\nprint(‘Hello from inside the pi!’)\n\nExecute your code from within VS Code using the play button.",
    "crumbs": [
      "Reterminal setup"
    ]
  },
  {
    "objectID": "notes/python-environments/index.html",
    "href": "notes/python-environments/index.html",
    "title": "Python environment management",
    "section": "",
    "text": "This section shows the use the python virtual environments (virt. envs.) with the venv built-in tool.\nvenv is the officially recommended way to manage virt. envs. but it’s not the only one. Other tools such as Conda, Pipenv, Poetry and PDM are other options.\n\nShort guide by python.org**: Installing packages using pip and virtual environments\nDetailed guide by RealPython.com: Python Virtual Environments: A Primer\n\n\n\nPython is not good at dependency management. We want to:\n\nAvoid system pollution\n\nInstalling packages to the OS’s global Python will mix them with OS relevant packages. This could have unexpected side effects on OS tasks.\nBecause of the reason above, updating OS packages might overwrite or delete global Python packages.\n\nAvoid project dependency conflicts\n\nPython projects might require different versions the same external library.\n\nMaking projects reproducible in other environments.\n\nSince all dependencies are isolated to a specific project, it is easier to identify and document them.\nOnce the dependencies are “locked”, the project dependency can be easily reproduced in other environments.\n\n\n\n\n\nCreating and using virtual environments involved the following steps:\n\nCreation\nActivation\nInstalling dependencies\nDeactivation\nLocking dependencies\n\n\n\n\n\nCreate a new folder for the project (if one doesn’t already exist).\ncd into the folder and create a new virt. env.:\n\n$ python3 -m venv .venv\nNote that .venv is the name of your virtual environment. venv is a popular choice.\n\n\n\nOnce created, the virt. env. needs to be activated.\n# Linux\n$ source .venv/bin/activate\n\n# Windows\n$ .venv/Scripts/activate.bat\n\nNote: You can work with virtual environments without activating it. To do this, provide the full path to its Python interpreter when executing a command.\nTypically, you’ll want to activate the virtual environment to avoid repeatedly having to type long paths.\n\nTo deactivate:\n# Notice the shell prompt indicates a virt. env. is active\n$ (.venv) deactivate\n\n\n\nOnce a virtual environment is active, packages installed with pip will be local to that virtual environment.\nBy default, project packages are installed in:\nproject-folder\n    |- &lt;project-venv&gt;\n    |       |- lib\n    |           |- python3.X\n    |                   |- site-packages \nFor example, installing aiohttp:\n# Notice the shell prompt indicates a virt. env. is active\n$ (.venv) pip install aiohttp\n\n\n\nProject dependencies can be listed in a file for portability. By convention, the file requirements.txt contains all the dependencies of a project created with a virtual environment.\n\nThis is analogous to package.json for Node.Js projects\n\nTo generate a requirements.txt for an existing project:\n$ pip freeze &gt; requirements.txt\nTo install packages from a requirements.txt:\n$ pip install -r requirements.txt\n\n\n\nIf you move a project that uses virtual environments to a different folder, you must re-initialize the virtual environment.\nFrom the official docs:\n\nScripts installed in environments contain the absolute paths to their environment’s interpreters.\n\nBecause of this, environments are inherently non-portable. You should always have a simple means of recreating an environment (for example, have a requirements file requirements.txt, and invoke pip install -r requirements.txt.\n\n\n\n\n\nThe following exercises are meant for familiarization with venv.\n\nCompare packages installed with pip inside and outside of a new virtual environment (virt. env.):\n\nList all globally installed pip packages: pip list\nCreate a new folder and use venv to create a virtual environment inside.\nActivate the new virt. env..\nList the pip packages as seen from inside the active virtual environment.\nDeactivate the new virtual environment.\n\nComplete the following steps inside the virtual environment (virt. env.) you created in the previous question.\n\nActivate the virt. env.\ninstall the following libraries:\n\nhowdoi\npyjokes\n\nList all pip packages to see the extra dependencies that got installed.\nHave some fun using both installed packages (see their docs).\nExport the dependency list to requirements.txt by using pip freeze.\nDeactivate the virt. env.\nMove requirements.txt to one folder level above the project folder (so it’s not deleted in the next step).\nDelete the project folder.\n\nCreate a new project and virtual environment using the requirements.txt from the previous exercise.\n\nCreate a new project folder with a virtual environment inside.\nMove requirements.txt inside the new project folder.\nInstall project packages using requirements.txt",
    "crumbs": [
      "Python environment management"
    ]
  },
  {
    "objectID": "notes/python-environments/index.html#virtual-environments",
    "href": "notes/python-environments/index.html#virtual-environments",
    "title": "Python environment management",
    "section": "",
    "text": "This section shows the use the python virtual environments (virt. envs.) with the venv built-in tool.\nvenv is the officially recommended way to manage virt. envs. but it’s not the only one. Other tools such as Conda, Pipenv, Poetry and PDM are other options.\n\nShort guide by python.org**: Installing packages using pip and virtual environments\nDetailed guide by RealPython.com: Python Virtual Environments: A Primer\n\n\n\nPython is not good at dependency management. We want to:\n\nAvoid system pollution\n\nInstalling packages to the OS’s global Python will mix them with OS relevant packages. This could have unexpected side effects on OS tasks.\nBecause of the reason above, updating OS packages might overwrite or delete global Python packages.\n\nAvoid project dependency conflicts\n\nPython projects might require different versions the same external library.\n\nMaking projects reproducible in other environments.\n\nSince all dependencies are isolated to a specific project, it is easier to identify and document them.\nOnce the dependencies are “locked”, the project dependency can be easily reproduced in other environments.\n\n\n\n\n\nCreating and using virtual environments involved the following steps:\n\nCreation\nActivation\nInstalling dependencies\nDeactivation\nLocking dependencies\n\n\n\n\n\nCreate a new folder for the project (if one doesn’t already exist).\ncd into the folder and create a new virt. env.:\n\n$ python3 -m venv .venv\nNote that .venv is the name of your virtual environment. venv is a popular choice.\n\n\n\nOnce created, the virt. env. needs to be activated.\n# Linux\n$ source .venv/bin/activate\n\n# Windows\n$ .venv/Scripts/activate.bat\n\nNote: You can work with virtual environments without activating it. To do this, provide the full path to its Python interpreter when executing a command.\nTypically, you’ll want to activate the virtual environment to avoid repeatedly having to type long paths.\n\nTo deactivate:\n# Notice the shell prompt indicates a virt. env. is active\n$ (.venv) deactivate\n\n\n\nOnce a virtual environment is active, packages installed with pip will be local to that virtual environment.\nBy default, project packages are installed in:\nproject-folder\n    |- &lt;project-venv&gt;\n    |       |- lib\n    |           |- python3.X\n    |                   |- site-packages \nFor example, installing aiohttp:\n# Notice the shell prompt indicates a virt. env. is active\n$ (.venv) pip install aiohttp\n\n\n\nProject dependencies can be listed in a file for portability. By convention, the file requirements.txt contains all the dependencies of a project created with a virtual environment.\n\nThis is analogous to package.json for Node.Js projects\n\nTo generate a requirements.txt for an existing project:\n$ pip freeze &gt; requirements.txt\nTo install packages from a requirements.txt:\n$ pip install -r requirements.txt\n\n\n\nIf you move a project that uses virtual environments to a different folder, you must re-initialize the virtual environment.\nFrom the official docs:\n\nScripts installed in environments contain the absolute paths to their environment’s interpreters.\n\nBecause of this, environments are inherently non-portable. You should always have a simple means of recreating an environment (for example, have a requirements file requirements.txt, and invoke pip install -r requirements.txt.\n\n\n\n\n\nThe following exercises are meant for familiarization with venv.\n\nCompare packages installed with pip inside and outside of a new virtual environment (virt. env.):\n\nList all globally installed pip packages: pip list\nCreate a new folder and use venv to create a virtual environment inside.\nActivate the new virt. env..\nList the pip packages as seen from inside the active virtual environment.\nDeactivate the new virtual environment.\n\nComplete the following steps inside the virtual environment (virt. env.) you created in the previous question.\n\nActivate the virt. env.\ninstall the following libraries:\n\nhowdoi\npyjokes\n\nList all pip packages to see the extra dependencies that got installed.\nHave some fun using both installed packages (see their docs).\nExport the dependency list to requirements.txt by using pip freeze.\nDeactivate the virt. env.\nMove requirements.txt to one folder level above the project folder (so it’s not deleted in the next step).\nDelete the project folder.\n\nCreate a new project and virtual environment using the requirements.txt from the previous exercise.\n\nCreate a new project folder with a virtual environment inside.\nMove requirements.txt inside the new project folder.\nInstall project packages using requirements.txt",
    "crumbs": [
      "Python environment management"
    ]
  },
  {
    "objectID": "notes/python-project-management/index.html",
    "href": "notes/python-project-management/index.html",
    "title": "Python project management",
    "section": "",
    "text": "Now that we’re making larger applications with multiple dependecies from different sources, as well as multiple modules, code organization starts to be a noticable problem. Managing a “project” (for now, a directory containing your source code for a given task) becomes much easier with tools and configuration.\nWe’ll look at:\n\nImport statements\nRelative imports\nConfiguration with dot files and project files",
    "crumbs": [
      "Python project management"
    ]
  },
  {
    "objectID": "notes/python-project-management/index.html#overview",
    "href": "notes/python-project-management/index.html#overview",
    "title": "Python project management",
    "section": "",
    "text": "Now that we’re making larger applications with multiple dependecies from different sources, as well as multiple modules, code organization starts to be a noticable problem. Managing a “project” (for now, a directory containing your source code for a given task) becomes much easier with tools and configuration.\nWe’ll look at:\n\nImport statements\nRelative imports\nConfiguration with dot files and project files",
    "crumbs": [
      "Python project management"
    ]
  },
  {
    "objectID": "notes/python-project-management/index.html#python-projects-import-statements",
    "href": "notes/python-project-management/index.html#python-projects-import-statements",
    "title": "Python project management",
    "section": "2 Python Projects & Import Statements",
    "text": "2 Python Projects & Import Statements\nFor programmers coming from other languages, how to organize your project files and how to import modules and classes can be confusing.",
    "crumbs": [
      "Python project management"
    ]
  },
  {
    "objectID": "notes/python-project-management/index.html#modules-vs-packages",
    "href": "notes/python-project-management/index.html#modules-vs-packages",
    "title": "Python project management",
    "section": "3 Modules vs Packages",
    "text": "3 Modules vs Packages\nLet’s clarify the terminology and distinction between a module vs a package.\n(Notes taken from The Joy of Packaging, Making a Python Package)\n\n3.1 Modules\nA module usually corresponds to a single file: something.py A python “module” is a single namespace, with a collection of values:\n\nfunctions\nconstants\nclass definitions\n\n\n\n3.2 Packages\nA “package” is essentially a module, except it can have other modules (and indeed other packages) inside it.\nA package usually corresponds to a directory with a file in it called __init__.py and any number of python files or other package directories:\na_package\n    __init__.py\n    module_a.py\n    a_sub_package\n        __init__.py\n    module_b.py\n__init__.py can be empty or it can have arbitrary code.\nThe code will be run when the package is imported (just like a module).\nModules inside packages are not automatically imported. So, with the above structure:\nimport a_package\nwill run the code in a_package/__init__.py\nAny names defined in the __init__.py will be available in: a_package.a_name",
    "crumbs": [
      "Python project management"
    ]
  },
  {
    "objectID": "notes/python-project-management/index.html#project-structure",
    "href": "notes/python-project-management/index.html#project-structure",
    "title": "Python project management",
    "section": "4 Project Structure",
    "text": "4 Project Structure\nThere’s no single way to do it. Below is a recommendation from RealPython.com on Python Application Layouts for Application with Internal Packages:\nhelloworld/\n│\n├── helloworld/\n│   ├── __init__.py\n│   ├── main_entry.py\n│   ├── hello/\n│   │   ├── __init__.py\n│   │   ├── hello.py\n│   │   └── helpers.py\n│   │\n│   └── world/\n│       ├── __init__.py\n│       ├── helpers.py\n│       └── world.py\n│\n├── data/\n│   ├── input.csv\n│   └── output.xlsx\n│\n├── tests/\n│   ├── hello\n│   │   ├── helpers_tests.py\n│   │   └── hello_tests.py\n│   │\n│   └── world/\n│       ├── helpers_tests.py\n│       └── world_tests.py\n│\n├── docs/\n│   ├── hello.md\n│   └── world.md\n│\n├── .gitignore\n├── LICENSE\n└── README.md",
    "crumbs": [
      "Python project management"
    ]
  },
  {
    "objectID": "notes/python-project-management/index.html#relative-import-statements",
    "href": "notes/python-project-management/index.html#relative-import-statements",
    "title": "Python project management",
    "section": "5 Relative Import Statements",
    "text": "5 Relative Import Statements\n(Taken from StackOverflow discussion Relative imports in Python 3, by Aya)\nConsider the project layout below:\nmain.py\nmypackage/\n    __init__.py\n    mymodule.py\n    myothermodule.py\nIn mymodule.py :\n\n\nmymodule.py\n\n# Exported function\ndef get_temperature():\n    return 23.45\n\nif __name__ == '__main__':\n    print(get_temperature())\n\nIn myothermodule.py :\n\n\nmyothermodule.py\n\n# Relative import using `.` for current directory, `..` for parent directory.\nfrom .mymodule import get_temperature\n\n# Exported function\ndef announce_weather():\n    return f\"Outside temperature is {get_temperature()}\"\n\nif __name__ == '__main__':\n    announce_weather()\n\nIn main.py :\n\n\nmain.py\n\nfrom mypackage.myothermodule import announce_weather\n\ndef main():\n    print(announce_weather())\n\nif __name__ == '__main__':\n    main()\n\nThis works when running main.py or mypackage/mymodule.py, but fails with mypackage/myothermodule.py, due to the relative import:\nfrom .mymodule import as_int\nException has occurred: ImportError\nattempted relative import with no known parent package\nSolution\nWhen running myothermodule.py as a stand-alone script, run Python using the -m option to run the file as a module. This will execute all the __init__.py files and load the module dependency.\npython -m mypackage.myothermodule\n# Note the \".\", NOT a \"/\", as in, the following won't work:\npython -m mypackage/myothermodule\nThis will allow you to write modules that contain main() functions you can test separately – we took advantage of this pattern in Assignment 1.",
    "crumbs": [
      "Python project management"
    ]
  },
  {
    "objectID": "notes/python-project-management/index.html#references",
    "href": "notes/python-project-management/index.html#references",
    "title": "Python project management",
    "section": "6 References",
    "text": "6 References\nMore on Modules and Packages:\n\nPython Modules and Packages – An Introduction by RealPython.com\n\nProject Structures\n\nPackaging a python library by ionel’s codelog.\n\nRelative Imports\n\nRelative imports for the billionth time in StackOverlow\nRelative imports in Python 3 in StackOverlow\nThe import system in Python.org docs.\nModules: Packages in Python.org docs.",
    "crumbs": [
      "Python project management"
    ]
  },
  {
    "objectID": "notes/python-collections/index.html",
    "href": "notes/python-collections/index.html",
    "title": "Python collection types and operations",
    "section": "",
    "text": "Image: https://realpython.com/python-lambda/",
    "crumbs": [
      "Python collection types and operations"
    ]
  },
  {
    "objectID": "notes/python-collections/index.html#overview",
    "href": "notes/python-collections/index.html#overview",
    "title": "Python collection types and operations",
    "section": "1 Overview",
    "text": "1 Overview\n\nLoops\nList comprehensions\nGenerators\nLambdas\nDictionaries",
    "crumbs": [
      "Python collection types and operations"
    ]
  },
  {
    "objectID": "notes/python-collections/index.html#loops",
    "href": "notes/python-collections/index.html#loops",
    "title": "Python collection types and operations",
    "section": "2 Loops",
    "text": "2 Loops\nThere are two types of loops in Python, for and while.\n\n2.1 The for loop\nFor loops iterate over a given sequence, or iterator. Here is an example:\n\n\nPython\n\nprimes = [2, 3, 5, 7]\nfor prime in primes:\n    print(prime)\n\nFor loops can iterate over a sequence of numbers using the range function. range returns an iterator object which can be looped using the following syntax:\n\n\nPython\n\n# Prints out the numbers 0,1,2,3,4\nfor x in range(5):\n    print(x)\n\n# Prints out 3,4,5\nfor x in range(3, 6):\n    print(x)\n\n# Prints out 3,5,7\nfor x in range(3, 8, 2):\n    print(x)\n\n\n\n2.2 break and continue statements\nbreak is used to exit a for loop or a while loop, whereas continue is used to skip the current block, and return to the “for” or “while” statement. A few examples:\n\n\nPython\n\n# Prints out only odd numbers - 1,3,5,7,9\nfor x in range(10):\n    # Check if x is even\n    if x % 2 == 0:\n        continue\n    print(x)\n\n# Prints out 0,1,2,3,4\n\ncount = 0\nwhile True:  # we have while loops in Python too.\n    print(count)\n    count += 1\n    if count &gt;= 5:\n        break",
    "crumbs": [
      "Python collection types and operations"
    ]
  },
  {
    "objectID": "notes/python-collections/index.html#list-comprehensions",
    "href": "notes/python-collections/index.html#list-comprehensions",
    "title": "Python collection types and operations",
    "section": "3 List Comprehensions",
    "text": "3 List Comprehensions\nList Comprehensions is a very powerful tool, which creates a new list based on another list, in a single, readable line.\nFor example, let’s say we need to create a list of integers which specify the length of each word in a certain sentence, but only if the word is not the word “the”.\n\n\nPython\n\nsentence = \"the quick brown fox jumps over the lazy dog\"\nwords = sentence.split()\nword_lengths = []\nfor word in words:\n    if word != \"the\":\n        word_lengths.append(len(word))\nprint(words)\nprint(word_lengths)\n\nUsing a list comprehension, we could simplify this process to this notation:\n\n\nPython\n\nsentence = \"the quick brown fox jumps over the lazy dog\"\nwords = sentence.split()\nword_lengths = [len(word) for word in words if word != \"the\"]\nprint(words)\nprint(word_lengths)",
    "crumbs": [
      "Python collection types and operations"
    ]
  },
  {
    "objectID": "notes/python-collections/index.html#generators",
    "href": "notes/python-collections/index.html#generators",
    "title": "Python collection types and operations",
    "section": "4 Generators",
    "text": "4 Generators\nGenerators are very easy to implement, but a bit difficult to understand.\nGenerators are used to create iterators, but with a different approach. Generators are simple functions which return an iterable set of items, one at a time, in a special way.\nWhen an iteration over a set of item starts using the for statement, the generator is run. Once the generator’s function code reaches a “yield” statement, the generator yields its execution back to the for loop, returning a new value from the set. The generator function can generate as many values (possibly infinite) as it wants, yielding each one in its turn.\nHere is a simple example of a generator function which returns 7 random integers:\n\n\nPython\n\nimport random\n\n\ndef lottery():\n    # returns 6 numbers between 1 and 40\n    for i in range(6):\n        yield random.randint(1, 40)\n\n    # returns a 7th number between 1 and 15\n    yield random.randint(1, 15)\n\n\nfor random_number in lottery():\n    print(\"And the next number is... %d!\" % (random_number))\n\nThis function decides how to generate the random numbers on its own, and executes the yield statements one at a time, pausing in between to yield execution back to the main for loop.",
    "crumbs": [
      "Python collection types and operations"
    ]
  },
  {
    "objectID": "notes/python-collections/index.html#lambda-functions",
    "href": "notes/python-collections/index.html#lambda-functions",
    "title": "Python collection types and operations",
    "section": "5 Lambda functions",
    "text": "5 Lambda functions\nNormally we define a function using the def keyword somewhere in the code and call it whenever we need to use it.\n\n\nPython\n\ndef sum(a, b):\n    return a + b\n\n\na = 1\nb = 2\nc = sum(a, b)\nprint(c)\n\nNow instead of defining the function somewhere and calling it, we can use python’s lambda functions, which are inline functions defined at the same place we use it. So we don’t need to declare a function somewhere and revisit the code just for a single time use.\nThey don’t need to have a name, so they also called anonymous functions. We define a lambda function using the keyword lambda.\n\n\nPython\n\nyour_function_name = lambda inputs: output\n\nSo the above sum example using lambda function would be,\n\n\nPython\n\na = 1\nb = 2\nsum = lambda x, y: x + y\nc = sum(a, b)\nprint(c)\n\nHere we are assigning the lambda function to the variable sum, and upon giving the arguments i.e. a and b, it works like a normal function.",
    "crumbs": [
      "Python collection types and operations"
    ]
  },
  {
    "objectID": "notes/python-collections/index.html#dictionaries",
    "href": "notes/python-collections/index.html#dictionaries",
    "title": "Python collection types and operations",
    "section": "6 Dictionaries",
    "text": "6 Dictionaries\n\nA dictionary is a data type similar to arrays, but works with keys and values instead of indexes. Each value stored in a dictionary can be accessed using a key, which is any type of object (a string, a number, a list, etc.) instead of using its index to address it.\nFor example, a database of phone numbers could be stored using a dictionary like this:\n\n\nPython\n\nphonebook = {}\nphonebook[\"John\"] = 938477566\nphonebook[\"Jack\"] = 938377264\nphonebook[\"Jill\"] = 947662781\nprint(phonebook)\n\nAlternatively, a dictionary can be initialized with the same values in the following notation:\n\n\nPython\n\nphonebook = {\"John\": 938477566, \"Jack\": 938377264, \"Jill\": 947662781}\nprint(phonebook)\n\n\n6.1 Iterating over dictionaries\nDictionaries can be iterated over, just like a list. However, a dictionary, unlike a list, does not keep the order of the values stored in it. To iterate over key value pairs, use the following syntax:\n\n\nPython\n\nphonebook = {\"John\": 938477566, \"Jack\": 938377264, \"Jill\": 947662781}\nfor name, number in phonebook.items():\n    print(\"Phone number of %s is %d\" % (name, number))\n\n\n\n6.2 Removing a value\nTo remove a specified index, use either one of the following notations:\n\n\nPython\n\nphonebook = {\"John\": 938477566, \"Jack\": 938377264, \"Jill\": 947662781}\ndel phonebook[\"John\"]\nprint(phonebook)\n\nor:\n\n\nPython\n\nphonebook = {\"John\": 938477566, \"Jack\": 938377264, \"Jill\": 947662781}\nphonebook.pop(\"John\")\nprint(phonebook)",
    "crumbs": [
      "Python collection types and operations"
    ]
  },
  {
    "objectID": "notes/bash-scripting/index.html",
    "href": "notes/bash-scripting/index.html",
    "title": "Bash scripting",
    "section": "",
    "text": "Image source: tudoubaba.net",
    "crumbs": [
      "Bash scripting"
    ]
  },
  {
    "objectID": "notes/bash-scripting/index.html#bash-theory",
    "href": "notes/bash-scripting/index.html#bash-theory",
    "title": "Bash scripting",
    "section": "1 Bash theory",
    "text": "1 Bash theory\nBash (and other shell languages) isn’t really a programming language – it is a command interpreter that’s best used for organizing the inputs and outputs of programs written in other languages. Historically this would be the C programming language. In our class, we will be writing “real” programs in Python.\nBut I digress. Despite the fact that Bash isn’t a programming language like C or Python, you can still program in it. What do you need to know to write Bash programs?\nThese notes will cover the basic programming concepts you need to know to get the most out of writing scripts in Bash.\n\n1.1 Variables\nThis section was adapted from (“Variable Substitution” n.d.)\nIn Bash, there is no concept of “types” – this is not a compiled language where different amounts of memory need to be reserved for different types of data.\nRather, all variables in Bash are simply value-placeholders. That is:\n\nThe name of a variable is a placeholder for its value, which is the data that it holds.\nReferencing/retrieving the value of a variable is called variable substitution.\n\nFor example, if variable1 is the name of a variable, then $variable1 is a reference to its value, the data it contains.\nAn equivalent syntax for variable substitution that is generally more robust is to add curly braces, as in: ${variable1}\nRunning a command such as echo $variable1 or echo ${variable1} will cause the value referenced by variable1 to be substituted and passed to the echo command.\nHere’s an example of variable assignment and substitution:\n\n\nbash\n\n# Create a variable with the name \"a\" and the value \"375\"\n$ a=375\n\n# Create a variable with the name \"hello\" and the value of the variable \"a\" (375)\n$ hello=$a\n\n# Not a variable reference, just the string \"hello\" ...\n$ echo hello\nhello\n\n# This *is* a variable substitution\n$ echo $hello\n375\n\n# This is another way to do a variable substitution\necho ${hello}\n375\n\n\n\n\n\n\n\n\nNote\n\n\n\nNo spaces are permitted on either side of = sign when initializing variables.\n\n\n1.1.1 Variable names\nWhat can you name variables? Here are the rules:\n\nVariable names must start with a letter (not a number or any other character)\nVariable names cannot contain whitespace or punctuation\n\n\n\n1.1.2 Command substitution\nWe can also assign the results of a command to a variable:\n\n\nbash\n\n# The \"%x %r %Z\" string is an example of a date format string.\n# See `man date` for examples of other date format strings\nright_now=\"$(date +\"%x %r %Z\")\"\n\nThe characters $( ) tell the shell: “substitute the results of the enclosed command”. This technique is known as command substitution.\nIn the above example script, the variable right_now gets assigned the result of calling the date command with with the argument \"%x %r %Z\" which outputs the current date and time.\nLike variable substitutions, it is a good idea to wrap command substitutions in double quotes to prevent unwanted word splitting in case the result of the expansion contains whitespace characters – see the (#Quoting) section below.\n\n\n1.1.3 Environment variables\nThis section was adapted from (“Writing Shell Scripts - Lesson 4: Variables” n.d.) and (“Export Man Page - Linux - SS64.com” n.d.)\nAny time a shell session is initialized, some variables are already set by startup files.\nTo see all the variables that are in the environment, use the printenv command:\n\n\nbash\n\n$ printenv\nSHELL=/bin/bash\n...\n\nYou can also view environment variables in a bash session using echo:\n\n\nbash\n\n$ echo $SHELL\n/bin/bash\n\nYou can create your own environment variables using the export command.\n\n\nbash\n\n$ MYDEPT=Sales\n$ echo $MYDEPT\nSales\n$ export MYDEPT\n\n# In one line:\nexport SOMEOTHERVAR=Value\n\n\n\n\n1.2 Quoting\nThis section was adapted from (“Quoting” n.d.)\nQuoting means just that, bracketing a string in quotes. This has the effect of protecting special characters in the string from reinterpretation or expansion by the shell or shell script.\n\nA character is special if it has an interpretation other than its literal meaning. For example, the asterisk * represents a wild card character in globbing and Regular Expressions.\n\n\n\nbash\n\n$ ls -l [Vv]*\n-rw-rw-r--    1 bozo  bozo       324 Apr  2 15:05 VIEWDATA.BAT\n-rw-rw-r--    1 bozo  bozo       507 May  4 14:25 vartrace.sh\n-rw-rw-r--    1 bozo  bozo       539 Apr 14 17:11 viewdata.sh\n\n$ ls -l '[Vv]*'\nls: [Vv]*: No such file or directory\n\n$ ls -l '[vv]*'\nls: [vv]*: no such file or directory\n\n\n1.2.1 Quoting variables\nWhen referencing a variable, it is generally advisable to enclose its name in double quotes, like so:\n\n\nbash\n\n$ local var=\"string-with-special-characters#*;&gt;,\"\n$ echo \"$var\"\nstring-with-special-characters#*;&gt;,\n\nThis prevents reinterpretation of all special characters within the quoted string, with the following exceptions:\n\n$ (used for variable dereferencing)\n\\ (escape character)\n\nKeeping $ as a special character within double quotes permits referencing a quoted variable ($variable), that is, replacing the variable with its value in the resulting string.\nUsing double quotes also prevents word splitting. An argument enclosed in double quotes presents itself as a single word, even if it contains whitespace separators.\n\n\nbash\n\nList=\"one two three\"\n\nfor a in $List     # Splits the variable in parts at whitespace.\ndo\n  echo \"$a\"\ndone\n# one\n# two\n# three\n\necho \"---\"\n\nfor a in \"$List\"   # Preserves whitespace in a single variable.\ndo #     ^     ^\n  echo \"$a\"\ndone\n# one two three\n\n\n\n1.2.2 Escaping\nEscaping is a method of quoting single characters. The escape \\ preceding a character tells the shell to interpret that character literally.\n\n\nbash\n\n$ echo \"Hello world\"\nHello world\n\n$ echo \"Hello \\\"world\\\"\"\nHello \"world\"\n\nYou can see more examples and information on escaping here.\n\n\n1.2.3 Single quotes\nSingle quotes ' operate similarly to double quotes, but do not permit referencing variables, since the special meaning of $ is turned off.\nWithin single quotes, every special character except ' gets interpreted literally. Consider single quotes (‘full quoting’) to be a stricter method of quoting than double quotes (“partial quoting”).\nSince even the escape character \\ gets a literal interpretation within single quotes, trying to enclose a single quote within single quotes will not yield the expected result.\n\n\n\n1.3 Functions\nThis section was adapted from (“Writing Shell Scripts - Lesson 6: Shell Functions” n.d.)\nAs programs get longer and more complex, they become more difficult to design, code, and maintain. As with any large endeavor, it is often useful to break a single, large task into a series of smaller tasks. We can do this with functions.\nA couple of important points about functions in bash:\n\nThey must be defined before they can be used.\nSecond, the function body (the portions of the function between the { and } characters) must contain at least one valid command.\n\nHere is an example:\n\n\nbash\n\n# The function definition must come before any function calls\nsystem_info()\n{\n  # At least one valid command is required in a function body\n  echo \"function system_info\"\n}\n\n# No brackets are included in the function call\nsystem_info\n\nRunning the above lines will define the system_info() function, then call it, simply result in the echo command running.\n\n1.3.1 Positional parameters\nFunctions in bash support positional parameters by default. This allows us to do things like specify the name of the output file on the command line, as well as set a default output file name if no name is specified.\nPositional parameters are a series of special variables ($0 through $9) that contain the contents of the command line.\nFor example, let’s change the earlier function definition above slightly:\n\n\nbash\n\nsystem_info()\n{\n  echo \"$1\"\n  echo \"$2\"\n  echo \"$3\"\n}\n\nThen, let’s see what happens if we were to use this function:\n\n\nbash\n\n$ system_info Hello world !\nHello\nworld\n!\n\n$ system_info \"Hello world!\" \"How are you?\" \"I am good, thanks!\"\nHello world!\nHow are you?\nI am good, thanks!\n\n\n\n1.3.2 Detecting positional parameters\nOften, we will want to check to see if we have command line arguments on which to act. There are a couple of ways to do this. First, we could simply check to see if $1 contains anything like so:\n\n\nbash\n\nif [ \"$1\" != \"\" ]; then\n    echo \"Positional parameter 1 contains something\"\nelse\n    echo \"Positional parameter 1 is empty\"\nfi\n\nSecond, the shell maintains a variable called $# that contains the number of items on the command line in addition to the name of the command ($0).\n\n\nbash\n\n# -gt means \"greater than\" in Bash, and -lt is \"less than\".\nif [ $# -gt 0 ]; then\n    echo \"Your command line contains $# arguments\"\nelse\n    echo \"Your command line contains no arguments\"\nfi\n\n\n\n1.3.3 Naming positional parameters with local\nIn a function that has many positional parameters, it can be difficult to keep track of what each $1 $2, etc. should mean.\nA common practise in bash functions is to create named variables within a function using local:\n\n\nbash\n\nsystem_info()\n{\n  local param1=\"$1\"\n  local param2=\"$2\"\n  local param3=\"$3\"\n}\n\nlocal is a bash builtin that can only be used within a function; it makes the variable name have a visible scope restricted to that function.\n\n\n\n1.4 Conditionals\nThis section was adapted from (“Writing Shell Scripts - Lesson 8: Flow Control - Part 1” n.d.)\nMost programs need to make decisions and perform different actions depending on various conditions. In bash, there are two main things to know for achieving conditional logic:\n\nHow the shell evaluates the success or failure of a command (Exit status)\nHow the shell can control the flow of execution in our program.\n\nThese two things are elaborated in the following sections.\n\n1.4.1 Exit status\nCommands (including the scripts and shell functions we write) issue a value to the system when they terminate, called an exit status. This value, which is an integer in the range of 0 to 2551, indicates the success or failure of the command’s execution. By convention, a value of zero indicates success and any other value indicates failure.\n1 Most of these numbers aren’t used – 0 (success) and 1 (failure) are most common. You can see a useful discussion of where to find more information about exit codes on stackoverflow.The shell provides a parameter $? that we can use to examine the exit status of the previously run command. Here we see it in action:\n\n\nbash\n\n$ ls -d /usr/bin\n/usr/bin\n\n# The last command terminated sucessfully, so we have a zero exit code when calling $?\n$ echo $?\n0\n\n$ ls -d /bin/usr\nls: cannot access /bin/usr: No such file or directory\n\n# The last command did not terminate successfully, so we have a non-zero exit code when calling $?\n$ echo $?\n2\n\nSome commands use different exit status values to provide diagnostics for errors, while many commands simply exit with a value of one when they fail. man pages often include a section entitled “Exit Status,” describing what codes are used. However, a zero always indicates success.\nThe shell provides two extremely simple builtin commands that do nothing except terminate with either a zero or one exit status. The true command always executes successfully and the false command always executes unsuccessfully:\n\n\nbash\n\n$ true\n$ echo $?\n0\n$ false\n$ echo $?\n1\n\nWe can use these commands to see how the if statement works. What the if statement really does is evaluate the success or failure of commands:\n\n\nbash\n\n$ if true; then echo \"It's true.\"; fi\nIt's true.\n\n$ if false; then echo \"It's true.\"; fi\n$\n\nThe command echo “It’s true.” is executed when the command following if executes successfully, and is not executed when the command following if does not execute successfully.\n\n\n1.4.2 exit\nWe can (and should!) set the exit status of our own scripts when they finish. To do this, use the exit command. The exit command causes the script to terminate immediately and set the exit status to whatever value is given as an argument.\nFor example: exit 0 exits our script and sets the exit status to 0 (success), whereas exit 1 exits your script and sets the exit status to 1 (failure).\n\n\n1.4.3 if\nThe if command is fairly simple on the surface; it makes a decision based on the exit status of a command. The if command’s syntax looks like this:\n\n\nbash\n\nif commands; then\n    commands\n[elif commands; then\n    commands...]\n[else\n    commands]\nfi\n\nTypically, you will see the if command combined with the test command, seen below.\n\n\n1.4.4 test\nThe test command is used most often with the if command to perform true/false decisions.\nThe command is unusual in that it has two different syntactic forms:\n\n\nbash\n\n# First form\ntest expression\n\n# Second form, which is far more common\n# Note: the word \"test\" does not appear, but this is in fact a \"test\" command!\n[ expression ]\n\n\n\nNotice the spaces between the [ ] braces and the expression – the whitespace is required.\nThe test command works simply. If the given expression is true, test exits with a status of zero; otherwise it exits with a status of 1.\nThe neat feature of test is the variety of expressions we can create. Here is an example:\n\n\nbash\n\nif [ -f .bash_profile ]; then\n    echo \"You have a .bash_profile. Things are fine.\"\nelse\n    echo \"Yikes! You have no .bash_profile!\"\nfi\n\nIn this example, we use the expression -f .bash_profile. This expression asks, “Is .bash_profile a file?” If the expression is true, then test exits with a zero (indicating true) and the if command executes the command(s) following the word then. If the expression is false, then test exits with a status of one and the if command executes the command(s) following the word else.\nHere is a partial list of the conditions that test can evaluate. Since test is a shell builtin, use help test to see a complete list:\n\n\n\nExpression\nDescription\n\n\n\n\n-d file\nTrue if file is a directory.\n\n\n-e file\nTrue if file exists.\n\n\n-f file\nTrue if file exists and is a regular file.\n\n\n-L file\nTrue if file is a symbolic link.\n\n\n-r file\nTrue if file is a file readable by you.\n\n\n-w file\nTrue if file is a file writable by you.\n\n\n-x file\nTrue if file is a file executable by you.\n\n\nfile1 -nt file2\nTrue if file1 is newer than file2.\n\n\nfile1 -ot file2\nTrue if file1 is older than file2.\n\n\n-z string\nTrue if string is empty.\n\n\n-n string\nTrue if string is not empty.\n\n\nstring1 = string2\nTrue if string1 equals string2.\n\n\nstring1 != string2\nTrue if string1 does not equal string2.\n\n\n\n\n\n1.4.5 A quick note on syntax\nNote that the above example can be written in a few ways:\n\n\nbash\n\n# Preferred form\nif [ -f .bash_profile ]; then\n    echo \"You have a .bash_profile. Things are fine.\"\nelse\n    echo \"Yikes! You have no .bash_profile!\"\nfi\n\n# Alternate form\nif [ -f .bash_profile ]\nthen echo \"You have a .bash_profile. Things are fine.\"\nelse echo \"Yikes! You have no .bash_profile!\"\nfi\n\nThe semicolon ; is a command separator. Using it allows us to put more than one command on a line.\nFor example: $ clear; ls will clear the screen, then execute the ls command.\nWe use the semicolon as we did to allow us to put the word then on the same line as the if command, because it’s easier to read that way.",
    "crumbs": [
      "Bash scripting"
    ]
  },
  {
    "objectID": "notes/bash-scripting/index.html#code-reuse",
    "href": "notes/bash-scripting/index.html#code-reuse",
    "title": "Bash scripting",
    "section": "2 Code reuse",
    "text": "2 Code reuse\nThe primary goal of writing a program in any language is to cryztalize useful logic in a reusable form – that is, to write a program. The outcome is that you don’t need to repeat yourself once you’ve solved a problem once.\nIn bash there are two main avenues we will take to achieve this goal:\n\ncreate function libraries\ncreate executable scripts\n\nThe following sections elaborate each technique.\n\n2.1 Function library with source\nThis section was adapted from (“Linux Command Line Adventure: Source” n.d.)\nMost programming languages permit programmers to specify external files to be included within their programs. This is often used to add “boilerplate” code to programs for such things as defining standard constants and referencing external library function definitions.\nBash has a builtin command, source, that implements this feature. This section will cover the ways it can make our scripts more powerful and easier to maintain.\nsource reads a specified file and executes the commands within it using the current shell. It works both with the interactive command line and within a script. Using the command line for example, we can reload the .bashrc file by executing the following command:\n$ source ~/.bashrc\nNote that the source command can be abbreviated by a single dot character like so:\n$ . ~/.bashrc\nWhen source is used on the command line, the commands in the file are treated as if they are being typed directly at the keyboard. In a shell script, the commands are treated as though they are part of the script.\nsource is a natural way to share functions and variables across many bash programs. For example, it makes sense to have a shared function to display error messages:\n\n\nbash\n\nerror_msg() {\n  printf \"%s\\n\" \"$1\" &gt;&2\n}\n\nTo share these functions across other scripts, we could build a library of functions and source that library. As an example, we could put all the common code in a file called ~/bash-scripts.sh and add the following code to both scripts to source that file:\n\n\nbash\n\nFUNCLIB=~/bash-scripts.sh\n\nif [[ -r \"$FUNCLIB\" ]]; then\n    source \"$FUNCLIB\"\nelse\n    echo \"Cannot read function library!\" &gt;&2\n    exit 1\nfi\n\nIf you put source statements like this in your ~/.bashrc, your functions will always be available each time you open a new terminal instance.\n\n\n2.2 Scripts\nIn the simplest terms, a shell script is a file containing a series of commands. The shell reads this file and carries out the commands as though they have been entered directly on the command line.\nSay we have the following file, example.sh:\n\n\nexample.sh\n\n#!/bin/bash\necho \"This is an example script\"\n\nWe can use the bash program to run this script:\n$ bash example.sh\nThis is an example script\nThis is fairly similar to the source command we saw earlier. The difference is a bit subtle:\n\nsource &lt;library-name&gt; will run within our current shell instance (preserving variables and functions)\nbash &lt;script-name&gt; will launch a new shell instance, which will NOT preserve variables and functions\n\nThe impetus to use one method or the other varies by purpose:\n\nwrite a library and use source to create re-useable functions to be used on the command line or in other scripts\nwrite a script to do a specific task\n\nThere is overlap between these two purposes, so don’t overthink it too much. You will often find yourself writing scripts that should be libraries, and vice versa – you can always make these changes to your programs whenever you like.\n\n2.2.1 Shebangs\nThe first line of any script, in ANY programming language (bash, sh, or, as we will soon see, `python) should start with a shebang:\n\n\nbash-script.sh\n\n#!/bin/bash\n\n# the first line beginning with #! is a shebang!\n\nLet’s break down the components of the shebang to better understand it:\n\n# – a comment\n! – in this context, a special character indicating that this line should be executed by the program that loads this file\n/bin/bash – the path to the interpreter for the code written in this file. Note that /bin/bash is an absolute path.\n\nWe will soon see that the most portable method for specifying a shebang is by providing the path to using the env program, like so:\n#!/usr/bin/env bash     # a bash script shebang\n#!/usr/bin/env sh       # a sh script shebang\n#!/usr/bin/env perl     # a perl script shebang\n#!/usr/bin/env python   # a python script shebang\nFor now, all that matters is that you provide a path to the bash program on your system. You can see valid options by running whereis bash:\n$ whereis bash\nbash: /bin/bash\n\n\n2.2.2 Permissions\nThis section was adapted from (“How-To: Set Permissions in Bash - Linux - SS64.com” n.d.)\nIn order to run a script without specifying an interpreter, you need to make the script executable, which is a permission that you can set on the file itself.\nThe sections below show how to view and set file permissions in linux filesystems.\n\n2.2.2.1 View permissions with ls\nThe ouptut of ls -l will show the current permissions for files and folders:\n-rwxr--rw- 1 user user 0 Jan 19 12:59 file1.txt\nThe letters rwx stand for Read/Write/Execute permission. These rights are shown three times, first for the Owner, then the Group and lastly Others (world)\n\n\n2.2.2.2 Edit permissions with chmod\nThe command to modify permissions is chmod. There are two ways to modify permissions, with numbers or with letters.\nCheck out this this chmod documentation for a really great interactive demo.\n\n2.2.2.2.1 Numeric\n\nchmod 400 file - Read by owner\nchmod 040 file - Read by group\nchmod 004 file - Read by world\nchmod 200 file - Write by owner\nchmod 020 file - Write by group\nchmod 002 file - Write by world\nchmod 100 file - execute by owner\nchmod 010 file - execute by group\nchmod 001 file - execute by world\n\nTo combine these, just add the numbers together:\n\nchmod 444 file - Allow read permission to owner and group and world\nchmod 777 file - Allow everyone to read, write, and execute file\n\n\n\n2.2.2.2.2 Symbolic\nchmod also accepts symbolic arguments for permission changes, where:\n\nrwx: read/write/execute\nugo: user/group/world\n\nSome examples:\n\nDeny execute permission to everyone: $ chmod a-x file\nAllow read permission to everyone: $ chmod a+r file\nMake a file readable and writable by the group and others: $ chmod go+rw file\nMake a shell script executable by the user/owner: $ chmod u+x myscript.sh\nAllow everyone to read, write, and execute the file and turn on the set group-ID: $ chmod =rwx,g+s file\n\nSome files are configured to have very restrictive permissions to prevent unauthorized access. Changing these permissions can create security problems.\nTo change or edit files that are owned by root, sudo chmod must be used. Note that changing permissions incorrectly can quickly make your system unusable! Please be careful when using sudo!\n$ sudo chmod o+x /usr/local/bin/somefile\n\n\n\n2.2.2.3 Recursive Permission Changes\nchmod -R will change all the permissions of each file and folder under a specified directory at once.\nFor example, $ chmod 777 -R /path/to/Dir will grant read/write/execute permissions to all users for ALL files in /path/to/Dir.\nTo assign reasonably secure permissions to files and folders/directories, it’s common to give files a permission of 644, and directories a 755 permission, using the find command and a pipe we can target just files or just folders as in the following examples.\n$ sudo find /path/to/Dir -type f -print0 | xargs -0 sudo chmod 644`\n$ sudo find /path/to/Dir -type d -print0 | xargs -0 sudo chmod 755\nAgain if using sudo be careful, in particular watch for extra spaces in your command/path.\n\n\n2.2.2.4 Changing Ownership and Group membership\nA file’s owner can be changed using the chown command.\n$ sudo chown kate file1.txt\nA file’s group can also be changed using the chown command.\n$ sudo chown :mygroup file1.txt\nchown can also change the owner and group in a single command:\n$ sudo chown tux:mygroup file1.txt\n\n\n\n2.2.3 Style\nYou can see the following resources for style guides for bash coding:\n\n“Unofficial Shell Scripting Stylesheet” by the Linux Documentation Project\n“Shell Style Guide” by Google developers",
    "crumbs": [
      "Bash scripting"
    ]
  },
  {
    "objectID": "notes/bash-scripting/index.html#resources",
    "href": "notes/bash-scripting/index.html#resources",
    "title": "Bash scripting",
    "section": "3 Resources",
    "text": "3 Resources\nThis section was adapted from https://linuxcommand.org/lc3_resources.php\nAside from everything covered in these notes, you can refer to the following resources:\n\nBash Builtins - commands built into the shell itself\nThe GNU Coreutils - the essential utilities included with most Linux distributions. These are divided into three groups:\n\nFile Utilities\nText Utilities\nShell Utilities\n\nOther Commands - other commonly-used Linux utilities",
    "crumbs": [
      "Bash scripting"
    ]
  },
  {
    "objectID": "notes/debug-reterminal/index.html",
    "href": "notes/debug-reterminal/index.html",
    "title": "Debugging Reterminal Issues",
    "section": "",
    "text": "So far, there have been two primary causes for the reTerminal screen to stop working:\nIn the first case, the reTerminal boots just fine, but the screen doesn’t work. In the second and third cases, the reTerminal fails to boot at all.\nFor each of these cases, I have a set of troubleshooting steps you can take to resolve these issues.",
    "crumbs": [
      "Debugging Reterminal Issues"
    ]
  },
  {
    "objectID": "notes/debug-reterminal/index.html#if-you-can-ssh-into-your-reterminal",
    "href": "notes/debug-reterminal/index.html#if-you-can-ssh-into-your-reterminal",
    "title": "Debugging Reterminal Issues",
    "section": "1 If you can ssh into your reTerminal",
    "text": "1 If you can ssh into your reTerminal\nMost likely it is the case that your reterminal is still connecting to the internet and accessible over ssh, even when the screen is not working.\n\nDouble check that you can see your reterminal in tailscale by running tailscale status on your developer machine. ssh into the device using this IP address.\n\nIf for some reason your device is offline on tailscale, try to determine its local ip address using your router webpage (at home). ssh into reterminal using this ip address\n\nOnce you have an ssh connection, follow the steps from the course notes: Display driver fix scripts",
    "crumbs": [
      "Debugging Reterminal Issues"
    ]
  },
  {
    "objectID": "notes/debug-reterminal/index.html#if-you-cannot-ssh-into-your-reterminal",
    "href": "notes/debug-reterminal/index.html#if-you-cannot-ssh-into-your-reterminal",
    "title": "Debugging Reterminal Issues",
    "section": "2 If you cannot ssh into your reTerminal",
    "text": "2 If you cannot ssh into your reTerminal\n\n2.1 Finding the problem\n\nAsk the teacher for a micro-usb to HDMI connector\nPlug the HDMI end into a workstation monitor, and the micro-usb end into your reTerminal (in the port labelled HDMI)\nPlug your reTerminal into a power supply to boot the reTerminal\nThe monitor will show logs of your reTerminal booting. Observe these logs closely. You are looking for specific error messages.\nWrite down the specific error messages. Search the course notes for any concepts you don’t understand. Think back: have you made any changes to the /boot files that seem related to these errors?\nIf you have a good idea of what happened and what needs to be fixed, proceed to the next section. Feel free to double check your understanding with the teacher at this point – for educational purposes, I may not give you the answer directly, but I can guide you in a direction that will help save you some time.\n\n\n\n2.2 Solving /boot/ firmare file problems\nIf indeed the error comes from an incorrect /boot configuration file setting, we can fix the error without reimaging the pi. Instead, we will mount the reTerminal storage on to a workstation and then fix the firmware files directly on that workstation.\nThe steps will look similar to Lab 1, except we won’t be reimaging the device at the end. You should open Reterminal Setup instructons for reference while you are working. The following steps will be necessary:\n\nMake sure your reTerminal is entirely unplugged and powered off.\nDisassemble your reTerminal, up to the step where you can toggle the Firmware Flash switch. Toggle the switch. This step will let you mount the storage device onto your workstation.\nOn your lab workstation, find the program rpibootloader and run it. You should not need elevated access.\nWhile the rpibootloader program is running, plug your reTerminal into the workstation using a USB-C to USB-3 connector.\nYou should see the rpibootloader acknowledge connection to your reterminal. A dozen or so logs will be printed to the rpibootloader program window. Once this is done: navigate to Windows Explorer (the file manager) and you should see the reTerminal (~28-32GB) as a connected drive on the workstation\nYou can now access all files on the reterminal filesystem.\nThe firmware files can now be edited directly to fix any mistakes made.\nJust in case, you can also recover any files you need that are stored on the reterminal in the /home/your-username directory.",
    "crumbs": [
      "Debugging Reterminal Issues"
    ]
  },
  {
    "objectID": "notes/serial-protocols/index.html",
    "href": "notes/serial-protocols/index.html",
    "title": "Serial protocols",
    "section": "",
    "text": "Many of the reTerminal’s General Purpose Input and Output (GPIO) pins also have specialized functions. These specialized functions typically include specific digital communication protocols:\n\nSerial\nSPI (serial peripheral interface)\nI2C (inter-integrated circuit)\nPWM (pulse-width modulation)\nPCM (pulse-code modulation)\n\nBelow is the pin out diagram illustrating the specialized pins:\n\n\n\nBreakout of 40-pin header for reTerminal\n\n\n\n GPIO and pin diagram of the reTerminal   - reTerminal Official Wiki, Seeed.",
    "crumbs": [
      "Serial protocols"
    ]
  },
  {
    "objectID": "notes/serial-protocols/index.html#serial-protocols",
    "href": "notes/serial-protocols/index.html#serial-protocols",
    "title": "Serial protocols",
    "section": "",
    "text": "Many of the reTerminal’s General Purpose Input and Output (GPIO) pins also have specialized functions. These specialized functions typically include specific digital communication protocols:\n\nSerial\nSPI (serial peripheral interface)\nI2C (inter-integrated circuit)\nPWM (pulse-width modulation)\nPCM (pulse-code modulation)\n\nBelow is the pin out diagram illustrating the specialized pins:\n\n\n\nBreakout of 40-pin header for reTerminal\n\n\n\n GPIO and pin diagram of the reTerminal   - reTerminal Official Wiki, Seeed.",
    "crumbs": [
      "Serial protocols"
    ]
  },
  {
    "objectID": "notes/serial-protocols/index.html#serial-communication",
    "href": "notes/serial-protocols/index.html#serial-communication",
    "title": "Serial protocols",
    "section": "2 Serial Communication",
    "text": "2 Serial Communication\nIn order for two devices to exchange information, they must share a common communication protocol.\nSerial interfaces stream their data, one bit at a time. These interfaces can operate with as little as one wire (for unidirectional communication), however they typically use 2 to 4 wires.\nSerial communication can be either: synchronous and asynchronous.\n\n2.1 Synchronous Serial\nA synchronous serial interface always pairs its data line(s) with a clock signal. Therefore, all devices on the same data bus share a common clock.\n\n\n\nSynonymous unidirectional serial communication with a clock line\n\n\n\n Serial interface unidirectionally transmitting one bit at every clock pulse   Serial Communication, Sparkfun.\n\n\nHow many wires are used in the image above? Can information flow in both directions?\n\nHaving a line dedicated to a clock makes for faster serial transfer, however, it also requires an extra wire between communicating devices.\nBelow are examples of synchronous digital protocols:\n\nSPI\nI2C\nUSB (uses clock-synchronization)\n\n\n\n2.2 Asynchronous Serial\nAsynchronous means that data is transferred without support from an external clock signal.\nThis transmission method minimizes wires and I/O pins, however, extra effort is put into reliably transferring and receiving data.\nAsynchronous serial communication is typically intended for only two devices to communicate\n\n\n\nWiring diagram of two serial communication devices\n\n\n\n Wiring diagram for two devices communicating with the Serial protocol   Serial Communication, Sparkfun.\n\n\nThis is the most common type of serial communication between devices.\nThe term “Serial” is commonly used to refer to Asynchronous Serial\n\nIn order to communicate reliably, both devices have to adhere to a number of rules such as: Data bits, Synchronization bits, Parity bits, and Baud rate.\nFor example, when using the serial monitor of the Arduino IDE, it’s necessary to properly select the Baud Rate so that both the client device (the Arduino) and the host (your PC) know the exactly clock frequency of the serial communication.\n\n\n\nSelecting the Baud rate in the Arduino serial monitor\n\n\n\n  Selecting the baud rate of the Arduino IDE’s serial monitor.",
    "crumbs": [
      "Serial protocols"
    ]
  },
  {
    "objectID": "notes/serial-protocols/index.html#uarts",
    "href": "notes/serial-protocols/index.html#uarts",
    "title": "Serial protocols",
    "section": "3 UARTs",
    "text": "3 UARTs\nA universal asynchronous receiver/transmitter (UART) is a block of circuitry responsible for implementing serial communication.\nA UART converts multiple parallel digital lines into two serial lines: Transmission (Tx) and Receiving (Rx) lines.\n\nThe Raspberry Pi has a built-in UART on GPIO14 (Tx) and GPIO15 (Rx).\n\nThe python library pySerial can be used to send and receive serial data to and from the Raspberry Pi:\nimport serial\nser = serial.Serial('/dev/ttyS0')  # open serial port (9600 default baud rate)\nprint(ser.name)         # check which port was really used\nser.write(b'hello')     # write a string\nser.close()             # close port\n\n3.1 Serial Demo\nThe demo below will show a reTerminal device devices communicating with a Raspberry Pi Pico over the asynchronous serial protocol.\nThe Rx port of the reTerminal is connected to the Tx port of the Pico and vice-versa.\nBelow is the code being run on the Pico using MicroPython:\nfrom time import sleep\nfrom machine import UART, Pin\n\nuart0 = UART(0, baudrate=9600, tx=Pin(0), rx=Pin(1))\nuart0.write('hello\\n')\n\nsleep(0.01)\n\nline = uart0.read()\nprint(f'My line: {line}')\nAnd this is the code running inside the reTerminal:\nimport serial\n\nwith serial.Serial('/dev/serial0', 9600) as ser:\n\n    while True:\n        line = ser.readline()   # read a '\\n' terminated line\n        print(f'Line: {line}')\n\n        if line == b'hello\\n':\n            break\n\n    ser.write(b'there\\n')\nOnce both scripts are run simultaneously, the signal observed would look the following:",
    "crumbs": [
      "Serial protocols"
    ]
  },
  {
    "objectID": "notes/serial-protocols/index.html#serial-peripheral-interface-spi",
    "href": "notes/serial-protocols/index.html#serial-peripheral-interface-spi",
    "title": "Serial protocols",
    "section": "4 Serial Peripheral Interface (SPI)",
    "text": "4 Serial Peripheral Interface (SPI)\nSerial Peripheral Interface (SPI) is commonly used to send data between microcontrollers and small peripherals (ei. shift registers, sensors, SD cards).\nIt uses separate clock and data lines, along with a select line to choose the device you wish to talk to.\nThe communication can happen between a controller device (controlling the terms of the communication) and one or multiple peripheral devices.\nThe following nomenclature is typically used:\n\nSCK: Clock Signal. Generated by the controller device.\nCOPI: Controller-Out Peripheral-In. Information flows from controller to peripheral device.\nCIPO: Controller-In Peripheral-Out. Information flows from peripheral to controller.\nCS: Chip Select. Every peripheral device has a unique connection to the controller. The controller uses this line to enable (wake-up) the peripheral when it wants to communicate by setting it low.\n\n\n\n\nWiring and bit exchange between SPI devices\n\n\n\n Wiring diagram for two devices communicating over SPI  Serial Peripheral Interface (SPI), Sparkfun.\n\nEach peripheral device connected to the controller will need a separate CS line. To talk to a particular peripheral, the controller makes that peripheral’s CS line low and keep the rest of them high.\n\n\n Multiple peripheral devices with unique CS lines talking to the same controller   Serial Peripheral Interface (SPI), Sparkfun.",
    "crumbs": [
      "Serial protocols"
    ]
  },
  {
    "objectID": "notes/serial-protocols/index.html#problematic-technical-terminology",
    "href": "notes/serial-protocols/index.html#problematic-technical-terminology",
    "title": "Serial protocols",
    "section": "5 Problematic Technical Terminology",
    "text": "5 Problematic Technical Terminology\nHistorically, the relationship between the Controller and Peripheral devices used to be called Master and Slave. This is incredibly problematic since the Master-Slave analogy is based on an extreme and violent power relationship between two individuals.\nThe Open Source Hardware Association (OSHA) has passed a resolution asking hardware manufactures to redefine SPI signal names. However, some legacy documentation and references still include the outdated terminology below.\nDeprecated signal names:\n\nMOSI – Master Out Slave In\nMISO – Master In Slave Out\nSS – Slave Select\nMOMI – Master Out Master In\nSOSI – Slave Out Slave In\n\nProblematic technical terminology is not unique to the SPI protocol and exists in many other technical domains. In 2022, Wire magazine published a nuanced article presenting multiple sides of the terminology debate: Tech Confronts Its Use of the Labels ‘Master’ and ‘Slave’.",
    "crumbs": [
      "Serial protocols"
    ]
  },
  {
    "objectID": "notes/serial-protocols/index.html#i2c",
    "href": "notes/serial-protocols/index.html#i2c",
    "title": "Serial protocols",
    "section": "6 I2C",
    "text": "6 I2C\nThe Inter-Integrated Circuit (I2C) Protocol is a protocol intended to allow multiple “peripheral” digital devices (chips) to communicate with one or more “controller” chip.\nI2C requires only two wires, however, those two wires can support up to 1008 peripheral devices.\nHardware required to implement I2C is more complex than SPI, but less than asynchronous serial. Data speeds are also faster than asynchronous serial but slower than SPI.\n\n\n\nWiring diagram of I2C devices connected\n\n\n\n Example wiring diagram for one controller and 3 peripheral devices   I2C, Wikipedia.\n\nIn I2C, each device has a unique identifier address (a hexadecimal number). When communication is initiated, the controller must announce the address of the target device.\nYou can check what are the I2C addresses of the peripherals attached to the reTerminal with the following bash command:\npi@raspberrypi:~ $ i2cdetect -y 1\n\n     0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f\n00:          -- -- -- -- -- -- -- -- -- -- -- -- --\n10: -- -- -- -- -- -- -- -- -- UU -- -- -- -- -- --\n20: -- -- -- -- -- -- -- -- -- UU -- -- -- -- -- --\n30: -- -- -- -- -- -- -- -- UU -- -- -- -- -- -- --\n40: -- -- -- -- -- UU -- -- -- -- -- -- -- -- -- --\n50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\n60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\n70: -- -- -- -- -- -- -- --\nThe output above shows peripheral devices with the hexadecimal addresses 0x19, 0x29, 0x38, 0x40.\nWhen communicating, messages are broken up into two types of frame:\n\nAn address frame, where the controller indicates the peripheral to which the message is being sent.\nOne or more data frames, which are 8-bit data messages passed from controller to peripheral or vice versa.\n\n\n\n Clock and data lines for I2C, showing address and data frames   I2C, Sparkfun.\n\nExamples of I2C devices used in this course:\n\nLCD driver of the reTerminal\nAHT20 I2C Temperature & Humidity Sensor\nreTerminal’s accelerometer.\nreTerminal’s light sensor\n\n\n6.1 I2C Python Library\nThe python library smbus2 supports I2C protocol.\nfrom smbus2 import SMBus\n\n# Open i2c bus 1\nwith SMBus(1) as bus:\n    # read one byte from address 80, offset 0\n    b = bus.read_byte_data(80, 0)\n    print(b)\n\n    # Write a byte to address 80, offset 0\n    data = 45\n    bus.write_byte_data(80, 0, data)\n\nEach peripheral device might require a series of initialization messages to be written to it. Typically you will use python libraries made to communicate with a specific peripheral device.\nAs an example, see the library for the AHT20 temperature sensor.",
    "crumbs": [
      "Serial protocols"
    ]
  },
  {
    "objectID": "notes/serial-protocols/index.html#pulse-width-modulation-pwm",
    "href": "notes/serial-protocols/index.html#pulse-width-modulation-pwm",
    "title": "Serial protocols",
    "section": "7 Pulse Width Modulation (PWM)",
    "text": "7 Pulse Width Modulation (PWM)\nPulse Width Modulation (PWM) is a type of digital signal. With PWM it’s possible to vary how much time the signal is high in an analog fashion.\nWhile the signal can only be high (usually 3.3V or 5V) or low (ground) at any time, we can change the proportion of time the signal is high compared to when it is low over a consistent time interval.\nThe duty cycle describes the amount of “ON time” as a percentage over an interval or period of time.\n\n\n\nDuty Cycle Examples.png\n\n\n\n Examples of duty cycles as a percentage of the total “High” signal   Pulse Width Modulation, Wikipedia.\n\n Signal with a constant amplitude (voltage) but duty cycle changing from 10% to 100%. MakerPortal.com\n\n7.1 Common PWM Applications\n\n7.1.1 Servo Motors\nFor servo motor positioning the width of the pulse indicates the position where the servo arm should be.\n Servo arm position changing according to the duty cycle of a PWM signal. 14Core.com\n\n\n7.1.2 LED Dimming\nLEDs are make to work with constant voltage (approximately 2 volts). It is not possible to dim their brightness by lowering the voltage (like in incandescent light bulb).\nHowever, with PWM, it is possible to change the amount of “ON time” to give the illusion that the LED is dimmer.\n Signal duty cycle affecting LED brightness. Pyroelectro.com\nIn reality, the LED is simply blinking so fast that the human eye cannot notice it.\n\n\n\n\n\n7.2 PWM Python Library\nThe python library GPIO Zero offers good support more a number of PWM devices:\n\nLED with variable brightness\nServo\n\n# LED intensity modulated with PWM\n\nfrom gpiozero import PWMLED\nfrom time import sleep\n\nled = PWMLED(17)\n\nwhile True:\n    led.value = 0  # off\n    sleep(1)\n    led.value = 0.5  # half brightness\n    sleep(1)\n    led.value = 1  # full brightness\n    sleep(1)\n# Servo position set with PWM\n\nfrom gpiozero import Servo\nfrom time import sleep\n\nservo = Servo(17)\n\nwhile True:\n    servo.min()\n    sleep(2)\n    servo.mid()\n    sleep(2)\n    servo.max()\n    sleep(2)",
    "crumbs": [
      "Serial protocols"
    ]
  },
  {
    "objectID": "notes/serial-protocols/index.html#references",
    "href": "notes/serial-protocols/index.html#references",
    "title": "Serial protocols",
    "section": "8 References",
    "text": "8 References\nSerial Communication Tutorial, Sparkfun.\nSerial Peripheral Interface (SPI), Sparkfun\nPulse Width Modulation, Sparkfun\nServo Control, Wikipedia\nRaspberry Pi: Python Libraries for I2C, SPI, UART by Sebastian via medium.com\nRaspberry Pi UART Communication using Python and C by ElectronicWings.com",
    "crumbs": [
      "Serial protocols"
    ]
  },
  {
    "objectID": "notes/serial-protocols/index.html#diving-deeper",
    "href": "notes/serial-protocols/index.html#diving-deeper",
    "title": "Serial protocols",
    "section": "9 Diving Deeper",
    "text": "9 Diving Deeper\nIf you want to know more about how USB uses clock synchronization even thought there is no clock wire:",
    "crumbs": [
      "Serial protocols"
    ]
  },
  {
    "objectID": "notes/course-hardware/index.html",
    "href": "notes/course-hardware/index.html",
    "title": "Course hardware",
    "section": "",
    "text": "Image source",
    "crumbs": [
      "Course hardware"
    ]
  },
  {
    "objectID": "notes/course-hardware/index.html#reterminal",
    "href": "notes/course-hardware/index.html#reterminal",
    "title": "Course hardware",
    "section": "1 reTerminal",
    "text": "1 reTerminal\nThe reTerminal is a development board based on the Raspberry Pi Compute Module 4 (CM4) manufactured by Seeed Studio.\n\nSee reTerminal Wiki page for the complete specs and documentation\n\nMost of the information from the following sections was scraped from (“Getting Started with reTerminal | Seeed Studio Wiki” 2023)\n\n1.1 Features\n\nIntegrated modular design with high stability and expandability\nPowered by Raspberry Pi Computer Module 4 with 4GB RAM & 32GB eMMC\n5-Inch IPS capacitive multi-touch screen at 1280 x 720 and 293 PPI\nWireless connectivity with dual-band 2.4GHz/5GHz Wi-Fi and Bluetooth 5.0 BLE\nHigh-speed expansion interface and rich I/O for more expandability\nCryptographic co-processor with secure hardware-based key storage\nBuilt-in modules such as accelerometer, light sensor and RTC\nGigabit Ethernet Port and Dual USB 2.0 Type-A ports\n40-Pin header for IoT applications\n\n\n\n1.2 Specifications\nSee Specifications on the reTerminal wiki webpage.\n\n\n1.3 Hardware Overview\n\n1.3.1 Chassis\n\n\n\npir\n\n\n\n\n1.3.2 Motherboard\n\n\n\npir\n\n\n\n\n1.3.3 Block Diagram​\n\n\n\npir\n\n\n\n\n1.3.4 Pinout Diagram​\n\n\n\nPlease carefully pay attention to the orientation of the reTerminal in the above diagram. The LCD and the onboard buttons are on the right side whereas the back of reTerminal is on the left side. Also the whole device is flipped upside down.\n\n\n\n\n\n1.4 Power Supply\n\n\n\n\n\nRPI USB-C POWER SUPPLY BLACK US\n\n\nThe reTerminal requires a power supply that can provide a minimum of 3 Amps. The official Raspberry Pi USB-C Power Supply in included in the kit.",
    "crumbs": [
      "Course hardware"
    ]
  },
  {
    "objectID": "notes/course-hardware/index.html#compute-module-4",
    "href": "notes/course-hardware/index.html#compute-module-4",
    "title": "Course hardware",
    "section": "2 Compute Module 4",
    "text": "2 Compute Module 4\n\n\n\n\n\nRaspberry PI CM 4\n\n\nThe Compute Module 4 (CM4) made by the Raspberry Pi Foundation is powering the reTerminal.\nNotable features:\n\nProcessor: Broadcom BCM2711 quad-core Cortex-A72 (ARM v8) 64-bit SoC @ 1.5GHz\n\n\nSee CM4 datasheet for details.\n\nSee Difference Between ARM64, ARMel, and ARMhf for more info on the different ARM architectures.\n\n2.1 Grove Base Hat for Raspberry Pi\n\n\n\n\n\nGrove Base Hat for GPIO connections\n\n\nIn a typical Raspberry Pi, sensors would be connected via the 40-pin GPIO.\nTo facilitate connections of the Grove sensors, this “Hat” (term for an add-on board of the Raspberry Pi) includes the following types of connection:\n\n6 Digital\n4 Analog\n3 I2C\n1 PWM\n1 UART\n\n\nSee Grove base hat Wiki for details.",
    "crumbs": [
      "Course hardware"
    ]
  },
  {
    "objectID": "notes/course-hardware/index.html#sensors",
    "href": "notes/course-hardware/index.html#sensors",
    "title": "Course hardware",
    "section": "3 Sensors",
    "text": "3 Sensors\n\n3.1 AHT20 I2C Temperature & Humidity\n\n\n\n\n\nAHT20 I2C temperature/humidity sensor\n\n\nSee AHT20 I2C Industrial Grade Temperature & Humidity Sensor wiki for details.\n\nTemperature measurement range -40 ~ 85°C, Humidity measurement range 0 ~ 100% RH.\nDigital output, Grove I2C interface.\n\n\n\n3.2 AHT20 Libraries\nThe main module for this sensor is provided by Seeed in this Github repository and can be installed with the grove.py library.\nFollow official Step by step installation for python 3 (see below). Don’t use the one-click installation or it will install to the wrong location\ngit clone [https://github.com/Seeed-Studio/grove.py](https://github.com/Seeed-Studio/grove.py)\ncd grove.py\nsudo pip3 install .\nAlternatively, it’s also possible to use Adafruit’s adafruit-circuitpython-ahtx0 library to communicate with the sensor (see library’s Pypi page). However, to instantiate the provided sensor class, you will need to pass it an I2C bus instance. To instantiate an I2C bus instance, install and use the adafruit-extended-bus library (see Pypi page).",
    "crumbs": [
      "Course hardware"
    ]
  },
  {
    "objectID": "notes/course-hardware/index.html#actuators-motors",
    "href": "notes/course-hardware/index.html#actuators-motors",
    "title": "Course hardware",
    "section": "4 Actuators & Motors",
    "text": "4 Actuators & Motors\n\n4.1 LED Socket\n\n\n\n\n\nLED\n\n\nLED in a removable socket and potentiometer for power adjustment. LED can be swapped with different colors.\nSee LED wiki page for details.\n\n\n4.2 Cooling Fan\n\n\n\n\n\nCooling Fan\n\n\n5V Cooling Fan 40mm x 10mm with 2-pin JST connector.\n\nSee product page here.\n\n\n\n4.3 Relay\n\n\n\n\n\nRelay switch\n\n\nA digital switch. Controls the on/off flow of electricity with a small digital signal.\n\nOperate voltage: 3.3V-5V\nInput current: 100mA\nRated load: 5A@250VAC 5A@30VDC\n\nSee relay wiki page for details.",
    "crumbs": [
      "Course hardware"
    ]
  },
  {
    "objectID": "notes/course-hardware/index.html#cabling",
    "href": "notes/course-hardware/index.html#cabling",
    "title": "Course hardware",
    "section": "5 Cabling",
    "text": "5 Cabling\nThe following cables are included in the base kit:\n\nGrove Universal 4 Pin Buckled 5cm Cable.\nGrove Universal 4 Pin Buckled 20cm Cable.\nGrove 4 pin Female Jumper to Grove 4 pin Cable\nGrove 4 pin Male Jumper to Grove 4 pin Cable\n40-pin flat ribbon cable 20cm (female-female).\n2-pin JST SM Plug, one end open.",
    "crumbs": [
      "Course hardware"
    ]
  },
  {
    "objectID": "notes/developer-environment/index.html",
    "href": "notes/developer-environment/index.html",
    "title": "Developer environment setup",
    "section": "",
    "text": "Photo by Tima Miroshnichenko",
    "crumbs": [
      "Developer environment setup"
    ]
  },
  {
    "objectID": "notes/developer-environment/index.html#overview",
    "href": "notes/developer-environment/index.html#overview",
    "title": "Developer environment setup",
    "section": "1 Overview",
    "text": "1 Overview\nThroughout this semester, we will make regular use of bash, python, git, and other 3rd party command line tools such as azure-cli and gh-cli.\nNo matter what hardware you have available at home, everyone should be comfortable completing coding assignments on their personal computers and on classroom computers.\nEveryone will need the following set up:\n\non a classroom computer:\n\na Debian WSL container with all class dependencies installed using apt\n\non personal computers:\n\nIf Windows: a Debian WSL container with all class dependencies installed using apt\nIf macOS: all class dependencies installed using brew\nIf Linux: all class dependencies installed using distribution package manager\n\n\nThe sections below show how to do that, and how to verify the installation, in each case.",
    "crumbs": [
      "Developer environment setup"
    ]
  },
  {
    "objectID": "notes/developer-environment/index.html#classroom-computers-linux-wsl",
    "href": "notes/developer-environment/index.html#classroom-computers-linux-wsl",
    "title": "Developer environment setup",
    "section": "2 Classroom computers: Linux WSL",
    "text": "2 Classroom computers: Linux WSL\nIf using a Windows machine (lab computers and/or personal) for this course, you will need to set up Windows Subsystem for Linux (WSL).\n\n2.1 Ensure necessary Windows software installed\nThese programs should already be installed on your Windows machine, but in case they are not:\n\nLink for installing git on Windows\nLink for installing Windows Terminal\n\n\n2.1.1 VS Code Extensions\nIf you have not already, install VSCode.\nThen, install the following extensions:\n\nRemote Development extension pack by Microsoft\nPython language support extension\nPython formatter/linter extension (ruff)\n\n\n\n\n2.2 Install Debian WSL\n\n\nPowershell\n\n# Verify that Debian is an available OS to install\nPS &gt; wsl --list --online # Debian should be one of the results\n\n# Install Debian\nPS &gt; wsl --install -d Debian\n\nYou will be prompted to create a username and password:\n\n\nPowershell\n\n# Recommended: All lower case. Something easy to type, e.g. your first name\nEnter new UNIX username:\n# Recommended: Don't overthink this, you can always change this later\nNew password:\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you forget the password for your WSL container, you can easily reset it.\nSee the Microsoft article Set up your linux username and password.\n\nAfter this, your installation is complete.\nSee the following links for more details if needed:\n\nInstall Debian on WSL\nChoosing Debian as the Linux distribution\nTroubleshooting WSL installation\n\n\n\n2.3 Configure terminal to use WSL\nFollow the steps in Set up Windows Terminal, particularly:\n\n\n\n\n\nWe’re going to spend a lot of time in terminal environments – you might as well enjoy using it. I find it motivating to use terminals that look and feel good to use. Image source\n\n\n\nEnsure your Debian WSL instance is the default profile\n\nThen, pin Windows Terminal to your taskbar, ideally as the first app\nUse Win+1 to open Windows Terminal automatically.\n\nChoose a theme\nChoose a color scheme\n\nCustomizing a color scheme\n\nPractise searching through terminal output using Ctrl+Shift+F\nMake sure you know how to copy/paste text in Windows Terminal\n\nYou can also use Ctrl+Shift+c and Ctrl+Shift+v to copy/paste in terminals\nYou can also use Ctrl+Insert and Shift+Insert to copy paste in terminals\n\nUse Ctrl+Shift+P to open the command palette to do almost any terminal config command (very similar to VSCode).\n\nThis is very useful for learning hotkeys for the following things:\n\nmaking terminal panes\nchanging focus\n\n\n\nSee Troubleshooting Windows Terminal for more details.\n\n\n2.4 Install dependencies\n\n2.4.1 Perform system update\n\n\nbash\n\n# Update system:\nsudo apt update && sudo apt upgrade -y\n\n\n\n2.4.2 Install python\n\n\nbash\n\n# Install python3 as the default python\nsudo apt install python3 python-is-python3\n\n# Verify default python version is &gt;= 3.9:\n# NOTE: python-is-python3 makes \"python\" the same as \"python3\"\npython --version\npython3 --version\n\n# Ensure pip is installed:\nsudo apt install python3-pip\npip --version\n\n# Ensure the python module venv is installed\nsudo apt install python3-venv\n\n# Ensure developer dependencies for python are installed\nsudo apt install python3-dev\n\n\n\n2.4.3 Set up git\nYou’ll need to do the following to set up git on both your WSL:\n\n\nbash\n\nsudo apt install git\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"youremail@domain.com\"\n\nSee Installing Git for more detail if needed.\n\n\n2.4.4 Install other needed tools\nWe’re going to need the following packages:\n\n\nbash\n\nsudo apt install man ssh wget ca-certificates rsync pass pass-extension-otp zbar-tools vim\n\nLast update to this command: April 22, 2025\n\n\n\n\n\n\nNote\n\n\n\nIf, when running sudo apt install, you have an error like this:\n\n\nbash\n\nE: Failed to fetch &lt;url&gt; 404 Not Found [IP: &lt;ip&gt;]\nE: Unable to fetch some archives, maybe run apt get update or try with --fix-missing?\n\nMake sure you update the system:\n\n\nbash\n\n# you can also run sudo apt upgrade -y, but it's not necessary all the time.\nsudo apt update\n\nThen, try the installation command again.\n\n\nI’ll keep this command updated throughout the semester as we encounter more packages we need.\n\n\n\n2.5 Backup container to OneDrive\nOnce the initial setup is complete, backups of the WSL container are easy to make.\nOnce a backup is made, it’s easy to:\n\nrecreate the exact same image on a new machine\nrestore your image in case the disk is wiped (this seems to be happening to our lab computers…)\n\n\n2.5.1 Backup command\nFirst, let’s ensure you have a folder to keep your backups.\nRecommendation: store WSL images on your college OneDrive account. That way, you can easily share your image with your personal computer, and restore your image automatically using any college computer.\n\n\nPowershell\n\nPS &gt; md -Force \"C:\\Users\\&lt;your-username&gt;\\OneDrive\\420-6P3-W25\"\n\nThen, we’ll use wsl --export to make a backup copy of your WSL container:\n\n\nPowershell\n\n# This can take around 5 minutes to finish.\nPS &gt; wsl --export Debian \"C:\\Users\\&lt;your-username\\OneDrive\\420-6P3-W25\\debian.tar\n\n\n\n2.5.2 Restore command\nOn a new machine (or on a machine with a freshly wiped hard drive…) you can --import the backup image you created:\n\n\nPowershell\n\n# This can take around 5 minutes to finish.\nPS &gt; wsl --import Debian .\\Debian \"C:\\Users\\&lt;your-username&gt;\\OneDrive\\debian.tar\"\n\n\n\n\n\n\n\nNote\n\n\n\nAfter restoring WSL, you will find that you are automatically logged in as root instead of your username.\nThe way to set a default user in a WSL container instance is to create a [user] entry in the container’s /etc/wsl.conf file:\nOpen your wsl instance and add the following entry to /etc/wsl.conf:\n\n\n/etc/wsl.conf\n\n[user]\ndefault=username\n\nExit your distro/instance, then run a wsl --terminate &lt;distroname&gt; from PowerShell.\nWhen you restart, the default user should be set to username.\nFor more detail: https://superuser.com/a/1627461",
    "crumbs": [
      "Developer environment setup"
    ]
  },
  {
    "objectID": "notes/developer-environment/index.html#personal-computers",
    "href": "notes/developer-environment/index.html#personal-computers",
    "title": "Developer environment setup",
    "section": "3 Personal computers",
    "text": "3 Personal computers\n\n3.1 Windows (WSL)\nAfter setting up WSL on a classroom computer, and backing up your WSL to OneDrive, the easiest way to set up WSL on your personal computer is to import your backup WSL image on your personal computer.\nNote that any changes made to either container, after the import, will not be automatically synchronized.\nIf you are making many customizations, you might want to keep your backup up-to-date.\n\n\n3.2 macOS / Linux\nOn a terminal on your computer, install the packages below.\nOn OS X we’ll use brew, on Linux you can use your system’s package manager:\n\n\nbash\n\n# Update system:\nbrew update && brew upgrade\n# Verify python version is &gt;= 3.9:\npython3 --version\n# Ensure pip is installed:\npython3 -m pip install --upgrade pip\n\n# Install other dependencies\nbrew install wget ca-certificates rsync pass pass-otp zbar vim\n\nLast update to this command: April 22, 2025\nAlso ensure you have installed VSCode and configured its extensions",
    "crumbs": [
      "Developer environment setup"
    ]
  },
  {
    "objectID": "notes/developer-environment/index.html#verify-environment",
    "href": "notes/developer-environment/index.html#verify-environment",
    "title": "Developer environment setup",
    "section": "4 Verify environment",
    "text": "4 Verify environment\nYour developer environment, whether on WSL, macOS, or Linux, should be able to run the following commands with the following results:\n\n\nbash\n\n# Verify python version is &gt;= 3.9 and pip is installed\npython3 --version\npip3 --version # or pip --version\n\n# Verify git config set up: ensure the output makes sense for you\ngit config user.name\ngit config user.email\n\n# For the following no specific version is required\n# but, these commands should not fail (show error message or exit error code 1)\nman -V\nssh -V\nrsync --version\npass --version\npass otp --version\nzbarimg --version\nvim --version",
    "crumbs": [
      "Developer environment setup"
    ]
  },
  {
    "objectID": "notes/github-basics/index.html#overview",
    "href": "notes/github-basics/index.html#overview",
    "title": "GitHub basics",
    "section": "1 Overview",
    "text": "1 Overview\nThese notes cover the basics for using GitHub in this class.",
    "crumbs": [
      "GitHub basics"
    ]
  },
  {
    "objectID": "notes/github-basics/index.html#branch-management",
    "href": "notes/github-basics/index.html#branch-management",
    "title": "GitHub basics",
    "section": "2 Branch management",
    "text": "2 Branch management\nAlmost all of our lab and assignment work will take place on branches.\n\n2.1 Creating a branch using the GitHub website\nYou can add branches to your repository directly by clicking the “Branches” icon, and then “New Branch” on the subsequent window (screenshots below).\nYou’ll need the following information:\n\nNew branch name: the lab name (e.g. lab-0).\nSource: the instructions branch from your own repository\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: The user interface for creating branches on GitHub.\n\n\n\n\n\n2.2 Creating a branch using VSCode\nYou can create branches within your project using VSCode:\n\n\n\n\n\n\nFigure 2: Clicking on the branch button (bottom left) launches a dialog which allows for a few branch operations: you can create a new branch, switch to a local branch, switch/pull a remote branch, etc.\n\n\n\n\n\n2.3 Creating a branch using the command line\n# make sure you are on the `instructions` branch before proceeding\ngit status\n# the switch command switches branches, -c flag stands for \"create\"\ngit switch -c lab-0\n# upload your branch to the remote repository\ngit push -u origin\n# ensure your new lab-0 branch is up to date with new remote lab-0 branch\ngit status",
    "crumbs": [
      "GitHub basics"
    ]
  },
  {
    "objectID": "notes/github-basics/index.html#authentication",
    "href": "notes/github-basics/index.html#authentication",
    "title": "GitHub basics",
    "section": "3 Authentication",
    "text": "3 Authentication\nMany git operations require authentication to get permission. Some examples:\n\nPushing to a repository\nPulling from a private repository\nUsing GitHub CLI\n\nSince July 2021, GitHub no longer accepts account passwords to authenticate git operations. You have probably run into this error many times when trying to push changes or clone your private repositories on a new machine.\nThe only reason VSCode works out of the box is because VSCode and GitHub are integrated by default, both being owned by Microsoft.\nThe following sections gives us more flexible and useful ways to authenticate git commands with GitHub.\n\n3.1 Creating a personal access token\nRead “Managing your personal access tokens” on Github, and create a classic (not fine-grained) personal access token.\nAt the very least, select the repo scope – this will give your token the ability to authenticate using git on the CLI. You can select all other scopes as well if you like.\nOnce you’re finished, you’ll see your token is a string of the following form:\nghp_&lt;long string of letters and numbers&gt;\nKeep this window open – the string of characters will disappear as soon as you refresh the page.\nWe need to configure a secure storage location for this string. For this, we will use the tool pass.\n\n\n3.2 Using a password manager to store your token\nSecrets like personal access tokens need to be readily accessible to be useful – but they also should be secret, so that others cannot easily impersonate you using the token.\nA common method for managing secrets is to use a password manager. In our course we use pass to securely manage our personal access tokens on our developer environment.\n\n3.2.1 Install pass dependencies\nFirst, ensure pass and some useful related dependencies are installed:\n# On WSL / Linux\nsudo apt install pass pass-extension-otp zbar-tools\n\n# On macOS\nbrew install pass pass-otp zbar\n\n\n3.2.2 Set up gpg\npass works by using asymmetric key encryption to store secrets. That means: you posess the private key that can decrypt secrets, and you make the public key available which can encrypt secrets.\nThis scheme is not only useful for private communication (something similar is used by apps like Signal and Telegram), but also for storing any secrets – for us, we will store our github token as a secret.\nTo get started, you’ll need to generated a gpg key-pair in order to use pass.\n\n\n\n\n\n\nNote\n\n\n\nThe GitHub instructions mention using git bash – ignore them, you have a developer environment to use instead.\nIn general, when I link to external instructions, you will need to pay attention to what parts of them may be different in our class. This is a good skill in general for making effective use of resources posted online when learning a new skill.\n\n\nFollow the instructions below:\n\nCreate the gpg key-pair following the instructions on GitHub: Generating a new GPG key\nrun gpg --full-generate-key to get started.\nRecommended: You can accept the default key type (RSA)\nRecommended: Choose 4096 bits for the keysize.\nRecommended: You can accept the default “does not expire” option.\nEnter user ID information. This information should match what you have provided to GitHub already (username/email address)\nYou have to choose a password for GPG keys. Choose something strong that you can remember.\nAdd the public key to your GitHub account following the instructions: Adding a GPG Key to your GitHub account.\n\nThe name of the key on GitHub does not matter (Personal GPG Key is fine)\nThe command: gpg --armor --export prints your key to the console, you can copy/paste this output for GitHub\nEven better: use a pipe to clip.exe to put the key in your clipboard automatically with gpg --armor --export | clip.exe\n\non macOS: use pbcopy instead of clip.exe\non Linux: use xclip or wl-copy instead of clip.exe\n\n\n\n3.2.3 Store personal access token in pass\nOnce you’ve created the gpg key-pair, we can now set up pass:\npass init &lt;the-email-you-used-for-gpg-key&gt;\nFinally, copy the token string from GitHub to your clipboard. Then, open your developer terminal:\n$ pass insert github/token\nEnter password for github/token: # paste your token here, then press enter\nOnce you’ve done this, you should be able to access your token using pass github/token, or pass github/token | clip.exe to place it on your clipboard directly.\n\n\n\n3.3 Troubleshooting\nSome common errors that arise with using gpg:\n\n3.3.1 No secret key\nThis error looks like:\n\n\nbash\n\n$ pass github/token\ngpg: decryption failed: No secret key\n\nTry the following:\n\n3.3.1.1 Double check the password store setup\nConfirm that your password store is encrypted with the gpg key you expect:\n\n\nbash\n\n# This command prints out your gpg key information\n$ gpg -k \n\n# This command shows the gpg key used to encrypt your password-store\n$ cat ~/.password-store/.gpg-id\n\n# The id and/or email address should match for both!\n\n\n\n3.3.1.2 Restart gpg daemon\n\n\nbash\n\n$ gpgconf --kill gpg-agent\n\nIf that doesn’t work, try restarting your WSL instance. In powershell:\n\n\nPowershell\n\nPS &gt; wsl --shutdown &lt;distro-name&gt;\n\nAfter the shutdown attempt, retry using pass in the WSL again.",
    "crumbs": [
      "GitHub basics"
    ]
  },
  {
    "objectID": "notes/github-basics/index.html#repository-management",
    "href": "notes/github-basics/index.html#repository-management",
    "title": "GitHub basics",
    "section": "4 Repository management",
    "text": "4 Repository management\n\n4.1 Troubleshooting\n\n4.1.1 Method 1\n# Adapted from: https://stackoverflow.com/a/40098509\n\n# Delete the corrupted .git/objects\nfind .git/objects/ -size 0 -exec rm -rf {} \\;\n\n# Might need to do this?\n# git symbolic-ref HEAD refs/heads/master\n\n# Update the git repository\ngit fetch\n\n\n4.1.2 Method 2\n# Adapted from: https://stackoverflow.com/a/18238322\n\n# Delete the corrupted .git folder\n$ rm -fr .git\n\n# Create a new .git folder\n$ git init\n\n# Add your GitHub repository as the \"origin\" remote\n$ git remote add origin [your-git-remote-url]\n\n# Update the .git folder to have the latest changes from remote\n$ git fetch\n\n# Keep your current changes, but set your git reference to your working branch\n$ git reset --mixed origin/&lt;branch-name&gt;\n\n# Ensure you're on the correct branch\n$ git switch &lt;branch-name&gt;",
    "crumbs": [
      "GitHub basics"
    ]
  }
]